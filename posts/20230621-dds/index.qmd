---
title: "数据科学实战"
date: "2023-06-21"
date-modified: "2023-07-02"
image: "dds.jpg"
categories: 
  - "Data Science"
  - "R"
---

::: {.callout-note title='Progress'}
`r stfun::progress(5, 16, stop = TRUE)`
:::

::: {.callout-tip title="Learning Source"}
- <https://book.douban.com/subject/26320485/>
:::

```{r}
#| include: false
1 + 1
```

# 前言 {.unnumbered}

- 之所以谈起这些，是因为现在很多学生觉得必须先知道自己这辈子到底想要干什么，我做学生时，不可能规划将来要从事数据科学相关的工作，因为那时根本还没有数据科学这样一个领域。因此我建议这些学生，或者其他愿意听我在这儿唠叨的人：大可不必这样。不必现在就规划好未来，走点弯路也没什么，谁知道这一路上你会发现什么呢？

- 要知道，没有人是全知全能的，但集合所有人的智慧，我们就做到“无所不能”。我们认识到了每种技能的价值，因此就成功了。我们的共同点是守信，对解决有趣的问题充满好奇心，对待新的科学发现既保有适度的怀疑又充满激情。我们喜爱这项工作，对数据背后的模式充满了好奇。

# 简介：什么是数据科学

# 统计推断

- 探索性数据分析是你和数据之间的桥梁，它不向任何人证明什么。

- 学习编程的首要条件是对编程本身的热爱，对于探索横亘于人脑和机器之间的神秘空间、如何使机器满足人类的需求抱有强烈的探索需求。  
第二个条件是允许失败。编程是一门设计算法的艺术，同时是一门调试错误程序的手艺。用Fortran语言的发明者、著名的计算机科学家约翰•巴克斯的话来说：“你要随时准备着出错，你要有很多方案，努力工作去发现那些不奏效的方案，不断地这样做，直到找到正确的方案。”

- 这和天气预报大相径庭，在预测天气时，你的模型对于结果没有任何影响。比如，你预测到下星期会下雨，除非你拥有某种超能力，否则不是你让天下雨的。但是假如你搭建了一个推荐系统，证明“很多人都喜欢这本书”，那就不一样了，看到这个推荐的人没准觉得大家都喜欢的东西应该不会太差，也喜欢上这本书了，这就形成了反馈。  
在做任何分析时，都要将这种反馈考虑在内，以此对模型产生的偏差进行调整。**模型不仅预测未来，它还在影响未来**。  
一个可供用户交互的数据产品和天气预报分别处于数据分析的两个极端，无论你面对何种类型的数据和基于该数据的数据产品，不管是基于统计模型的公共政策、医疗保险还是被广泛报道的大选调查，报道本身或许会左右观众的选票，你都要将模型对你所观察和试图理解的现象的影响考虑在内。

# 算法

## 三大基本算法

### 线性回归模型

- 问题之所以成为问题，就在于它的解决方案不是显而易见的。要时刻保持这样虚心的态度，这样当你去解决一个问题时，就会更加审慎。你不必成为一个建模领域的“百事通”，不要说：“显然，这个问题使用带有惩罚项的线性回归模型就能解决。”千万别轻易下结论，即使有时你认为答案很明显，也要多听听旁人的意见。

- 当你熟练掌握了这项技术后，**真正的挑战在于你能否从一开始就知道什么情况下应该使用线性回归模型**。这里面存在的挑战往往是教科书不会告诉我们的。

- 模型对于数据来说，主要是用来捕捉其中两个方面的信息：第一个是**趋势**（trend），第二个是**变动幅度**（variation）。

- 线性模型得到的预测值只是所有可能预测值的一个总体趋势，而围绕这个趋势的波动性还没有被模型考虑进来。

- 对残差项的正态分布假设有时候是不合理的，比如说当数据具有明显的“厚尾分布”（fat-tailed distribution）特征时，或者模型的主体部分只能拟合数据中的一小部分特征时。在金融数据建模中，这都是经常发生的，因此正态分布的假设很难适用于对金融数据的建模。

- 还有另外一种评估模型的方法，它大概要遵循这样的程序：分割数据，80%用作训练数据集，20%用作测试。在训练数据及上拟合模型并估计模型的参数，并在测试数据集上计算出模型的均方误差，并与训练数据集上模型的均方误差进行比较。如果两种均方误差的差异很小，那么可以认为模型具有不错的扩展性，其过拟合的风险较小。我们也建议大家尝试改变一下训练数据集的大小，看看两者之间有何联合变动关系，这样的模型验证过程叫作“**交叉验证法**”。

### k近邻模型（k-NN）

- k近邻的主要想法是，根据属性值的相似度找到某个对象的相似对象们，并让其相似对象们一起“投票”决定该对象到底应该属于哪一类。如果有某两个或者更多个的类别投票数相同，那么就从这些类别中随机挑选一个作为该对象的类别值。

- k近邻方法需要解决两个核心问题：其一是如何根据属性定义个体之间的相似性或者紧密程度。一旦有了明确的定义，我们就可以把某个带预测个体的“近邻”们找出来，让它们投票决定该个体的类别属性。这就自然引申出了第二个问题：到底如何才能确定一个最优的k值呢？对于数据科学家来说，k值的确定十分关键。

- k近邻方法工作的详细步骤：
  1. 首先确定相似性定义，这通常也叫作“距离”。
  2. 将数据分割成训练数据和测试数据集。
  3. 选择一个模型评价标准。（对于分类问题来说，误分率是个不错的选择）
  4. 选择不同的k值，并应用k近邻模型，看哪个模型的效果最好。
  5. 基于选定的模型评价标准，选出最优的k值。
  6. 选定k之后，就可以做样本外测试了。

- 相似性测度的选择在很大程度上要取决于问题的背景和数据本身的特征。举个例子来说，对于社交网络数据来说，如何衡量两人之间的“相似性”呢？一个通常的做法是用两人之间共同好友的个数来衡量。而这样的测度方式放在别的数据上可能就不太合适了。

- 线性回归模型和k近邻模型都属于“监督性学习算法”，也就是说，预测变量的观测值x和待预测变量的观测值y都是已知的，学习的目的是找到x产生y的函数。

### K均值算法

- 统计学中的分层模型就内含了聚类的想法：比如在家庭消费调查数据中，分层建模会根据地理位置信息的不同，对不同的区域采用不同的模型形式。

- 聚类的整个过程，可以总结为以下四步。
  1. 首先，在数据空间中随机挑选k个点（叫作中心点，centroid）。这也叫k均值聚类的初始化操作，要尽量让中心点靠近数据点，并且各个中心点之间要明显不同。
  2. 将数据点分类到离它最近的中心点。
  3. 重新计算中心点的位置。
  4. 重复上述两步直到数据中心点的位置不再变动，或者变动幅度很小为止。

- k均值聚类算法有一些已知的缺点。
  - k的选择是颇具艺术性的。当然，k的取值也是有上下限的，需要满足1 \leq k \leq n，其中n是数据的样本量。
  - 收敛性问题：可能不存在唯一解，导致算法在两种可能解之间来回迭代而无法收敛。
  - 可解释性问题：也许最终的聚类结果很难给出合理的解释。这通常是k均值聚类最棘手的问题。

# 垃圾邮件过滤器、朴素贝叶斯与数据清理

## 思维实验：从实例中学习

### 线性回归为何不适用

- 传统的线性回归不能处理这种变量个数多于样本量的情形。线性回归模型参数估计的理论告诉我们，这会导致线性回归最小二乘解中的矩阵不可逆。就算换个角度，从计算机存储的角度来说，处理这样规模的数据集，普通的个人计算机还是会显得捉襟见肘。

# 逻辑回归












































::: {.callout-tip title="To be continued"}
- 5 逻辑回归
:::




