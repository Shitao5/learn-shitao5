---
title: "Applied Machine Learning Using mlr3 in R"
date: "2023-08-16"
date-modified: "2023-09-26"
image: "logo.png"
categories: 
  - Machine Learning
  - R
  - mlr3
---

```{r}
#| include: false
1 + 1
```

::: {.callout-note title='Progress'}
`r stfun::progress(3.6, 15)`
:::

::: {.callout-tip title="Learning Source"}
- <https://mlr3book.mlr-org.com/>
- 中文翻译由 ChatGPT 3.5 提供
:::

# Getting Started {.unnumbered}

```{r}
#| message: false
library(mlr3verse)
library(mlr3pipelines)
library(mlr3benchmark)
library(ggplot2)
```

```{r}
#| label: setup
#| include: false

theme_set(theme_minimal())
options(datatable.print.topn = 4)
```

# Introduction and Overview

`mlr3` by Example:

```{r}
#| warning: false
#| message: false
set.seed(123)

task = tsk("penguins")
split = partition(task)
learner = lrn("classif.rpart")

learner$train(task, row_ids = split$train)
learner$model

prediction = learner$predict(task, row_ids = split$test)
prediction

prediction$score(msr("classif.acc"))
```

The `mlr3` interface also lets you run more complicated experiments in just a few lines of code:

```{r}
#| eval: false
#| echo: false
tasks = tsks(c("breast_cancer", "sonar"))

glrn_rf_tuned = as_learner(
  ppl("robustify") %>>% 
    auto_tuner(tnr("grid_search", resolution = 5),
    lrn("classif.ranger", num.trees = to_tune(200, 500)),
    rsmp("holdout"))
)
glrn_rf_tuned$id = "RF"

# 这里报错
glrn_stack = as_learner(ppl("robustify") %>>% ppl("stacking",
    lrns(c("classif.rpart", "classif.kknn")),
    lrn("classif.log_reg")
))
glrn_stack$id = "Stack"
```



We use dictionaries to group large collections of relevant objects so they can be listed and retrieved easily.
For example, you can see an overview of available learners (that are in loaded packages) and their properties with `as.data.table(mlr_learners)` or by calling the sugar function without any arguments, e.g. `lrn()`.

> 我们使用字典来分组大量相关对象，以便可以轻松地列出和检索它们。例如，您可以通过 `as.data.table(mlr_learners)` 查看可用学习器（位于加载的包中）及其属性的概述，或者通过调用糖函数而不带任何参数，例如 `lrn()`。

```{r}
as.data.table(mlr_learners)[1:3]
```

# Fundamentals {.unnumbered}

# Data and Basic Modeling

## Tasks

### Constructing Tasks

`mlr3` includes a few predefined machine learning tasks in the `mlr_tasks` Dictionary.

```{r}
mlr_tasks
# the same as 
# tsk()
```

```{r}
tsk_mtcars = tsk("mtcars")
tsk_mtcars
```

```{r}
# create my own regression task
data("mtcars", package = "datasets")
mtcars_subset = subset(mtcars, select = c("mpg", "cyl", "disp"))
tsk_mtcars = as_task_regr(mtcars_subset, target = "mpg", id = "cars")
tsk_mtcars
```

The `id` argument is optional and specifies an identifier for the task that is used in plots and summaries; if omitted the variable name of the data will be used as the `id`.

```{r}
#| message: false
library(mlr3viz)
autoplot(tsk_mtcars, type = "pairs")
```

### Retrieving Data

```{r}
c(tsk_mtcars$nrow, tsk_mtcars$ncol)
```

```{r}
c(Features = tsk_mtcars$feature_names,
  Target = tsk_mtcars$target_names)
```

Row IDs are not used as features when training or predicting but are metadata that allow access to individual observations. Note that row IDs are not the same as row numbers.

This design decision allows tasks and learners to transparently operate on real database management systems, where primary keys are required to be unique, but not necessarily consecutive.

> 行ID在训练或预测时不作为特征使用，而是元数据，用于访问个别观测数据。需要注意的是，行ID与行号不同。
>
> 这种设计决策使得任务和学习器能够透明地在真实的数据库管理系统上运行，其中要求主键是唯一的，但不一定连续。

```{r}
task = as_task_regr(data.frame(x = runif(5), y = runif(5)),
                    target = "y")
task$row_ids

task$filter(c(4, 1, 3))
task$row_ids
```

```{r}
tsk_mtcars$data()[1:3]
tsk_mtcars$data(rows = c(1, 5, 10), cols = tsk_mtcars$feature_names)
```

### Task Mutators

```{r}
tsk_mtcars_small = tsk("mtcars")
tsk_mtcars_small$select("cyl")
tsk_mtcars_small$filter(2:3)
tsk_mtcars_small$data()
```

As `R6` uses reference semantics, you need to use `$clone()` if you want to modify a task while keeping the original object intact.

```{r}
tsk_mtcars = tsk("mtcars")
tsk_mtcars_clone = tsk_mtcars$clone()
tsk_mtcars_clone$filter(1:2)
tsk_mtcars_clone$head()
```

To add extra rows and columns to a task, you can use `$rbind()` and `$cbind()` respectively:

```{r}
tsk_mtcars_small
tsk_mtcars_small$cbind(data.frame(disp = c(150, 160)))
tsk_mtcars_small$rbind(data.frame(mpg = 23, cyl = 5, disp = 170))
tsk_mtcars_small$data()
```

## Learners

```{r}
# all the learners available in mlr3
mlr_learners
# lrns()
```

```{r}
lrn("regr.rpart")
```

All `Learner` objects include the following metadata, which can be seen in the output above:

- `$feature_types`: the type of features the learner can handle.

- `$packages`: the packages required to be installed to use the learner.

- `$properties`: the properties of the learner. For example, the “missings” properties means a model can handle missing data, and “importance” means it can compute the relative importance of each feature.

- `$predict_types`: the types of prediction that the model can make.

- `$param_set`: the set of available hyperparameters.

### Training

```{r}
# load mtcars task
tsk_mtcars = tsk("mtcars")

# load a regression tree
lrn_rpart = lrn("regr.rpart")

# pass the task to the learner via $train()
lrn_rpart$train(tsk_mtcars)
```

After training, the fitted model is stored in the `$model` field for future inspection and prediction:

```{r}
lrn_rpart$model

splits = partition(tsk_mtcars)
splits

lrn_rpart$train(tsk_mtcars, row_ids = splits$train)
```

### Predicting

```{r}
prediction = lrn_rpart$predict(tsk_mtcars, row_ids = splits$test)
prediction

autoplot(prediction)
```

```{r}
mtcars_new = data.table(cyl = c(5, 6), disp = c(100, 120),
  hp = c(100, 150), drat = c(4, 3.9), wt = c(3.8, 4.1),
  qsec = c(18, 19.5), vs = c(1, 0), am = c(1, 1),
  gear = c(6, 4), carb = c(3, 5))
prediction = lrn_rpart$predict_newdata(mtcars_new)
prediction
```

### Hyperparameters

```{r}
lrn_rpart$param_set
```

```{r}
# change hyperparameter
lrn_rpart = lrn("regr.rpart", maxdepth = 1)

lrn_rpart$param_set$values
```

```{r}
# learned regression tree
lrn_rpart$train(tsk("mtcars"))$model
```

```{r}
# another way to update hyperparameters
lrn_rpart$param_set$values$maxdepth = 2
lrn_rpart$param_set$values

# now with depth 2
lrn_rpart$train(tsk("mtcars"))$model
```

```{r}
# or with set_values()
lrn_rpart$param_set$set_values(xval = 2, cp = .5)
lrn_rpart$param_set$values
```

### Baseline Learners

Baselines are useful in model comparison and as fallback learners. For regression, we have implemented the baseline `lrn("regr.featureless")`, which always predicts new values to be the mean (or median, if the `robust` hyperparameter is set to `TRUE`) of the target in the training data:

基线在模型比较和作为备用学习器中非常有用。对于回归问题，我们已经实现了名为 `lrn("regr.featureless")` 的基线，它总是预测新值为训练数据中目标的均值（如果鲁棒性参数设置为 `TRUE`，则为中位数）：

```{r}
task = as_task_regr(data.frame(x = runif(1000), y = rnorm(1000, 2, 1)),
                    target = "y")
lrn("regr.featureless")$train(task, 1:995)$predict(task, 996:1000)
```

It is good practice to test all new models against a baseline, and also to include baselines in experiments with multiple other models. In general, a model that does not outperform a baseline is a ‘bad’ model, on the other hand, a model is not necessarily ‘good’ if it outperforms the baseline.

> 在实践中，对所有新模型进行与基线的测试是一个良好的做法，同时在与多个其他模型进行实验时也要包括基线。通常情况下，如果一个模型无法超越基线，那么它可以被视为是一个不好的模型；另一方面，如果一个模型超越了基线，也不一定就是一个好模型。

## Evaluation

```{r}
lrn_rpart = lrn("regr.rpart")
tsk_mtcars = tsk("mtcars")
splits = partition(tsk_mtcars)
lrn_rpart$train(tsk_mtcars, splits$train)
prediction = lrn_rpart$predict(tsk_mtcars, splits$test)
```

### Measures

```{r}
as.data.table(msr())[1:3]
```

```{r}
measure = msr("regr.mae")
measure
```

### Scoring Predictions

Note that all task types have default measures that are used if the argument to `$score()` is omitted, for regression this is the mean squared error (`msr("regr.mse")`).

```{r}
prediction$score()
prediction$score(measure)
prediction$score(msrs(c("regr.mse", "regr.mae")))
```

### Technical Measures

`mlr3` also provides measures that do not quantify the quality of the predictions of a model, but instead provide ‘meta’-information about the model. These include:

- `msr("time_train")`: The time taken to train a model.

- `msr("time_predict")`: The time taken for the model to make predictions.

- `msr("time_both")`: The total time taken to train the model and then make predictions.

- `msr("selected_features")`: The number of features selected by a model, which can only be used if the model has the “selected_features” property.

```{r}
measures = msrs(c("time_train", "time_predict", "time_both"))
prediction$score(measures, learner = lrn_rpart)
```

These can be used after model training and predicting because we automatically store model run times whenever `$train()` and `$predict()` are called, so the measures above are equivalent to:

```{r}
c(lrn_rpart$timings, both = sum(lrn_rpart$timings))
```

The `selected_features` measure calculates how many features were used in the fitted model.

```{r}
msr_sf = msr("selected_features")
msr_sf
```

```{r}
# accessed hyperparameters with `$param_set`
msr_sf$param_set
```

```{r}
msr_sf$param_set$values$normalize = TRUE
prediction$score(msr_sf, task = tsk_mtcars, learner = lrn_rpart)
```

Note that we passed the task and learner as the measure has the `requires_task` and `requires_learner` properties.

## Our First Regression Experiment

We have now seen how to train a model, make predictions and score them. What we have not yet attempted is to ascertain if our predictions are any ‘good’. So before look at how the building blocks of `mlr3` extend to classification, we will take a brief pause to put together everything above in a short experiment to assess the quality of our predictions. We will do this by comparing the performance of a featureless regression learner to a decision tree with changed hyperparameters.

> 我们已经了解了如何训练模型、进行预测并对其进行评分。但是，我们尚未尝试确定我们的预测是否“好”。因此，在深入研究 `mlr3` 的构建模块如何扩展到分类之前，我们将简要停顿一下，通过一个简短的实验来评估我们预测的质量。我们将通过比较无特征的回归学习器与更改超参数的决策树的性能来进行评估。


```{r}
set.seed(349)
tsk_mtcars = tsk("mtcars")
splits = partition(tsk_mtcars)
lrn_featureless = lrn("regr.featureless")
lrn_rpart = lrn("regr.rpart", cp = .2, maxdepth = 5)
measures = msrs(c("regr.mse", "regr.mae"))

# train learners
lrn_featureless$train(tsk_mtcars, splits$train)
lrn_rpart$train(tsk_mtcars, splits$train)
# make and score predictions
lrn_featureless$predict(tsk_mtcars, splits$test)$score(measures)
lrn_rpart$predict(tsk_mtcars, splits$test)$score(measures)
```

## Classification

### Our First Classification Experiment

```{r}
set.seed(349)
tsk_penguins = tsk("penguins")
splits = partition(tsk_penguins)
lrn_featureless = lrn("classif.featureless")
lrn_rpart = lrn("classif.rpart", cp = .2, maxdepth = 5)
measure = msr("classif.acc")

# train learners
lrn_featureless$train(tsk_penguins, splits$train)
lrn_rpart$train(tsk_penguins, splits$train)

# make and score predictions
lrn_featureless$predict(tsk_penguins, splits$test)$score(measure)
lrn_rpart$predict(tsk_penguins, splits$test)$score(measure)
```

### TaskClassif

```{r}
as.data.table(tsks())[task_type == "classif"]
```

The `sonar` task is an example of a binary classification problem, as the target can only take two different values, in `mlr3` terminology it has the “twoclass” property:

```{r}
tsk_sonar = tsk("sonar")
tsk_sonar
```

```{r}
tsk_sonar$class_names
```

In contrast, `tsk("penguins")` is a multiclass problem as there are more than two species of penguins; it has the “multiclass” property:

```{r}
tsk_penguins = tsk("penguins")
tsk_penguins$properties
tsk_penguins$class_names
```

A further difference between these tasks is that binary classification tasks have an extra field called `$positive`, which defines the ‘positive’ class. In binary classification, as there are only two possible class types, by convention one of these is known as the ‘positive’ class, and the other as the ‘negative’ class. It is arbitrary which is which, though often the more ‘important’ (and often smaller) class is set as the positive class. You can set the positive class during or after construction. If no positive class is specified then `mlr3` assumes the first level in the `target` column is the positive class, which can lead to misleading results.

> 这两种任务之间的另一个区别是，二分类任务有一个额外的字段称为 `$positive`，它定义了“正类”（positive class）。在二分类问题中，由于只有两种可能的类别类型，按照惯例，其中一种被称为“正类”，另一种被称为“负类”。哪个是哪个是任意的，尽管通常更“重要”（通常更小）的类别被设置为正类。您可以在构建期间或之后设置正类。如果未指定正类，则 `mlr3` 假定目标列中的第一个级别是正类，这可能导致误导性的结果。

```{r}
Sonar = tsk_sonar$data()
tsk_classif = as_task_classif(Sonar, target = "Class", positive = "R")
tsk_classif$positive
```

```{r}
# changing after construction
tsk_classif$positive = "M"
tsk_classif$positive
```

### LearnerClassif and MeasureClassif

Classification learners, which inherit from `LearnerClassif`, have nearly the same interface as regression learners. However, a key difference is that the possible predictions in classification are either `"response"` – predicting an observation’s class (a penguin’s species in our example, this is sometimes called “hard labeling”) – or `"prob"` – predicting a vector of probabilities, also called “posterior probabilities”, of an observation belonging to each class. In classification, the latter can be more useful as it provides information about the confidence of the predictions:

> 分类学习器（继承自 `LearnerClassif`）几乎具有与回归学习器相同的接口。然而，分类中的一个关键区别是，分类问题中可能的预测结果要么是 `"response"` （预测观测的类别，例如我们示例中的企鹅物种，有时称为“硬标签”），要么是 `"prob"` （预测属于每个类别的概率向量，也称为“后验概率”）。在分类中，后者可能更有用，因为它提供了有关预测的置信度信息：

```{r}
lrn_rpart = lrn("classif.rpart", predict_type = "prob")
lrn_rpart$train(tsk_penguins, splits$train)
prediction = lrn_rpart$predict(tsk_penguins, splits$test)
prediction
```

Also, the interface for classification measures, which are of class `MeasureClassif`, is identical to regression measures. The key difference in usage is that you will need to ensure your selected measure evaluates the prediction type of interest. To evaluate "response" predictions, you will need measures with `predict_type = "response"`, or to evaluate probability predictions you will need `predict_type = "prob"`. The easiest way to find these measures is by filtering the `mlr_measures` dictionary:

> 此外，分类度量标准的接口，其类别为 `MeasureClassif`，与回归度量标准完全相同。在使用上的主要区别在于，您需要确保所选的度量标准评估感兴趣的预测类型。要评估 `“response”` 预测，您需要使用 `predict_type = "response"` 的度量标准，或者要评估概率预测，您需要使用 `predict_type = "prob"` 的度量标准。查找这些度量标准的最简单方法是通过筛选 `mlr_measures` 字典：

```{r}
as.data.table(msr())[
  task_type == "classif" & predict_type == "prob" &
  !sapply(task_properties, \(x) "twoclass" %in% x)
]
```

```{r}
measures = msrs(c("classif.mbrier", "classif.logloss", "classif.acc"))
prediction$score(measures)
```

### PredictionClassif, Confusion Matrix, and Thresholding

`PredictionClassif` objects have two important differences from their regression analog. Firstly, the added field `$confusion`, and secondly the added method `$set_threshold()`.

> `PredictionClassif` 对象与其回归模型的预测对象有两个重要的区别。首先是新增的字段 `$confusion`，其次是新增的方法 `$set_threshold()`。

#### Confusion Matrix

```{r}
prediction$confusion
```

The rows in a confusion matrix are the predicted class and the columns are the true class. All off-diagonal entries are incorrectly classified observations, and all diagonal entries are correctly classified. In this case, the classifier does fairly well classifying all penguins, but we could have found that it only classifies the Adelie species well but often conflates Chinstrap and Gentoo, for example.

> 混淆矩阵中的行表示预测的类别，列表示真实的类别。所有非对角线条目都是被错误分类的观测值，而所有对角线条目都是被正确分类的。在这种情况下，分类器在对所有企鹅进行分类时表现得相当不错，但我们也可能发现它只能很好地对 Adelie 物种进行分类，但经常将 Chinstrap 和 Gentoo 混为一谈。

```{r}
#| label: fig-confusion_matrix
#| fig-cap: "Counts of each class label in the ground truth data (left) and predictions (right)."
autoplot(prediction)
```

In the binary classification case, the top left entry corresponds to true positives, the top right to false positives, the bottom left to false negatives and the bottom right to true negatives. Taking `tsk_sonar` as an example with `M` as the positive class:

> 在二分类情况下，左上角的条目对应于真正例（true positives），右上角对应于假正例（false positives），左下角对应于假负例（false negatives），右下角对应于真负例（true negatives）。以 `tsk_sonar` 为例，`M` 为正类：

```{r}
splits = partition(tsk_sonar)
lrn_rpart$
  train(tsk_sonar, splits$train)$
  predict(tsk_sonar, splits$test)$
  confusion
```

#### Thresholding

**阈值化**

This 50% value is known as the threshold and it can be useful to change this threshold if there is class imbalance (when one class is over- or under-represented in a dataset), or if there are different costs associated with classes, or simply if there is a preference to ‘over’-predict one class. As an example, let us take `tsk("german_credit")` in which 700 customers have good credit and 300 have bad. Now we could easily build a model with around “70%” accuracy simply by always predicting a customer will have good credit:

> 这个 50% 的值被称为阈值，如果数据集中存在类别不平衡（即一个类别在数据集中过多或过少出现），或者不同的类别具有不同的成本，或者只是有一种“过度”预测一种类别的倾向，那么更改这个阈值可能会很有用。举个例子，让我们看看 `tsk("german_credit")`，其中有 700 个客户信用良好，300 个客户信用不良。现在，我们可以很容易地构建一个模型，总是预测客户会有良好的信用，从而获得 “70%” 左右的准确性：

```{r}
task_credit = tsk("german_credit")
lrn_featureless = lrn("classif.featureless", predict_type = "prob")
splits = partition(task_credit)
lrn_featureless$train(task_credit, splits$train)
prediction = lrn_featureless$predict(task_credit, splits$test)
prediction$score(msr("classif.acc"))
```

::: {.callout-caution}
TODO：等待后续添加交叉引用
:::

While this model may appear to have good performance on the surface, in fact, it just ignores all ‘bad’ customers – this can create big problems in this finance example, as well as in healthcare tasks and other settings where false positives cost more than false negatives (see Section 13.1 for cost-sensitive classification).

Thresholding allows classes to be selected with a different probability threshold, so instead of predicting that a customer has bad credit if P(good) < 50%, we might predict bad credit if P(good) < 70% – notice how we write this in terms of the positive class, which in this task is ‘good’. Let us see this in practice:

> 虽然这个模型表面上看起来性能不错，但实际上它只是忽略了所有“不良”的客户 - 这在金融示例以及在医疗任务和其他一些情况下可能会带来很大问题，特别是在假阳性的成本高于假阴性的情况下（请参见第13.1节的成本敏感分类）。
>
> 阈值化允许使用不同的概率阈值选择类别，因此，与其在P(好) < 50%时预测客户信用不良，我们可以在P(好) < 70%时预测客户信用不良。请注意，我们是根据正类别来表示这一点，而在这个任务中正类别是“好”。让我们看看实际应用中的情况：

```{r}
prediction$set_threshold(0.7)
prediction$score(msr("classif.acc"))
```

```{r}
lrn_rpart = lrn("classif.rpart", predict_type = "prob")
lrn_rpart$train(task_credit, splits$train)
prediction = lrn_rpart$predict(task_credit, splits$test)
prediction$score(msr("classif.acc"))
prediction$confusion
```

```{r}
prediction$set_threshold(0.7)
prediction$score(msr("classif.acc"))
prediction$confusion
```

# Evaluation and Benchmarking

**Resampling Does Not Avoid Model Overfitting**: 
A common **misunderstanding** is that holdout and other more advanced resampling strategies can prevent model overfitting. In fact, these methods just make overfitting visible as we can separately evaluate train/test performance. Resampling strategies also allow us to make (nearly) unbiased estimations of the generalization error.

> **重采样不能避免模型过拟合**：一个常见的误解是，留出策略和其他更高级的重采样策略可以防止模型过拟合。实际上，这些方法只是使过拟合问题更加显而易见，因为我们可以单独评估训练/测试性能。重采样策略还允许我们对泛化误差进行（几乎）无偏估计。

## Holdout and Scoring

In practice, one would usually create an intermediate model, which is trained on a subset of the available data and then tested on the remainder of the data. The performance of this intermediate model, obtained by comparing the model predictions to the ground truth, is an estimate of the generalization performance of the final model, which is the model fitted on all data.

> 在实践中，通常会创建一个中间模型，该模型在可用数据的子集上进行训练，然后在剩余的数据上进行测试。通过将模型的预测与真实情况进行比较，中间模型的性能可以作为最终模型的泛化性能的估计。最终模型是在所有可用数据上训练的模型。

```{r}
tsk_penguins = tsk("penguins")
splits = partition(tsk_penguins)
lrn_rpart = lrn("classif.rpart")
lrn_rpart$train(tsk_penguins, splits$train)
prediction = lrn_rpart$predict(tsk_penguins, splits$test)
prediction$score(msr("classif.acc"))
```

## Resampling

### Constructing a Resampling Strategy

```{r}
as.data.table(rsmp())
```

```{r}
rsmp("holdout", ratio = .8)
```

```{r}
# three-fold CV
cv3 = rsmp("cv", folds = 3)
# subsampling with 3 repeats and 9/10 ratio
ss390 = rsmp("subsampling", repeats = 3, ratio = .9)
# 2-repeats 5-fold cv
rcv25 = rsmp("repeated_cv", repeats = 2, folds = 5)
```

When a `"Resampling"` object is constructed, it is simply a definition for how the data splitting process will be performed on the task when running the resampling strategy. However, it is possible to manually instantiate a resampling strategy, i.e., generate all train-test splits, by calling the `$instantiate()` method on a given task.

> 当构建一个 `"Resampling"` 对象时，它只是对在运行重采样策略时如何执行数据拆分过程的定义。然而，可以通过在给定任务上调用 `$instantiate()` 方法来手动实例化一个重采样策略，即生成所有的训练-测试拆分。

```{r}
cv3$instantiate(tsk_penguins)
# first 5 observations in first traininng set
cv3$train_set(1)[1:5]
# fitst 5 observations in thirt test set
cv3$test_set(3)[1:5]
```

When the aim is to fairly compare multiple learners, best practice dictates that all learners being compared use the same training data to build a model and that they use the same test data to evaluate the model performance. Resampling strategies are instantiated automatically for you when using the `resample()` method. Therefore, manually instantiating resampling strategies is rarely required but might be useful for debugging or digging deeper into a model’s performance.

> 当目标是公平比较多个学习器时，最佳实践要求所有进行比较的学习器都使用相同的训练数据来构建模型，并且它们使用相同的测试数据来评估模型性能。在使用 `resample()` 方法时，重采样策略会自动为您实例化。因此，手动实例化重采样策略很少是必需的，但在调试或深入研究模型性能时可能会有用。

### Resampling Experiments

The `resample()` function takes a given `Task`, `Learner`, and `Resampling` object to run the given resampling strategy. `resample()` repeatedly fits a model on training sets, makes predictions on the corresponding test sets and stores them in a `ResampleResult` object, which contains all the information needed to estimate the generalization performance.

`resample()` 函数接受给定的任务（`Task`）、学习器（`Learner`）和重采样（`Resampling`）对象，以运行给定的重采样策略。`resample()` 函数会在训练集上反复拟合模型，在相应的测试集上进行预测，并将预测结果存储在 `ResampleResult` 对象中，该对象包含了估算泛化性能所需的所有信息。

```{r}
#| results: hide
rr = resample(tsk_penguins, lrn_rpart, cv3)
```

```{r}
rr
```

```{r}
# calculate the score for each iteration
acc = rr$score(msr("classif.ce"))
acc[, .(iteration, classif.ce)]
```

```{r}
# aggregated score across all resampling iterations
rr$aggregate(msr("classif.ce"))
```

By default, the majority of measures will aggregate scores using a macro average, which first calculates the measure in each resampling iteration separately, and then averages these scores across all iterations. However, it is also possible to aggregate scores using a micro average, which pools predictions across resampling iterations into one `Prediction` object and then computes the measure on this directly:

> 默认情况下，大多数性能度量会使用宏平均（macro average）来汇总分数，它首先在每个重采样迭代中分别计算度量，然后在所有迭代中对这些分数进行平均。但也可以使用微平均（micro average）来汇总分数，它将重采样迭代中的预测汇总到一个 `Prediction` 对象中，然后直接在该对象上计算度量：

```{r}
rr$aggregate(msr("classif.ce", average = "micro"))
```

::: {.callout-caution}
TODO：等待后续添加交叉引用

已加，待检查
:::

To visualize the resampling results, you can use the `autoplot.ResampleResult()` function to plot scores across folds as boxplots or histograms (@fig-resamp-viz). Histograms can be useful to visually gauge the variance of the performance results across resampling iterations, whereas boxplots are often used when multiple learners are compared side-by-side (see @sec-benchmarking).

> 要可视化重采样结果，您可以使用 `autoplot.ResampleResult()` 函数绘制跨折叠的分数箱线图或直方图（@fig-resamp-viz）。直方图可以用于直观评估跨重采样迭代的性能结果方差，而箱线图通常用于比较多个学习器并排放置在一起时（请参阅 @sec-benchmarking）。

```{r}
#| results: hide
rr = resample(tsk_penguins, lrn_rpart, rsmp("cv", folds = 10))
```


```{r}
#| layout-ncol: 2
#| label: fig-resamp-viz
#| fig-subcap: 
#|   - "Boxplot of accuracy scores."
#|   - "Histogram of accuracy scores."
#| fig-cap: "Boxplot and Histogram of accuracy scores."
#| fig-alt: "Left: a boxplot ranging from 0.875 to 1.0 and the interquartile range between 0.925 and 0.7. Right: a histogram with five bars in a roughly normal distribution with mean 0.95, minimum 0.875 and maximum 1.0."
#| message: false

autoplot(rr, measure = msr("classif.acc"), type = "boxplot")
autoplot(rr, measure = msr("classif.acc"), type = "histogram")
```

### ResampleResult Objects

```{r}
# list of prediction objects
rrp = rr$predictions()
# print first two
rrp[1:2]
```

```{r}
# macro averaged performance
mean(sapply(rrp, \(x) x$score()))
```

By default, the intermediate models produced at each resampling iteration are discarded after the prediction step to reduce memory consumption of the `ResampleResult` object (only the predictions are required to calculate most performance measures). However, it can sometimes be useful to inspect, compare, or extract information from these intermediate models. We can configure the `resample()` function to keep the fitted intermediate models by setting `store_models = TRUE`. Each model trained in a specific resampling iteration can then be accessed via `$learners[[i]]$model`, where `i` refers to the `i`-th resampling iteration:

> 默认情况下，在进行预测步骤后，每个重新采样迭代产生的中间模型都会被丢弃，以降低 `ResampleResult` 对象的内存消耗（大多数性能指标仅需要预测）。然而，有时候检查、比较或从这些中间模型中提取信息可能是有用的。我们可以通过设置 `store_models = TRUE` 来配置 `resample()` 函数以保留拟合的中间模型。然后，可以通过 `$learners[[i]]$model` 来访问在特定重新采样迭代中训练的每个模型，其中 `i` 指的是第 `i` 个重新采样迭代：

```{r}
#| results: hide
rr = resample(tsk_penguins, lrn_rpart, cv3, store_models = TRUE)
```

```{r}
# get the model from the first iteration
rr$learners[[1]]$model
```

In this example, we could then inspect the most important variables in each iteration to help us learn more about the respective fitted models:

```{r}
# print 2nd and 3rd iteration
lapply(rr$learners[2:3], \(x) x$model$variable.importance)
```

## Benchmarking {#sec-benchmarking}

### benchmark()

Benchmark experiments in `mlr3` are conducted with `benchmark()`, which simply runs `resample()` on each task and learner separately, then collects the results. The provided resampling strategy is automatically instantiated on each task to ensure that all learners are compared against the same training and test data.

To use the `benchmark()` function we first call `benchmark_grid()`, which constructs an exhaustive *design* to describe all combinations of the learners, tasks and resamplings to be used in a benchmark experiment, and instantiates the resampling strategies.

> `mlr3` 中的基准实验是使用 `benchmark()` 函数进行的，该函数简单地在每个任务和学习器上分别运行 `resample()`，然后收集结果。提供的重新采样策略会自动在每个任务上进行实例化，以确保所有学习器都与相同的训练和测试数据进行比较。
>
> 要使用 `benchmark()` 函数，我们首先调用 `benchmark_grid()` 函数，该函数构建一个详尽的设计来描述在基准实验中要使用的所有学习器、任务和重新采样的组合，并实例化重新采样策略。

```{r}
tasks = tsks(c("german_credit", "sonar"))
learners = lrns(c("classif.rpart", "classif.ranger", "classif.featureless"),
                predict_type = "prob")
rsmp_cv5 = rsmp("cv", folds = 5)

design = benchmark_grid(tasks, learners, rsmp_cv5)
design
```

By default, `benchmark_grid()` instantiates the resamplings on the tasks, which means that concrete train-test splits are generated. Since this process is stochastic, it is necessary to set a seed **before** calling `benchmark_grid()` to ensure reproducibility of the data splits.

> 在默认情况下，`benchmark_grid()` 会在任务上实例化重新采样，这意味着会生成具体的训练-测试拆分。由于这个过程是随机的，所以在调用 `benchmark_grid()` 之前需要设置一个种子，以确保数据拆分的可重现性。

```{r}
#| results: hide
# pass design to benchmark()
bmr = benchmark(design)
```

```{r}
bmr
```

As `benchmark()` is just an extension of `resample()`, we can once again use `$score()`, or `$aggregate()` depending on your use-case, though note that in this case `$score()` will return results over each fold of each learner/task/resampling combination.

> 由于 `benchmark()` 只是 `resample()` 的扩展，因此我们可以再次使用 `$score()` 或 `$aggregate()`，具体取决于您的用例，但请注意，在这种情况下，`$score()` 将返回每个学习器/任务/重新采样组合的每个折叠的结果。

```{r}
bmr$score()[c(1, 7, 13), .(iteration, task_id, learner_id, classif.ce)]
```

```{r}
bmr$aggregate()[, .(task_id, learner_id, classif.ce)]
```

::: {.callout-caution}
TODO：等待后续添加交叉引用
:::

This would conclude a basic benchmark experiment where you can draw tentative conclusions about model performance, in this case we would possibly conclude that the random forest is the best of all three models on each task. We draw conclusions cautiously here as we have not run any statistical tests or included standard errors of measures, so we cannot definitively say if one model outperforms the other.

As the results of `$score()` and `$aggregate()` are returned in a `data.table`, you can post-process and analyze the results in any way you want. A common mistake is to average the learner performance across all tasks when the tasks vary significantly. This is a mistake as averaging the performance will miss out important insights into how learners compare on ‘easier’ or more ‘difficult’ predictive problems. A more robust alternative to compare the overall algorithm performance across multiple tasks is to compute the ranks of each learner on each task separately and then calculate the average ranks. This can provide a better comparison as task-specific ‘quirks’ are taken into account by comparing learners within tasks before comparing them across tasks. However, using ranks will lose information about the numerical differences between the calculated performance scores. Analysis of benchmark experiments, including statistical tests, is covered in more detail in Section 11.3.

> 这将总结了一个基本的基准实验，您可以初步得出关于模型性能的结论，在这种情况下，我们可能会得出结论，随机森林在每个任务上都是三个模型中最好的。我们在这里谨慎地得出结论，因为我们没有进行任何统计测试，也没有包括性能度量的标准错误，因此我们不能明确地说一个模型是否优于另一个。
>
> 由于 `$score()` 和 `$aggregate()` 的结果以 `data.table` 返回，您可以以任何您想要的方式进行后处理和分析结果。一个常见的错误是在任务差异明显的情况下，对所有任务的学习器性能进行平均。这是一个错误，因为对性能进行平均将错过对学习器在“更容易”或“更困难”的预测问题上的比较重要的洞察。比较多个任务上的整体算法性能的更强大的替代方法是分别计算每个任务上每个学习器的排名，然后计算平均排名。这可以提供更好的比较，因为通过在比较任务之前在任务内部比较学习器，可以考虑到特定于任务的“怪癖”。然而，使用排名会丢失关于计算的性能分数之间的数值差异的信息。关于基准实验的分析，包括统计测试，在第11.3节中将更详细地介绍。

### BenchmarkResult Objects

A `BenchmarkResult` object is a collection of multiple `ResampleResult` objects.

```{r}
bmrdt = as.data.table(bmr)
bmrdt[1:2, .(task, learner, resampling, iteration)]
```

```{r}
rr1 = bmr$resample_result(1)
rr2 = bmr$resample_result(2)
rr1
```

In addition, `as_benchmark_result()` can be used to convert objects from `ResampleResult` to `BenchmarkResult.` The `c()`-method can be used to combine multiple `BenchmarkResult` objects, which can be useful when conducting experiments across multiple machines:

> 此外，可以使用 `as_benchmark_result()` 将 `ResampleResult` 对象转换为 `BenchmarkResult`。`c()` 方法可用于组合多个 `BenchmarkResult` 对象，这在跨多台计算机进行实验时非常有用：

```{r}
bmr1 = as_benchmark_result(rr1)
bmr2 = as_benchmark_result(rr2)

c(bmr1, bmr2)
```

Boxplots are most commonly used to visualize benchmark experiments as they can intuitively summarize results across tasks and learners simultaneously.

> 箱线图最常用于可视化基准实验，因为它们可以直观地同时总结任务和学习器之间的结果。

```{r}
#| label: fig-benchmark-box
#| fig-cap: 'Boxplots of accuracy scores for each learner across resampling iterations and the three tasks. Random forests (`lrn("classif.ranger")`) consistently outperforms the other learners.'
autoplot(bmr, measure = msr("classif.acc"))
```

## Evaluation of Binary Classifiers

### Confusion Matrix

It is possible for a classifier to have a good classification accuracy but to overlook the nuances provided by a full confusion matrix, as in the following `tsk("german_credit")` example:

```{r}
tsk_german = tsk("german_credit")
lrn_ranger = lrn("classif.ranger", predict_type = "prob")
splits = partition(tsk_german, ratio = .8)

lrn_ranger$train(tsk_german, splits$train)
prediction = lrn_ranger$predict(tsk_german, splits$test)
prediction$score(msr("classif.acc"))
prediction$confusion
```

On their own, the absolute numbers in a confusion matrix can be less useful when there is class imbalance. Instead, several normalized measures can be derived (@fig-confusion):

- **True Positive Rate (TPR)**, **Sensitivity** or **Recall**: How many of the true positives did we predict as positive?

- **True Negative Rate (TNR)** or **Specificity**: How many of the true negatives did we predict as negative?

- **False Positive Rate (FPR)**, or $1 -$ **Specificity**: How many of the true negatives did we predict as positive?

- **Positive Predictive Value (PPV)** or **Precision**: If we predict positive how likely is it a true positive?

- **Negative Predictive Value (NPV)**: If we predict negative how likely is it a true negative?

- **Accuracy (ACC)**: The proportion of correctly classified instances out of the total number of instances.

- **F1-score**: The harmonic mean of precision and recall, which balances the trade-off between precision and recall. It is calculated as $2 \times \frac{Precision \times Recall}{Precision + Recall}$.

```{r}
#| echo: false
#| label: fig-confusion
#| fig-cap: "Binary confusion matrix of ground truth class vs. predicted class."

knitr::include_graphics("imgs/confusion_matrix.svg")
```

The `mlr3measures` package allows you to compute several common confusion matrix-based measures using the `confusion_matrix()` function:

```{r}
mlr3measures::confusion_matrix(
  truth = prediction$truth,
  response = prediction$response,
  positive = tsk_german$positive
)
```

### ROC Analysis

The ROC curve is a line graph with TPR on the y-axis and the FPR on the x-axis. 

Consider classifiers that predict probabilities instead of discrete classes. Using different thresholds to cut off predicted probabilities and assign them to the positive and negative class will lead to different TPRs and FPRs and by plotting these values across different thresholds we can characterize the behavior of a binary classifier – this is the ROC curve.

> 考虑预测概率而不是离散类别的分类器。使用不同的阈值来截断预测的概率并将其分配到正类别和负类别将导致不同的 TPR 和 FPR，并通过在不同的阈值上绘制这些值，我们可以表征二元分类器的行为 - 这就是 ROC 曲线。

```{r}
#| label: fig-basics-roc-ranger
#| fig-cap: "ROC-curve based on the `german_credit` dataset and the `classif.ranger` random forest learner. Recall FPR = $1 -$ Specificity and TPR = Sensitivity."

autoplot(prediction, type = "roc")
```

A natural performance measure that can be derived from the ROC curve is the area under the curve (AUC), implemented in `msr("classif.auc")`. The AUC can be interpreted as the probability that a randomly chosen positive instance has a higher predicted probability of belonging to the positive class than a randomly chosen negative instance. Therefore, higher values (closer to 
) indicate better performance. Random classifiers (such as the featureless baseline) will always have an AUC of (approximately, when evaluated empirically) 0.5.

> 从 ROC 曲线中可以导出的一个自然性能度量是曲线下面积（AUC），在 `msr("classif.auc")` 中实现。AUC 可以解释为随机选择的正实例具有较高的预测概率，属于正类别，而不是随机选择的负实例的概率。因此，较高的值（越接近 1）表示更好的性能。随机分类器（例如没有特征的基线）的AUC总是为（在经验上评估时约为 0.5）。

```{r}
prediction$score(msr("classif.auc"))
```

We can also plot the precision-recall curve (PRC) which visualizes the PPV/precision vs. TPR/recall. The main difference between ROC curves and PR curves is that the number of true-negatives are ignored in the latter. This can be useful in imbalanced populations where the positive class is rare, and where a classifier with high TPR may still not be very informative and have low PPV. See Davis and Goadrich (2006) for a detailed discussion about the relationship between the PRC and ROC curves.

> 我们还可以绘制精确度-召回曲线（PRC），该曲线可视化了 PPV/精确度 与 TPR/召回 之间的关系。ROC曲线和PR曲线之间的主要区别在于后者忽略了真负例的数量。在不平衡的人群中，正类别很少见的情况下，具有高TPR的分类器可能仍然不太具有信息性，并且具有较低的PPV。有关PRC和ROC曲线之间关系的详细讨论，请参阅 Davis 和 Goadrich（2006）。

```{r}
#| fig-cap: 'Precision-Recall curve based on `tsk("german_credit")` and `lrn("classif.ranger")`.'
#| label: fig-basics-prc-ranger

autoplot(prediction, type = "prc")
```

















































::: {.callout-tip title="To be continued"}
- <https://mlr3book.mlr-org.com/chapters/chapter3/evaluation_and_benchmarking.html#sec-roc-space>
:::
