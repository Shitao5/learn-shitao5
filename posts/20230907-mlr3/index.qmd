---
title: "mlr3verse 技术手册"
date: "2023-09-07"
date-modified: "2023-09-12"
image: "cover.jpg"
categories: 
  - Machine Learning
  - R
  - mlr3
---

::: {.callout-note title='Progress'}
`r stfun::progress(137, 137)`
:::

::: {.callout-tip title="Learning Source"}
- 张敬信老师 QQ 群（222427909）文件
- <https://github.com/zhjx19/RConf15/tree/main>
:::

```{r}
#| include: false
#| label: setup

options(datatable.print.nrows = 6)
```


```{r}
#| message: false
library(mlr3verse)
```

# 基础知识

## 任务：封装数据

任务是表格数据的封装，自变量为特征，因变量为目标或结果变量。

```{r}
tsks() # 查看所有自带任务
```

```{r}
# 创建任务
dat = tsk("german_credit")$data() # 提取数据
task = as_task_classif(dat, target = "credit_risk")
task
```

```{r}
# 选择特征
task$select(cols = setdiff(task$feature_names, "telephone"))
task
```

```{r}
# 划分训练集、测试集
set.seed(1)
split = partition(task, ratio = .7)
```

`stratify = TRUE` 默认按目标变量分层，得到训练集和测试集的索引（行号）。

## 学习器：封装算法

```{r}
lrns()  # 查看所有自带学习器名字
```

```{r}
# 选择随机森林分类学习器
learner = lrn("classif.ranger", num.trees = 100, predict_type = "prob")
learner
```

学习器 `$model` 属性为 `NULL`，用 `$train()` 方法在训练集上训练模型，模型结果存入 `$model`，再用 `predict()` 方法在测试集上做预测，得到结果是 `Prediction` 对象。

```{r}
learner$train(task, row_ids = split$train)
learner$model
```

```{r}
#| eval: false
prediction = learner$predict(task, row_ids = split$test)
prediction
```

## 性能评估

```{r}
msrs() # 查看所有支持的性能度量指标
```

用预测对象的 `$score()` 方法，计算该度量指标的得分：

```{r}
#| eval: false
prediction$score(msr("classif.acc"))  # 准确率
```

```{r}
#| eval: false
# 绘制 ROC 曲线
library(precrec)
autoplot(prediction, type = "roc")
```

```{r}
#| eval: false
prediction$score(msr("classif.auc"))  # auc 面积
```

## 重抽样

```{r}
rsmps()  # 查看所有支持的重抽样方法
```

```{r}
cv10 = rsmp("cv", folds = 10)  # 10 折交叉验证
```

### 实例化重抽样对象

```{r}
cv10$instantiate(task)  # 实例化
cv10$iters  # 数据副本数
```

```{r}
#| eval: false
cv10$train_set(1)  # 第 1 个数据副本的训练集索引
cv10$test_set(1)   # 第 1 个数据副本的测试集索引
```

### 使用重抽样

```{r}
#| message: false
rr =  resample(task, learner, cv10, store_models = TRUE)
```

```{r}
rr$aggregate(msr("classif.acc"))  # 所有重抽样的平均准确率
```

```{r}
rr$score(msr("classif.acc"))  # 各个重抽样的平均准确率
```

```{r}
#| eval: false

# 查看第 1 个数据副本（第 1 折）上的结果
rr$resampling$train_set(1)  # 第 1 折的训练集索引
rr$learners[[1]]$model      # 第 1 折学习器的拟合模型
rr$predictions()[[1]]       # 第 1 折的预测结果
```

## 基准测试

基准测试（benchmark）用来比较不同学习器（算法）、在多个任务（数据）和/或不同重抽样策略（多个数据副本）上的平均性能表现。

基准测试时有个关键问题：测试的公平性。即每个算法的每次测试必须在相同的重抽样训练集拟合模型，在相同的重抽样测试集评估性能。这些事情 `beachmark()` 会自动做好。

```{r}
tasks = tsk("sonar")  # 可以多任务
learners = lrns(c("classif.rpart", "classif.kknn", "classif.ranger", "classif.svm"),
                predict_type = "prob")
design = benchmark_grid(tasks, learners, rsmps("cv", folds = 5))
design
```

```{r}
#| results: hide
bmr = benchmark(design)  # 执行基准测试
bmr$aggregate(list(msr("classif.acc"), msr("classif.auc")))  # 汇总基准测试结果
```

```{r}
# 可视化：对比性能
autoplot(bmr, type = "roc")  # ROC 曲线
autoplot(bmr, measure = msr("classif.auc"))  # AUC 箱线图
```

## 可视化

mlr3viz 包定义了 `autoplot()` 函数来用 ggplot2 绘图。通常一个对象有不止一种类型的图，可以通过 `type` 参数来改变绘图。图形使用 viridis 的调色板，外观由 theme 参数控制，默认是 minimal 主题。

### 可视化任务

#### 分类任务

```{r}
task = tsk("penguins")
task$select(c("body_mass", "bill_length"))
```

```{r}
#| warning: false
#| message: false
autoplot(task, type = "target")  # "target" 图：条形图展示目标变量的各类别频数
autoplot(task, type = "duo")     # "duo" 图：箱线图展示多个特征的分布
autoplot(task, type = "pairs")   # "pairs" 图：展示多个特征的成对比较
```

#### 回归任务

```{r}
task = tsk("mtcars")
task$select(c("am", "carb"))
```

```{r}
autoplot(task, type = "target")  # "target" 图：箱线图展示目标变量的分布
autoplot(task, type = "pairs")   # "pairs" 图：展示多个特征与目标变量的成对比较
```

### 可视化学习器

#### glmnet 回归学习器

```{r}
task = tsk("mtcars")
learner = lrn("regr.glmnet")
learner$train(task)
autoplot(learner)
```

#### 决策树学习器

```{r}
# 分类树
task = tsk("penguins")
learner = lrn("classif.rpart", keep_model = TRUE)
learner$train(task)
autoplot(learner)
```

```{r}
# 回归树
task = tsk("mtcars")
learner = lrn("regr.rpart", keep_model = TRUE)
learner$train(task)
autoplot(learner)
```

#### 层次聚类学习器

```{r}
# 层次聚类树
task = tsk("usarrests")
learner = lrn("clust.hclust")
learner$train(task)
autoplot(learner, type = "dend", task = task)
```

```{r}
# 碎石图
autoplot(learner, type = "scree")
```

### 可视化预测对象

#### 分类预测对象

```{r}
# "stacked" 图：堆叠条形图展示预测类别和真实类别频数对比
task = tsk("spam")
learner = lrn("classif.rpart", predict_type = "prob")
pred = learner$train(task)$predict(task)
autoplot(pred, type = "stacked")
```

```{r}
# ROC 曲线：展示不同阈值下的真阳率与假阳率
autoplot(pred, type = "roc")
```

```{r}
# PR 曲线：展示不同阈值下的查准率与召回率
autoplot(pred, type = "prc")
```

```{r}
# “threshold” 图：展示二元分类在不同阈值下的性能
autoplot(pred, type = "threshold")
```

#### 回归预测对象

```{r}
# "xy" 图：散点图展示回归预测的真实值与预测值
task = tsk("boston_housing")
learner = lrn("regr.rpart")
pred = learner$train(task)$predict(task)
autoplot(pred, type = "xy")
```

```{r}
# "residual" 图：绘制响应的残差图
autoplot(pred, type = "residual")
```

```{r}
#| message: false
# "histogram" 图：残差直方图展示残差的分布
autoplot(pred, type = "histogram")
```

#### 聚类预测对象

```{r}
#| warning: false
# "scatter" 图：绘制按聚类预测结果着色的散点图
task = tsk("usarrests")
learner = lrn("clust.kmeans", centers = 3)
pred = learner$train(task)$predict(task)
autoplot(pred, task, type = "scatter")
```

```{r}
# "sil" 图：展示聚类的silhouette 宽度，虚线是平均silhouette 宽度
autoplot(pred, task, type = "sil")
```

```{r}
# "pca" 图：展示数据的前两个主成分，不同聚类用颜色区分
autoplot(pred, task, type = "pca")
```

### 可视化重抽样结果

#### 分类重抽样结果

```{r}
#| results: hide
# "boxplot"/"histogram" 图：箱线图展示性能度量的分布
task = tsk("sonar")
learner = lrn("classif.rpart", predict_type = "prob")
resampling = rsmp("cv")
rr = resample(task, learner, resampling)
autoplot(rr, type = "boxplot")
autoplot(rr, type = "histogram")
```

```{r}
# ROC 曲线：展示不同阈值下的真阳率与假阳率
autoplot(rr, type = "roc")
```

```{r}
# PR 曲线：展示不同阈值下的查准率与召回率
autoplot(rr, type = "prc")
```

```{r}
#| warning: false
#| results: hide
# "prediction" 图：展示两个特征表示的测试集样本点和以背景色区分的预测类别
task = tsk("pima")
task$filter(seq(100))
task$select(c("age", "glucose"))
learner = lrn("classif.rpart")
resampling = rsmp("cv", folds = 3)
rr = resample(task, learner, resampling, store_models = TRUE)
autoplot(rr, type = "prediction")
```

```{r}
#| warning: false
#| results: hide
# 若将学习器的预测类型改为"prob"，则用颜色深浅展示概率值
learner = lrn("classif.rpart", predict_type = "prob")
resampling = rsmp("cv", folds = 3)
rr = resample(task, learner, resampling, store_models = TRUE)
autoplot(rr, type = "prediction")
```

```{r}
#| warning: false
#| results: hide
# 上面是只绘制测试集，也可以加入训练集
learner = lrn("classif.rpart", predict_type = "prob",
              predict_sets = c("train", "test"))
resampling = rsmp("cv", folds = 3)
rr = resample(task, learner, resampling, store_models = TRUE)
autoplot(rr, type = "prediction",
         predict_sets = c("train", "test"))
```

```{r}
#| results: hide
# "prediction" 图也可以绘制分类特征
task = tsk("german_credit")
task$filter(seq(100))
task$select(c("housing", "employment_duration"))
learner = lrn("classif.rpart")
resampling = rsmp("cv", folds = 3)
rr = resample(task, learner, resampling, store_models = TRUE)
autoplot(rr, type = "prediction")
```

#### 回归重抽样结果

```{r}
#| results: hide
# "prediction" 图：绘制一个特征与响应的散点图，散点表示测试集中的观测
task = tsk("boston_housing")
task$select("age")
task$filter(seq(100))
learner = lrn("regr.rpart")
resampling = rsmp("cv", folds = 3)
rr = resample(task, learner, resampling, store_models = TRUE)
autoplot(rr, type = "prediction")
```

```{r}
#| results: hide
# 若将学习器的预测类型改为"se"，还可以加上置信带
learner = lrn("regr.lm", predict_type = "se")
resampling = rsmp("cv", folds = 3)
rr = resample(task, learner, resampling, store_models = TRUE)
autoplot(rr, type = "prediction")
```

```{r}
#| results: hide
# 上面是只绘制测试集，也可以加入训练集
task$select("age")
learner = lrn("regr.lm", predict_type = "se",
              predict_sets = c("train", "test"))
resampling = rsmp("cv", folds = 3)
rr = resample(task, learner, resampling, store_models = TRUE)
autoplot(rr, type = "prediction",
         predict_sets = c("train", "test"))
```

```{r}
#| results: hide
# 还可以将预测面添加到背景
task = tsk("boston_housing")
task$select(c("age", "rm"))
task$filter(seq(100))
learner = lrn("regr.rpart")
resampling = rsmp("cv", folds = 3)
rr = resample(task, learner, resampling, store_models = TRUE)
autoplot(rr, type = "prediction")
```

### 可视化基准测试结果

```{r}
#| results: hide
# "boxplot" 图：箱线图展示了多个任务的基准测试的性能分布
tasks = tsks(c("pima", "sonar"))
learners = lrns(c("classif.featureless", "classif.rpart", "classif.xgboost"),
                predict_type = "prob")
resampling = rsmps("cv")
bmr = benchmark(benchmark_grid(tasks, learners, resampling))
autoplot(bmr, type = "boxplot")
```

```{r}
#| results: hide
# 绘制一个任务与多个学习器的基准测试
task = tsk("pima")
learners = lrns(c("classif.featureless", "classif.rpart", "classif.xgboost"),
                predict_type = "prob")
resampling = rsmp("cv")
bmr = benchmark(benchmark_grid(task, learners, resampling))
```

```{r}
autoplot(bmr, type = "roc")
autoplot(bmr, type = "prc")
```

### 可视化调参实例

```{r}
#| results: hide
# "performance" 图：散点折线图展示随批次的模型性能变化
instance = tune(
  tuner = tnr("gensa"),
  task = tsk("sonar"),
  learner = lts(lrn("classif.rpart")),
  resampling = rsmp("holdout"),
  measures = msr("classif.ce"),
  term_evals = 100
)
autoplot(instance, type = "performance")
```


```{r}
# "parameter" 图：散点图展示每个超参数取值与模型性能变化
autoplot(instance, type = "parameter", cols_x = c("cp", "minsplit"))
```

```{r}
# "marginal" 图：展示不同超参数值的性能，颜色表示批次
autoplot(instance, type = "marginal", cols_x = "cp")
```

```{r}
# "parallel" 图：可视化超参数之间的关系
autoplot(instance, type = "parallel")
```

```{r}
# "points" 图：散点热力图展示两个超参数的性能对比，用颜色深浅表示模型性能
autoplot(instance, type = "points", cols_x = c("cp", "minsplit"))
```

```{r}
#| message: false
#| results: hide
# "pairs" 图：展示所有超参数成对对比
autoplot(instance, type = "pairs")
```

```{r}
# "surface" 图：绘制两个超参数的模型性能面，该面是用一个学习器插值的
autoplot(instance, type = "surface", cols_x = c("cp", "minsplit"),
         learner = lrn("regr.ranger"))
```

### 可视化特征过滤器

```{r}
# 条形图展示基于过滤器的特征得分
task = tsk("mtcars")
ft = flt("correlation")
ft$calculate(task)
autoplot(ft, n = 5)
```

更多细节在 mlr3viz 包的 reference 页。

# 图学习器

一个管道运算（PipeOp），表示机器学习管道中的一个计算步骤。一系列的 PipeOps  通过边连接（`%>>%`）构成图（Graph），图可以是简单的线性图，也可以是复杂的非线性图。

搭建图学习器：

- 用 `po()` 获取 PipeOp，通过连接符 `%>>%` 连接 Graphs 与 PipeOps

- 通过 `gunion()` 将 Graphs 与 PipeOps 并起来

- 用 `ppl("replicate", graph, n)` 将 Graph 或 PipeOps 复制 n 份并起来

- `Graph$plot()` 绘制图的结构关系

- `as_learner(Graph)` 将图转化为学习器，即可跟普通学习器一样使用

管道、图学习器的主要用处在于：

- 特征工程：缺失值插补、特征提取、特征选择、处理不均衡数据……

- 集成学习：袋装法、堆叠法

- 分支训练、分块训练

## 线形图

```{r}
# 搭建图
graph = po("scale") %>>%
  po("encode") %>>%
  po("imputemedian") %>>%
  lrn("classif.rpart")
```

```{r}
# 可视化图
graph$plot()
```

```{r}
# 转为图学习器
gl = as_learner(graph)
```

```{r}
# 训练任务
task = tsk("iris")
gl$train(task)
```

调试图：

```{r}
# 取出或设置图学习器超参数
graph$pipeops$scale$param_set$values$center = FALSE
```

```{r}
# 获取单独PipeOp 的$state（经过$train() 后）
graph$keep_results = TRUE
graph$train(task)
```

```{r}
graph$pipeops$scale$state$scale
```


```{r}
# 查看图的中间结果：$.result（需提前设置$keep_results = TRUE）
graph$pipeops$scale$.result[[1]]$head()
```

## 非线性图

### 分支

集成学习的袋装法、堆叠法都是非线性图，另一种非线性图是分支：即只执行若干条备选路径中的一条。

```{r}
graph_branch = ppl("branch", list(
  pca = po("pca"),
  ica = po("ica")
)) %>>%
  lrn("classif.kknn")
graph_branch$plot()
```

### 分块训练

在数据太大，无法载入内存的情况下，一个常用的技术是将数据分成几块，然后分别对各块数据进行训练，之后再用 PipeOpClassifAvg 或 PipeOpRegrAvg 将模型按加权平均汇总。

```{r}
graph_chunks = po("chunk", 4) %>>%
  ppl("greplicate", lrn("classif.rpart"), 4) %>>%
  po("classifavg", 4)
graph_chunks$plot()
```

```{r}
# 转化为图学习器，与学习器一样使用
chunks_lrn = as_learner(graph_chunks)
chunks_lrn$train(tsk("iris"))
```

## 集成学习

集成学习，是通过构建多个基学习器，并按一定策略结合成强学习器来完成学习任务，即所谓“博采众长”，最终效果是优于任何一个原学习器。集成学习可用于分类/回归集成、特征选择集成、异常值检测集成等。

这多个基学习器可以是同质的，比如都用决策树或都用神经网络，以 Bagging 和 Boosting 模式为代表；也可以是异质的，即采用不同的算法，以 Stacking 模式为代表。

### 装袋法（Bagging）

Bagging 采用的是并行机制，即基学习器的训练之间没有前后顺序可以同时进行。

Bagging 是用“有放回” 抽样（Bootstrap 法）的方式抽取训练集，对于包含 $m$ 个样本的训练集，进行 $m$ 次有放回的随机抽样操作，得到样本子集（有重复）中有接近 36.8% 的样本没有被抽到。

按照同样的方式重复进行，就可以采集到 $T$ 个包含 $m$ 个样本的数据副本，从而训练出 $T$ 个基学习器。

最终对这 $T$ 个基学习器的输出进行结合，分类问题就采用“多数决”，回归问题就采用“取平均”。

```{r}
# 单分支：数据子抽样 + 决策树
single_path = po("subsample") %>>% lrn("classif.rpart")
# 复制 10 次得到 10 个分支，再接类平均
graph_bag = ppl("greplicate", single_path, n = 10) %>>%
  po("classifavg")
# 可视化结构关系
graph_bag$plot()
```

```{r}
# 转为图学习器，可与普通学习器一样使用
baglrn = as_learner(graph_bag)
baglrn$train(tsk("iris"))
```

### 提升法（Boosting）

Boosting 采用的是串行机制，即基学习器的训练存在依赖关系，按次序一一进行训练（实现上可以做到并行）。

基本思想：基模型的训练集按照某种策略每次都进行一定的转化，对所有基模型预测的结果进行线性合成产生最终的预测结果。

从偏差-方差分解来看，Boosting 算法主要关注于降低偏差，每轮的迭代都关注于训练过程中预测错误的样本，将弱学习器提升为强学习器。

### 堆叠法（Stacking）

Stacking 法，采用的是分阶段机制，将若干基模型的输出作为输入，再接一层主学习器，得到最终的预测。

将训练好的所有基模型对训练集进行预测，第 $j$ 个基模型对第 $i$ 个训练样本的预测值将作为新的训练集中第 $i$ 个样本的第 $j$ 个特征值，最后基于新的训练集进行训练。同理，预测的过程也要先经过所有基模型的预测形成新的测试集，最后再对测试集进行预测。

用于堆叠的基模型通常采用不同的模型，作用在相同的训练集上。

为了充分利用数据，Stacking 通常采用 $k$ 折交叉训练法（类似 $k$ 折交叉验证）：每个基学习器分别在各个 $k-1$ 折数据上训练，在其剩下的 1 折数据上预测，就可以得到对任意 1 折数据的预测结果，进而用于训练主模型。

```{r}
graph_stack = gunion(list(
  po("learner_cv", lrn("regr.lm")),
  po("learner_cv", lrn("regr.svm")),
  po("nop")
)) %>>%
  po("featureunion") %>>%
  lrn("regr.ranger")

graph_stack$plot()
```

```{r}
#| eval: false
# 转为图学习器，可与普通学习器一样使用
stacklrn = as_learner(graph_stack)
stacklrn$train(tsk("mtcars"))
```

# 特征工程

机器学习中的数据预处理，通常也统称为特征工程，主要包括：缺失值插补、特征变换，目的是提升模型性能。

## 特征工程概述

用 mlr3pipelines 包实现特征工程。

```{r}
# 查看所有 PipeOp
po()
```

```{r}
gr = po("scale") %>>% po("pca", rank. = 2)
gr$plot()
```

特征工程管道的三种用法：

1. <p>调试：查看特征工程步对输入数据做了什么</p>

```{r}
# 训练特征工程管道：提供任务，访问特征工程之后的数据
task = tsk("iris")
gr$train(task)[[1]]$data()
```

```{r}
# 将训练好的特征工程管道，用于新的数据
gr$predict(task$filter(1:5))[[1]]$data()
```

2. <p>用于机器学习：原始任务经过特征工程管道变成预处理之后的任务，再正常用于机器学习流程</p>

```{r}
newtask = gr$train(task)[[1]]
newtask
```

3. <p>用于机器学习：再接一个学习器，转化成图学习器，同普通学习器一样使用，适合对特征工程步、ML 算法一起联动调参</p>

```{r}
gr = gr %>>% lrn("classif.rpart")
gr$plot()
```

```{r}
gr_learner = as_learner(gr)
gr_learner
```

## 缺失值插补

目前支持的插补方法：

```{r}
as.data.table(mlr_pipeops)[tags %in% "missings", "key"]
```

### 简单插补

```{r}
task = tsk("pima")
task$missings()
```

```{r}
# 常数插补
po = po("imputeconstant", param_vals = list(
  constant = -999, affect_columns = selector_name("glucose")
))
new_task = po$train(list(task = task))[[1]]
new_task$missings()
```

```{r}
# 均值插补
po = po("imputemean")
new_task = po$train(list(task = task))[[1]]
new_task$missings()
```

### 随机抽样插补

通过从非缺失的训练数据中随机抽样来插补特征。

```{r}
po = po("imputesample")
new_task = po$train(list(task = task))[[1]]
new_task$missings()
```

### 直方图法插补

```{r}
po = po("imputehist")
new_task = po$train(list(task = task))[[1]]
new_task$missings()
```

### 学习器插补

```{r}
# 决策树插补
po = po("imputelearner", lrn("regr.rpart"))
new_task = po$train(list(task = task))[[1]]
new_task$missings()
```

```{r}
# KNN插补，训练KNN学习器时，先用直方图法插补
po = po("imputelearner", 
        po("imputehist") %>>% lrn("regr.kknn"))
new_task = po$train(list(task = task))[[1]]
new_task$missings()
```

## 特征工程

### 特征缩放

不同数值型特征的数据量纲可能相差多个数量级，这对很多数据模型会有很大影响，所以有必要做归一化处理，就是将列或行对齐并转化为一致。

```{r}
#| message: false
library(tidyverse)
df = as_tibble(iris) %>% 
  set_names(str_c("x", 1:4), "Species")
task = as_task_classif(df, target = "Species")
```

#### 标准化

标准化也称为 Z 标准化，将数据变成均值为 0，标准差为 1。

```{r}
pos = po("scale")  # 参数 scale = FALSE，只做中心化
pos$train(list(task))[[1]]$data()
```

注：中心化后，0 就代表均值，更方便模型解释。

#### 归一化

归一化是将数据线性放缩到 $\left[0, 1\right]$（根据需要也可以线性放缩到 $\left[a, b\right]$）, 一般还同时考虑指标一致化，将正向指标（值越大越好）和负向指标（值越小越好）都变成正向。

正向指标：

$$
x_i'=\frac{x_i - \text{min} x_i}{\text{max} x_i - \text{min} x_i}
$$

负向指标：

$$
x_i'=\frac{\text{max} x_i - x_i}{\text{max} x_i - \text{min} x_i}
$$

```{r}
pop = po("scalerange", param_vals = list(lower = 0, upper = 1))
pop$train(list(task))[[1]]$data()
```

#### 行规范化

行规范化，常用于文本数据或聚类算法，是保证每行具有单位范数，即每行的向量“长度”相同。想象一下，$m$ 个特征下，每行数据都是 $m$ 维空间中的一个点，做行规范化能让这些点都落在单位球面上（到原点的距离均为 1）。

行规范化一般采用 L2 范数：

$$
x_{ij}' = \frac{x_{ij}}{\|x_i\|} = \frac{x_{ij}}{\sqrt{\sum_{j=1}^m x_{ij}^2}}
$$

```{r}
pop = po("spatialsign")
pop$train(list(task))[[1]]$data()
```

### 特征变换

#### 非线性特征

对于数值特征 $x_1, x_2, \cdots$，可以穿件更多的多项式特征：$x_1^2, x_1, x_2^2, x2, \cdots$，这相当于是用自变量的更高阶泰勒公式去逼近因变量。

```{r}
pop = po("modelmatrix", formula = ~ . ^ 2 + I(x1 ^ 2) + log(x2))
pop$train(list(task))[[1]]$data()
```

另一种常用的非线性特征是基于自然样条的样条特征（暂时没有专门的PipeOp）：

```{r}
pop = po("modelmatrix", formula = ~ splines::ns(x1, 5))
pop$train(list(task))[[1]]$data()
```

#### 计算新特征

管道运算“mutate”，根据以公式形式给出的表达式添加特征，这些表达式可能取决于其他特征的值。这可以增加新的特征，也可以改变现有的特征。

```{r}
pom = po("mutate", mutation = list(
  x1_p = ~ x1 + 1,
  Area1 = ~ x1 * x2,
  Area2 = ~ x3 * x4,
  Area = ~ Area1 + Area2
))
pom$train(list(task))[[1]]$data()
```

利用正则表达式从复杂字符串列提取出相关信息，并转化为因子特征：

```{r}
po_ftextract = po("mutate", mutation = list(
  fare_per_person = ~ fare / (parch + sib_sp + 1),
  deck = ~ factor(str_sub(cabin, 1, 1)),
  title = ~ factor(str_extract(name, "(?<=, ).*(?=\\.)")),
  surname = ~ factor(str_extract(name, "^.*(?=,)")),
  ticket_prefix = ~ factor(str_extract(ticket, "^.*(?= )"))
))
po_ftextract$train(list(tsk("titanic")))[[1]]$data()
```

若数据噪声太多的问题，通常就需要做数据平滑。

```{r}
pom = po("mutate", mutation = list(
  x1_s = ~ slider::slide_dbl(x1, mean, .before = 2, .after = 2) # 五点移动平均
))
dat = pom$train(list(task = task))[[1]]$data()
```

```{r}
library(patchwork)
p1 = ggplot(dat, aes(1:150, x1)) + geom_line()
p2 = ggplot(dat, aes(1:150, x1_s)) + geom_line()
p1 / p2
```

```{r}
# colapply：应用函数到任务的每一列，常用于类型转换
poca = po("colapply", applicator = as.character)
```

```{r}
# renamecolumns：修改列名
pop = po("renamecolumns", param_vals = list(
  renaming = c("Petal.Length" = "PL")
))
```

#### 正态性变换

Box-Cox 变换是更神奇的正态性变换，用最大似然估计选择最优的 $\lambda$ 值，让非负的非正态数据变成正态数据：

$$
y' = 
\begin{cases}
\ln y  & \lambda = 0 \\
(y^{\lambda} - 1)/ \lambda  & \lambda \neq 0
\end{cases}
$$

```{r}
pop = po("boxcox")
pop$train(list(task))[[1]]$data()
```

若数据包含 0 或负数，则 Box-Cox 变换不再适用，可以改用同样原理的 Yeo-Johnson 变换：

$$
y' = 
\begin{cases}
\ln (y+1)                                 & \lambda = 0, y \geq 0\\
\frac{(y + 1)^{\lambda} - 1}{\lambda}     & \lambda \neq 0, y \geq 0\\
- \ln (1-y)                               & \lambda = 2, y < 0\\
\frac{(1-y)^{2-\lambda} - 1}{\lambda - 2} & \lambda \neq 2, y < 0
\end{cases}
$$

```{r}
pop = po("yeojohnson")
pop$train(list(task))[[1]]$data()
```

#### 连续变量分箱

在统计和机器学习中，有时需要将连续变量转化为离散变量，称为连续变量离散化或分箱，常用于银行风控建模，特别是线性回归或 Logistic 回归模型。

分箱的好处有：

1. 使得结果更便于分析和解释。比如，年龄从中年到老年，患高血压比例增加25%，而年龄每增加一岁，患高血压比例不一定有显著变化；

1. 简化模型，将自变量与因变量间非线性的潜在的关系，转化为简单的线性关系。当然，分箱也可能带来问题：简化的模型关系可能与潜在的模型关系不一致（甚至发现的是错误的模型关系）、删除数据中的细微差别、切分点可能没有实际意义。

```{r}
# 等宽分箱
pop = po("histbin", param_vals = list(breaks = 4))
pop$train(list(task))[[1]]$data()
```

```{r}
# 分位数分箱
pop = po("quantilebin", numsplits = 4)
pop$train(list(task))[[1]]$data()
```

### 特征降维

有时数据集可能包含过多特征，甚至是冗余特征，可以用降维技术进压缩特征，但通常会降低模型性能。

#### PCA

$n$ 个特征，若转化为 $n$ 个主成分，则会保留原始数据的 100% 信息，但这就失去了降维的意义。所以1一般是只选择前若干个主成分，一般原则是选择至保留 85% 以上信息的主成分。

```{r}
pop = po("pca", rank. = 2)
pop$train(list(task))[[1]]$data()
```

#### 核 PCA

PCA 适用于数据的线性降维。而核主成分分析（Kernel PCA, KPCA）可实现数据的非线性降维，用于处理线性不可分的数据集。

KPCA 的大致思路是：对于输入空间中的矩阵 $X$，先用一个非线性映射把 $X$ 中的所有样本映射到一个高维甚至是无穷维的特征空间（使其线性可分），然后在这个高维空间进行 PCA 降维。

#### ICA：独立成分分析

独立成分分析，提取统计意义上的独立成分：

```{r}
pop = po("ica", n.comp = 3)
pop$train(list(task))[[1]]$data()
```

#### NMF：非负矩阵分解

对于任意给定的非负矩阵 $V$，NMF 算法能够寻找到非负矩阵 $W$ 和非负矩阵 $H$，使它们的积为矩阵。非负矩阵分解的方法在保证矩阵的非负性的同时能够减少数据量，相当于把 $n$ 维的数据降维到 $r$ 维。

```{r}
# BiocManager::install("Biobase")
pop = po("nmf", rank = 3)
pop$train(list(task))[[1]]$data()
```

#### 剔除常量特征

从任务中剔除常量特征。对于每个特征，计算不同于其众数值的比例。所有比例低于可设置阈值的特征都会从任务中剔除。缺失值可以被忽略，也可以视为与非缺失值不同的常规值。

```{r}
data = data.table(y = runif(10), a = 1:10, b = rep(1, 10), 
                  c = rep(1:2, each = 5))
task_ex = as_task_regr(data, target = "y")
po = po("removeconstants")  # 剔除常量特征 b
po$train(list(task_ex))[[1]]$data()
```

### 分类特征

#### 因子折叠

管道运算“collapsefactors”，对因子或有序因子进行折叠，折叠训练样本中最少的水平，直到剩下 `target_level_count` 个水平。然而，那些出现率高于 `no_collapse_above_prevalence` 的水平会被保留。对于因子变量，它们被折叠到下一个更大的水平，对于有序变量，稀有变量被折叠到相邻的类别，以样本数较少者为准。

训练集中没有出现的水平在预测集中不会被使用，因此经常与因子修正结合起来使用。

```{r}
dat = tibble(color = factor(starwars$skin_color), y = 1)
dat %>% count(color)
```

```{r}
task = as_task_regr(dat, target = "y")
poc = po("collapsefactors", target_level_count = 5)
poc$train(list(task))[[1]]$data() %>% count(color)
```

#### 因子修正

管道运算“fixfactors”（参数：`droplevels = TRUE`）确保预测过程中的因子水平与训练过程中的相同；可能会在之前丢弃空的训练因子水平。

注意，如果发现未见过的因子水平，这可能会在预测期间引入缺失值。

```{r}
dattrain = data.table(
  a = factor(c("a", "b", "c", NA), levels = letters),
  b = ordered(c("a", "b", "c", NA)),
  target = 1:4
)
dattrain
```

```{r}
dattest = data.table(
  a = factor(c("a", "b", "c", "d"), levels = letters[10:1]),
  b = ordered(c("a", "b", "c", "d")),
  target = 1:4
)
dattest
```

```{r}
task_train = as_task_regr(dattrain, "target")
task_test = as_task_regr(dattest, "target")
po = po("fixfactors")
po$train(list(task_train))
```

```{r}
po$predict(list(task_test))[[1]]$data()
```

#### 因子编码

对因子、有序因子、字符特征进行编码。参数 `method` 指定编码方法：

- “one-hot”：独热编码；

- “treatment”：虚拟编码，创建 $n-1$ 列，留出每个因子变量的第一个因子水平（见`stats::conc.treatment()`）；

- “helmert”：根据 Helmert 对比度创建列（见`stats::conc.helmert()`）；

- “poly”：根据正交多项式创建对比列（见`stats::conc.poly()`）；

- “sum”：创建对比度相加为零的列，（见`stats::conc.sum()`）

新创建的列按模式 `[column-name].[x]` 命名，其中 `x` 是 ”one-hot” 和 ”treatment” 编码的各自因子水平，否则是一个整数序列。

```{r}
task = tsk("penguins")
poe = po("encode", method = "one-hot")   # 独热编码
poe$train(list(task))[[1]]$data()[, 7:11]
```

```{r}
poe = po("encode", method = "treatment")  # 虚拟编码
poe$train(list(task))[[1]]$data()[, 7:9]
```

```{r}
poe = po("encode", method = "helmert")  # Helmert 编码
poe$train(list(task))[[1]]$data()[, 7:9]
```

```{r}
poe = po("encode", method = "poly")  # 多项编码
poe$train(list(task))[[1]]$data()[, 7:9]
```

```{r}
poe = po("encode", method = "sum")  # sum 编码
poe$train(list(task))[[1]]$data()[, 7:9]
```

::: {.callout-warning}
以下内容暂缓：

- 效应编码

- 日期时间特征

- 文本特征
:::

### 处理不均衡数据

通过采样对任务进行类平衡，可能有利于对不平衡的训练数据进行分类，采样只发生在训练阶段。

#### 欠采样与过采样

欠采样：只保留多数类的一部分行；过采样：对少数类进行超量采样（重复数据点）。

PipeOp 名字为 ”classbalancing”，参数：

- ratio：相对于 `$reference` 值，要保留的类的行数的比率，默认为 1；

- reference：`$ratio` 的值是根据什么来衡量的。可以是 “all”（默认值，所有类的平均实例数），“major”（拥有最多实例的类的实例数），“minor”（拥有最少实例的类的实例数），“nonmajor”（除主要类外所有类的平均实例数），“nonminor”（除次要类外所有类的平均实例数），以及 “one”（`$ratio` 决定每个类的实例数）；

- adjust：哪些类要向上/向下采样。可以是 “all”（默认）、“major”、“minor”、“nonmajor”、“nonminor”、“upsample”（只过采样）和“downsample”（只欠采样）；

- shuffle：是否对结果任务的行进行洗牌，默认为 `TRUE`。如果数据被过采样且 `shuffle = FALSE`，结果任务将以原始顺序显示原始行（在欠采样中不被移除），然后是所有新添加的行，按目标类别排序。

过/欠采样的过程如下：

- 首先，计算” 目标类计数”，方法是取 `reference` 参数所指示的所有类的平均数（例如，如果 reference 参数是 ”nonmajor”：所有不是 “major” 类的类的平均数，即拥有最多样本的类），然后将其与比率参数的值相乘。如果 reference 是 ”one”，那么“目标类计数” 就是比率的值（即 1 * 比率）。

- 然后，对于每个被 `adjust` 参数引用的类（例如，若调整是“nonminor”：每个不是样本最少的类），PipeOpClassBalancing 要么抛出样本（欠采样），要么增加与随机选择的样本相等的额外行（过采样），直到这些类的样本数等于“目标类计数”。

- 使用 `task$filter()` 来删除行。当在过采样过程中添加相同的行时，那么由于[混淆]，不能使用 `task$row_roles$use` 来复制行；而是使用 `task$rbind()` 函数，并附加一个新的 data.table，其中包含所有被复制的行，其次数正好与被添加的行相同。

```{r}
task = tsk("german_credit")
table(task$truth())
```

```{r}
## 欠采样
opb_down = po("classbalancing", reference = "minor", adjust = "major")
# 默认ratio = 1, 若ratio = 2, 结果是600 good, 300 bad
result = opb_down$train(list(task))[[1]]
table(result$truth())
```

```{r}
## 过采样
opb_up = po("classbalancing", reference = "major", adjust = "minor")
# 默认ratio = 1, 若ratio = 2, 结果是700 good, 1400 bad
result = opb_up$train(list(task))[[1]]
table(result$truth())
```

#### SMOTE 法

用 SMOTE 算法创建少数类别的合成观测，生成一个更平衡的数据集。该算法为每个少数类观测取样，基于该观测的 $K$ 个最近邻居生成新观测。它只能应用于具有纯数值特征的任务。详见 `smotefamily::SMOTE`。

PipeOp 名字：“smote”，其参数（具体参阅 `SMOTE()`）：

- K：用于抽取新值的近邻的数量；

- dup_size：合成的少数实例在原始多数实例数量上的期望次数。

```{r}
# 只支持double 型特征, 需安装 smotefamily 包
pop = po("colapply", applicator = as.numeric,
         affect_columns = selector_type("integer")) %>>%
  po("encodeimpact") %>>%
  po("smote", K = 5, dup_size = 1)  # 少数类增加 1 倍
result = pop$train(task)[[1]]
table(result$truth())
```

### 目标变换

为了提高预测能力，对于异方差或右偏的因变量数据，经常需要做取对数变换、或 Box-Cox 变换，以变成正态数据，再进行回归建模，预测值要回到原数据量级，还要做逆变换。

mlr3pipelines 包还提供了目标变换管道，将目标变换及其逆变换，都封装到图学习器，这样就省了手动做两次变换，还能整体进行调参或基准测试。

对于方差逐渐变大的异方差的时间序列数据，或右偏分布的数据，可以尝试做对数变换或开平方变换，以稳定方差和变成正态分布。

对数变换特别有用，因为具有可解释性：**对数值的变化是原始尺度上的相对（百分比）变化**。若使用以 10 为底的对数，则对数刻度上每增加 1 对应原始刻度上的乘以 10。

注意，原始数据若存在零或负，则不能取对数或开根号，解决办法是做平移：$a = \max \{ 0, -\min \{x_i\} + \varepsilon \}$

```{r}
task = tsk("mtcars")
learner = lrn("regr.lm")
g_ppl = ppl("targettrafo", graph = learner)
g_ppl$param_set$values$targetmutate.trafo = function(x) log(x)
g_ppl$param_set$values$targetmutate.inverter = function(x) list(response = exp(x$response))
g_ppl$plot()
```

```{r}
gl = as_learner(g_ppl)
gl$train(task)
gl$predict(task)
```

# 嵌套重抽样

构建模型，是如何从一组潜在的候选模型（如不同的算法，不同的超参数，不同的特征子集）中选择最佳模型。在构建模型过程中所使用的重抽样划分，不应该原样用来评估最终选择模型的性能。

通过在相同的测试集或相同的 CV 划分上反复评估学习器，测试集的信息会“泄露”到评估中，导致最终的性能估计偏于乐观。

模型构建的所有部分（包括模型选择、预处理）都应该纳入到训练数据的模型寻找过程中。测试集应该只使用一次，测试集只有在模型完全训练好之后才能被使用，例如已确定好了超参数。这样从测试集获得的性能才是真实性能的无偏估计。

对于本身需要重抽样的步骤（如超参数调参），这需要两个嵌套的重抽样循环，即内层调参和外层评估都需要重抽样策略。

下面从实例来看：

1. 使用3 折交叉验证来获得不同的非测试集和测试集（外层重抽样）

2. 在非测试集上使用 4 折交叉验证来获得不同的内层训练集和内层测试集（内层重抽样）

3. 使用内层数据划分来做超参数调参

4. 使用经过内层重抽样调参的最优超参数在外层非测试集上拟合学习器

5. 评估学习器在外层测试集上的性能

6. 对外层重抽样的三折中的每一折重复执行 2 - 5

7. 三个性能值被汇总为一个无偏的性能估计

过程看起来复杂，实现非常简单，只需要将内层重抽样放在自动调参器，外层重抽样放在 `resample()` 中，其中的学习器换成自动调参器。

```{r}
#| results: hide
# 内层重抽样（超参数调参）
at = auto_tuner(
  tuner = tnr("grid_search", resolution = 10),
  learner = lrn("classif.rpart", cp = to_tune(lower = .001, upper = .1)),
  resampling = rsmp("cv", folds = 4),
  measure = msr("classif.acc"),
  term_evals = 5
)

# 外层重抽样
task = tsk("pima")
outer_resampling = rsmp("cv", folds = 3)
rr = resample(task, at, outer_resampling, store_models = TRUE)
```

```{r}
# 提取内层重抽样结果，查看最优的超参数
extract_inner_tuning_results(rr)
```

```{r}
# 查看外层重抽样的每次结果
rr$score()
```

```{r}
# 查看外层重抽样的平均模型性能
rr$aggregate()
```

```{r}
#| results: hide
# 用全部数据自动调参，预测新数据
at$train(task)
dat = task$data()[1:5, -1]
at$predict_newdata(dat)
```

```{r}
#| eval: false
# 若改用tune_nested() 函数，嵌套重抽样过程还可以进一步简化
rr = tune_nested(
  tuner = tnr("grid_search", resolution = 10),
  task = task,
  learner = lrn("classif.rpart", cp = to_tune(lower = .001, upper = .1)),
  inner_resampling = rsmp("cv", folds = 4),
  outer_resampling = rsmp("cv", folds = 3),
  measure = msr("classif.acc"),
  term_evals = 5
)
```

# 超参数调参

机器学习的模型参数是模型的一阶（直接）参数，是训练模型时用梯度下降法寻优的参数，比如正则化回归模型的回归系数；而超参数是模型的二阶参数，需要事先设定为某值，才能开始训练一阶模型参数，比如正则化回归模型的惩罚参数、KNN 的邻居数等。

超参数会对所训练模型的性能产生重大影响，所以不能是随便或凭经验随便指定，而是需要设定很多种备选配置，从中选出让模型性能最优的超参数配置，这就是**超参数调参**。

超参数调参是用 mlr3tuning 包实现，是一项多方联动的系统工作，需要设定：搜索空间、学习器、任务、重抽样策略、模型性能度量指标、终止条件。

```{r}
learner = lrn("classif.svm")
learner$param_set  # 查看学习器的超参数
```

## 独立调参

适合传统的机器学习套路：将数据集按留出法重抽样划分为训练集和测试集，在训练集上做内层重抽样，多次“训练集拟合模型+ 验证集评估性能”，根据平均性能选出最优超参数。

1. 选取任务，划分数据集，将测试集索引设置为留出

```{r}
task = tsk("iris")
split = partition(task, ratio = .8)
task$set_row_roles(split$test, "holdout")
```

2. 选择学习器，同时指定部分超参数，需要调参的超参数用 `to_tune()` 设定搜索范围：

```{r}
learner = lrn("classif.svm", type = "C-classification", kernel = "radial",
              cost = to_tune(.1, 10), gamma = to_tune(0, 5))
```

3. 用 `tune()` 对学习器做超参数调参，需要设定（有些是可选）：

```{r}
#| results: hide
instance = tune(
  tuner = tnr("grid_search", batch_size = 5),
  task = task,
  learner = learner,
  resampling = rsmp("cv", folds = 5),
  measures = msr("classif.ce")
)
```

4. 提取超参数调参结果

```{r}
instance$result
```

```{r}
#| eval: false
instance$archive  # 调参档案
```

```{r}
autoplot(instance, type = "surface", cols_x = c("cost", "gamma"))
```

5. 用最优超参数更新学习器参数，在训练集上训练模型，在测试集上做预测

```{r}
learner$param_set$values = instance$result_learner_param_vals
learner$train(task)
predictions = learner$predict(task, row_ids = split$test)
predictions
```

## 自动调参器

上述独立调参有两个不足：

- 调参得到的最优超参数需要手动更新到学习器；

- 不方便实现嵌套重抽样。

自动调参器可以弥补上述不足，AutoTuner 是将超参数调参与学习器封装到一起，能实现自动调参的过程，并且可以像其他学习器一样使用。

1. 创建任务、选择学习器

```{r}
task = tsk("iris")
learner = lrn("classif.svm", type = "C-classification", kernel = "radial")
```

2. 用 `ps()` 生成搜索空间，用 `auto_tuner()` 创建自动调参器，注意：不需要提供任务，其它参数与独立调参的 `tune()` 基本一样：

```{r}
search_space = ps(cost = p_dbl(0.1, 10), gamma = p_int(0, 5))
at = auto_tuner(
  tuner = tnr("grid_search"),
  learner = learner,
  search_space = search_space,
  resampling = rsmp("cv", folds = 5),
  measure = msr("classif.ce")
)
```

3. 外层采用 4 折交叉验证重抽样，设置 `store_models = TRUE` 保存每次的模型：

```{r}
#| results: hide
rr = resample(task, at, rsmp("cv", folds = 4), store_models = TRUE)
```

这就是执行嵌套重抽样，外层（整体拟合模型+ 评估）循环 4 次，每次内层（超参数调参）循环 5 次。

4. 查看结果：

```{r}
rr$aggregate()  # 总的平均模型性能, 也可以提供其它度量
rr$score()      # 外层 4 次迭代的每次结果
extract_inner_tuning_results(rr)  # 内层每次的调参结果
```

```{r}
#| eval: false
extract_inner_tuning_archives(rr) # 内层调参档案
```

5. 在全部数据上自动调参，预测新数据

```{r}
#| results: hide
at$train(task)
dat = task$data()[1:5, -1]
at$predict_newdata(dat)
```

注：调参结束后，也可以取出最优超参数，更新学习器参数：

```{r}
learner$param_set$values = at$tuning_result$learner_param_vals[[1]]
```

或者按最优超参数重新创建学习器。

另外，上述的“自动调参+ 外层重抽样”，若改用嵌套调参 `tune_nested()` 实现，代码会更加简洁：

```{r}
#| eval: false
rr = tune_nested(
  tuner = tnr("grid_search"),
  task = task,
  learner = learner,
  search_space = search_space,
  inner_resampling = rsmp("cv", folds = 5),
  outer_resampling = rsmp("cv", folds = 4),
  measure = msr("classif.ce")
)
```

## 定制搜索空间

用 `ps()` 创建搜索空间，它支持 5 种超参数类型构建：

- `p_dbl()`：实值型（double）

- `p_int()`：整数型

- `p_fct()`：离散型（factor）

- `p_lgl()`：逻辑性

- `p_uty()`：Untyped

```{r}
search_space = ps(cost = p_dbl(.1, 10),
                  kernel = p_fct(c("polynomial", "radial")))
```

### 变换

对于数值型超参数，在调参时经常希望前期的点比后期的点更密集一些。这可以通过对数-指数变换来实现，这也适用于大范围搜索空间。

```{r}
tibble(x = 1:20,
       y = exp(seq(log(3), log(50), length.out = 20))) %>% 
  ggplot(aes(x, y)) +
  geom_point()
```

当 $x$ 均匀变化时，变换后作为超参数能前密后疏。

```{r}
search_space = ps(
  cost = p_dbl(log(.1), log(10), trafo = \(x) exp(x)),
  kernel = p_fct(c("polynomial", "radial"))
)
```

```{r}
# 查看调参网格
params = generate_design_grid(search_space, resolution = 5)
params
```

### 依赖关系

有些超参数只有在另一个参数取某些值时才有意义。例如，支持向量机有一个 `degree` 参数，只有在 `kernel` 为 ”polynomial” 时才有效。这可以用 `depends` 参数来指定：

```{r}
search_space = ps(
  cost = p_dbl(log(.1), log(10), trafo = \(x) exp(x)),
  kernel = p_fct(c("polynomial", "radial")),
  degree = p_int(1, 3, depends = kernel == "polynomial")
)
```

### 图学习器调参

图学习器一旦成功创建，就可以像普通学习器一样使用，超参数调参时，原算法的超参数名字都自动带了学习器名字前缀，另外还可以对管道参数调参。

```{r}
task = tsk("pima")
```

该任务包含缺失值，还有若干因子特征，都需要做预处理：插补和特征工程。

```{r}
prep = gunion(list(
  po("imputehist"),
  po("missind", affect_columns = selector_type(c("numeric", "integer")))
)) %>>%
  po("featureunion") %>>%
  po("encode") %>>%
  po("removeconstants")
```

选择三个学习器：KNN、SVM、Ranger 作为三分支分别拟合模型，再合并分支保证是一个输出结果：

```{r}
learners = list(
  knn = lrn("classif.kknn", id = "kknn"),
  svm = lrn("classif.svm", id = "svm", type = "C-classification"),
  rf = lrn("classif.ranger", id = "ranger")
)
graph = ppl("branch", learners)
```

将预处理图和算法图连接得到整个图，并可视化图：

```{r}
graph = prep %>>% graph
graph$plot()
```

转化为图学习器，查看其超参数：

```{r}
glearner = as_learner(graph)
glearner$param_set
```

可见，所有超参数都多了学习器或管道名字前缀，比如 kknn.k 是KNN 学习器的邻居数参数 k。

嵌套重抽样超参数调参，与前文语法一样，为了加速计算，启动并行：

```{r}
future::plan("multicore")
```

设置搜索空间，用 `tune_nested()` 做嵌套调参：

```{r}
#| results: hide
search_space = ps(
  branch.selection = p_fct(c("kknn", "svm", "ranger")),
  kknn.k = p_int(3, 50, logscale = TRUE,
                 depends = branch.selection == "kknn"),
  svm.cost = p_dbl(-1, 1, trafo = \(x) 10^x,
                   depends = branch.selection == "svm"),
  ranger.mtry = p_int(1, 8,
                      depends = branch.selection == "ranger")
)
rr = tune_nested(
  tuner = tnr("random_search"),
  task = task,
  learner = glearner,
  inner_resampling = rsmp("cv", folds = 3),
  outer_resampling = rsmp("cv", folds = 4),
  measure = msr("classif.ce"),
  term_evals = 10
)
```

查看结果：

```{r}
rr$aggregate()    # 总的平均模型性能
rr$score()        # 外层 4 次迭代每次的模型性能
```

# 特征选择

当数据集包含很多特征时，只提取最重要的部分特征来建模，称为**特征选择**。特征选择可以增强模型的解释性、加速学习过程、改进学习器性能。

常用的特征选择方法有两种：过滤法、包装法。另外，有些学习器内部提供了选择有助于做预测的特征子集的方法，称为是嵌入法。

## 过滤法

过滤法，基于某种衡量特征重要度的指标（如相关系数），用外部算法计算变量的排名，只选用排名靠前的若干特征，用 mlr3filters 包实现。

### 基于重要度指标

过滤法给每个特征计算一个重要度指标值，基于此可以对特征进行排序，然后就可以选出特征子集。

```{r}
task = tsk("sonar")
filter = flt("auc")
as.data.table(filter$calculate(task))
```

### 基于学习器的变量重要度

有些学习器可以计算变量重要度，特别是基于树的模型，目前支持：

- "classif.featureless"，"regr.featureless"

- "classif.ranger"，"regr.ranger"

- "classif.rpart"，"regr.rpart"

- "classif.xgboost"，"regr.xgboost"

有些学习器需要在创建时” 激活” 其变量重要性度量。例如，通过 ranger 包来使用随机森林的 ”impurity” 度量：

```{r}
task = tsk("iris")
learner = lrn("classif.ranger", importance = "impurity")
filter = flt("importance", learner = learner)
filter$calculate(task)
as.data.table(filter)
```

使用上述特征选择可以对特征得分可视化，根据肘法确定保留特征数，然后用 `task$select()` 选择特征；也可以直接通过管道连接学习器构建图学习器：

```{r}
task = tsk("spam")
graph = po("filter", filter = flt("auc"), filter.frac = .5) %>>%
  po("learner", lrn("classif.rpart"))
graph$plot()
```

```{r}
learner = as_learner(graph)
rr = resample(task, learner, rsmp("cv", folds = 5))
rr$aggregate()
```

## 包装法

包装法，随机选择部分特征拟合模型并评估模型性能，通过交叉验证找到最佳的特征子集，用 mlr3fselect 包实现。

包装法特征选择，与超参数调参道理完全一样。

### 独立特征选择

适合传统的机器学习套路：将数据集按留出法重抽样划分为训练集和测试集，在训练集上做内层重抽样，多次“训练集拟合模型 + 验证集评估性能”，根据平均性能选出最优特征子集。

1. 选取任务，划分数据集，将测试集索引设置为留出

```{r}
task = tsk("pima")
split = partition(task, ratio = .8)
task$set_row_roles(split$test, "holdout")
```

2. 选择学习器

```{r}
learner = lrn("classif.rpart")
```

3. 用 `fselect()` 对学习器做特征选择

```{r}
#| results: hide
instance = fselect(
  fselector = fs("rfe"),
  task = task,
  learner = learner,
  resampling = rsmp("cv", folds = 4),
  measures = msr("classif.ce"),
  term_evals = 10,
  store_models = TRUE
)
```

查看特征选择结果：

```{r}
instance$result  # 最佳特征子集
```

```{r}
instance$archive  # 特征选择档案
```

4. 对任务选择特征自己，拟合最终模型

```{r}
task$select(instance$result_feature_set)
learner$train(task)
```

### 自动特征选择

上述独立特征选择有两个不足：

- 特征选择得到的最优特征子集需要手动更新到任务；

- 不方便实现嵌套重抽样。

自动特征选择器可以弥补上述不足，AutoFSelector 是将特征选择与学习器封装到一起，能实现自动特征选择的过程，并且可以像其他学习器一样使用。

1. 创建任务、选择学习器

```{r}
task = tsk("pima")
learner = lrn("classif.rpart")
```

2. 用 `auto_fselector()` 创建自动特征选择器，注意：不需要提供任务，其它参数与独立特征选择的 `fselect()` 基本一样：

```{r}
afs = auto_fselector(
  fselector = fs("random_search", batch_size = 5),
  learner = lrn("classif.rpart"),
  resampling = rsmp("cv", folds = 4),
  measure = msr("classif.ce"),
  term_evals = 10
)
```

3. 外层采用 5 折交叉验证重抽样，设置 `store_models = TRUE` 保存每次的模型：

```{r}
#| results: hide
rr = resample(task, afs, rsmp("cv", folds = 5), store_models = TRUE)
```

查看结果：

```{r}
rr$aggregate()  # 总的平均模型性能, 也可以提供其它度量
```

注：若外层重抽样不想做这么多次，可直接用留出重抽样，或者划分训练集测试集，然后将 afs 当学习器使用。

另外，上述的“自动特征选择+ 外层重抽样”，若改用嵌套特征选择 `fselect_nested()` 实现，代码会更加简洁：

```{r}
#| eval: false
rr = fselect_nested(
  fselector = fs("random_search", batch_size = 5),
  task = tsk("pima"),
  learner = lrn("classif.rpart"),
  inner_resampling = rsmp("cv", folds = 4),
  outer_resampling = rsmp("cv", folds = 5),
  measure = msr("classif.ce"),
  term_evals = 10
)
```

# 模型解释

机器学习模型预测性能强大，但天生不好解释。R 有两个通用框架致力于机器学习模型的解释（支持但不属于 mlr3verse）：iml 包和 DALEX 包。

## iml 包

iml 包提供了分析任何黑盒机器学习模型的工具。

```{r}
dat = tsk("penguins")$data() %>% na.omit()
task = as_task_classif(dat, target = "species")
learner = lrn("classif.ranger", predict_type = "prob")
learner$train(task)
learner$model
```

iml 包中的所有解释方法都需要机器学习模型和数据封装在 Predictor 对象中。

```{r}
library(iml)
```

### 特征效应

特征效应（FeatureEffects），是计算所有给定特征对模型预测的影响。实现了不同的方法：累积局部效应（ALE）图，部分依赖图（PDP）和个体条件期望（ICE）曲线。

```{r}
mod = Predictor$new(learner, data = dat, y = "species")
effect = FeatureEffects$new(mod)
effect$plot(features = c("bill_length", "bill_depth",
                         "flipper_length", "body_mass", "year"))
```

可见，除了年份，所有的特征都提供了有意义的可解释信息。

### 夏普利值

计算具有夏普利（Shapley）值的单个观测的特征贡献（一种来自合作博弈理论的方法），即单个数据点的特征值是如何影响其预测的。

```{r}
mod = Predictor$new(learner, data = dat, y = "species")
shapley = Shapley$new(mod, x.interest = dat[1,])
plot(shapley)
```

$\phi$值给出了纵轴上数值的概率增加或减少，例如，如果 `bill_depth = 18.7`，那么一只企鹅是 Gentoo 企鹅的可能性就比较小，而是 Adelie 企鹅的可能性则比 Chinstrap 企鹅大得多。

### 特征重要度

根据特征重排后模型的预测误差的增量，计算特征的重要度。

```{r}
mod = Predictor$new(learner, data = dat, y = "species")
imp = FeatureImp$new(mod, loss = "ce")
imp$plot(features = c("bill_length", "bill_depth",
                      "flipper_length", "body_mass", "year"))
```

可见，bill_length 具有很高的特征重要度。

注：以上模型解释也可以分别在训练集和测试集上来做，以看对比效果。

## DALEX 包

DALEX 包可以透视任何模型并帮助探索和解释其行为，用各种方法帮助理解输入变量和模型输出之间的联系，有助于在单个观测以及整个数据特征的层面上解释模型。所有的模型解释器都是与模型无关的，可以在不同的模型之间进行比较。该包是 “DrWhy.AI” 可视化模型解释系列包的基石。

![模型解释方法分类](DALEX.jpg){#fig-DALEX}

以 fifa 数据，预测球员价值 value_eur 为例。

```{r}
#| message: false
library(DALEX)
library(DALEXtra)
fifa[, c("nationality", "overall", "potential", "wage_eur")] = NULL
fifa = fifa %>% mutate(across(everything(), .fns = as.numeric))
```

创建任务，选择包含 250 棵决策树的随机森林学习器，训练模型：

```{r}
task = as_task_regr(fifa, target = "value_eur")
ranger = lrn("regr.ranger", num.trees = 250)
ranger$train(task)
ranger$model
```

在开始解释模型行为之前，先创建一个解释器：

```{r}
ranger_exp = explain_mlr3(ranger, data = fifa, y = fifa$value_eur,
                          label = "Ranger RF")
```

### 特征层面的解释

`model_parts()` 函数基于排列组合的重要度来计算变量的重要度：

```{r}
fifa_vi = model_parts(ranger_exp)
head(fifa_vi)
```

```{r}
plot(fifa_vi, max_vars = 12, show_boxplots = FALSE)
```

一旦知道了哪些变量是最重要的，就可以使用部分依赖图来显示模型（平均而言）如何随着选定变量的变化而变化。在本例中，它们显示了特定变量和球员价值之间的平均关系。

```{r}
vars = c("age", "movement_reactions", "skill_ball_control", "skill_dribbling")
fifa_pd = model_profile(ranger_exp, variables = vars)$agr_profiles
fifa_pd
```

```{r}
plot(fifa_pd)
```

可见，大多数球员特征的一般趋势是相同的。技能越高，球员的价值就越高，只有一个变量例外——年龄。

### 观测层面的解释

探索模型在单个观测/球员身上的表现，下面以 Cristiano Ronaldo 为例。

```{r}
ronaldo = fifa["Cristiano Ronaldo", ]
ronaldo_bd = predict_parts(ranger_exp, new_observation = ronaldo)
head(ronaldo_bd)
```

```{r}
plot(ronaldo_bd)
```

上图展示了各变量对最终预测的贡献估计。Ronaldo 是一名前锋，因此影响他身价的特征是那些与进攻有关的特征，如 attacking_ volleys 或 skill_dribbling。唯一具有负向影响的特征是 age。

检查模型的局部行为的另一种方法是用 SHapley Additive exPlanations（SHAP）。它在局部显示了变量对单一观测的贡献。

```{r}
ronaldo_shap = predict_parts(ranger_exp, new_observation = ronaldo,
                             type = "shap")
plot(ronaldo_shap)
```

特征效应有部分依赖关系图，对于观测也有相应的版本：Ceteris Paribus。它显示了当只改变一个变量而其他变量保持不变时，模型对观测的反应（蓝点代表原始值）：

```{r}
ronaldo_cp = predict_profile(ranger_exp, ronaldo, variables = vars)
plot(ronaldo_cp, variables = vars)
```

更多机器学习模型解释理论方法请参阅 Interpretable Machine Learning: A Guide for Making Black
Box Models Explainable.
