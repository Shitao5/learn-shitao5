{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"动手学深度学习\"\n",
    "date: \"2023-06-20\"\n",
    "date-modified: \"2023-07-29\"\n",
    "image: \"front.png\"\n",
    "categories: [\"Deep Learning\", \"Python\"]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "::: {.callout-note title='Progress'}\n",
    "Learning Progress: 17.5%.\n",
    ":::\n",
    "\n",
    "::: {.callout-tip title=\"Learning Source\"}\n",
    "- <https://zh-v2.d2l.ai/index.html>\n",
    ":::"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前言 {.unnumbered}\n",
    "\n",
    "- 任何一种计算技术要想发挥其全部影响力，都必须得到充分的理解、充分的文档记录，并得到成熟的、维护良好的工具的支持。关键思想应该被清楚地提炼出来，尽可能减少需要让新的从业者跟上时代的入门时间。成熟的库应该自动化常见的任务，示例代码应该使从业者可以轻松地修改、应用和扩展常见的应用程序，以满足他们的需求。\n",
    "\n",
    "# 引言\n",
    "\n",
    "- 通常，即使我们不知道怎样明确地告诉计算机如何从输入映射到输出，大脑仍然能够自己执行认知功能。 换句话说，即使我们不知道如何编写计算机程序来识别“Alexa”这个词，大脑自己也能够识别它。 有了这一能力，我们就可以收集一个包含大量音频样本的*数据集*（dataset），并对包含和不包含唤醒词的样本进行标记。 利用机器学习算法，我们不需要设计一个“明确地”识别唤醒词的系统。 相反，我们只需要定义一个灵活的程序算法，其输出由许多*参数*（parameter）决定，然后使用数据集来确定当下的“最佳参数集”，这些参数通过某种性能度量方式来达到完成任务的最佳性能。  \n",
    "那么到底什么是参数呢？ 参数可以被看作旋钮，旋钮的转动可以调整程序的行为。 任一调整参数后的程序被称为*模型*（model）。 通过操作参数而生成的所有不同程序（输入-输出映射）的集合称为“模型族”。 使用数据集来选择参数的元程序被称为*学习算法*（learning algorithm）。\n",
    "\n",
    "- 深度学习与经典方法的区别主要在于：前者关注的功能强大的模型，这些模型由神经网络错综复杂的交织在一起，包含层层数据转换，因此被称为*深度学习*（deep learning）\n",
    "\n",
    "- 当一个模型在训练集上表现良好，但不能推广到测试集时，这个模型被称为*过拟合*（overfitting）的。 就像在现实生活中，尽管模拟考试考得很好，真正的考试不一定百发百中。\n",
    "\n",
    "- 虽然监督学习只是几大类机器学习问题之一，但是在工业中，大部分机器学习的成功应用都使用了监督学习。 这是因为在一定程度上，许多重要的任务可以清晰地描述为，在给定一组特定的可用数据的情况下，估计未知事物的概率。\n",
    "\n",
    "- 回归是训练一个回归函数来输出一个数值； 分类是训练一个分类器来输出预测的类别。\n",
    "\n",
    "- 分类可能变得比二项分类、多项分类复杂得多。 例如，有一些分类任务的变体可以用于寻找层次结构，层次结构假定在许多类之间存在某种关系。 因此，**并不是所有的错误都是均等的**。 人们宁愿错误地分入一个相关的类别，也不愿错误地分入一个遥远的类别，这通常被称为*层次分类*(hierarchical classification)。\n",
    "\n",
    "- 学习预测不相互排斥的类别的问题称为*多标签分类*（multi-label classification）。\n",
    "\n",
    "- 无监督学习\n",
    "    - 聚类（clustering）问题：没有标签的情况下，我们是否能给数据分类呢？\n",
    "    - 主成分分析（principal component analysis）问题：我们能否找到少量的参数来准确地捕捉数据的线性相关属性？\n",
    "    - 因果关系（causality）和概率图模型（probabilistic graphical models）问题：我们能否描述观察到的许多数据的根本原因？\n",
    "    - 生成对抗性网络（generative adversarial networks）：为我们提供一种合成数据的方法，甚至像图像和音频这样复杂的非结构化数据。\n",
    "\n",
    "- 简单的离线学习有它的魅力。 好的一面是，我们可以孤立地进行模式识别，而不必分心于其他问题。 但缺点是，解决的问题相当有限。 这时我们可能会期望人工智能不仅能够做出预测，而且能够与真实环境互动。 **与预测不同，“与真实环境互动”实际上会影响环境**。 这里的人工智能是“智能代理”，而不仅是“预测模型”。 因此，我们必须考虑到它的行为可能会影响未来的观察结果。\n",
    "\n",
    "- 在强化学习问题中，智能体（agent）在一系列的时间步骤上与环境交互。 在每个特定时间点，智能体从环境接收一些观察（observation），并且必须选择一个动作（action），然后通过某种机制（有时称为执行器）将其传输回环境，最后智能体从环境中获得奖励（reward）。 此后新一轮循环开始，智能体接收后续观察，并选择后续操作，依此类推。 强化学习的过程在 @fig-rl-environment 中进行了说明。 请注意，**强化学习的目标是产生一个好的策略（policy）**。 强化学习智能体选择的“动作”受策略控制，即一个从环境观察映射到行动的功能。\n",
    "\n",
    "![强化学习和环境之间的相互作用](img/rl-environment.svg){width=60% #fig-rl-environment}\n",
    "\n",
    "- 当环境可被完全观察到时，强化学习问题被称为*马尔可夫决策过程*（markov decision process）。 当状态不依赖于之前的操作时，我们称该问题为*上下文赌博机*（contextual bandit problem）。 当没有状态，只有一组最初未知回报的可用动作时，这个问题就是经典的*多臂赌博机*（multi-armed bandit problem）。\n",
    "\n",
    "- 如前所述，机器学习可以使用数据来学习输入和输出之间的转换，例如在语音识别中将音频转换为文本。 在这样做时，通常需要以适合算法的方式表示数据，以便将这种表示转换为输出。 深度学习是“深度”的，模型学习了许多“层”的转换，每一层提供一个层次的表示。 例如，靠近输入的层可以表示数据的低级细节，而接近分类输出的层可以表示用于区分的更抽象的概念。 由于**表示学习（representation learning）目的是寻找表示本身，因此深度学习可以称为“多级表示学习”**。\n",
    "\n",
    "- 事实证明，这些多层模型能够以以前的工具所不能的方式处理低级的感知数据。 毋庸置疑，深度学习方法中最显著的共同点是使用**端到端训练**。 也就是说，与其基于单独调整的组件组装系统，不如构建系统，然后联合调整它们的性能。 例如，在计算机视觉中，科学家们习惯于将特征工程的过程与建立机器学习模型的过程分开。 Canny边缘检测器 (Canny, 1987) 和SIFT特征提取器 (Lowe, 2004) 作为将图像映射到特征向量的算法，在过去的十年里占据了至高无上的地位。 在过去的日子里，将机器学习应用于这些问题的关键部分是提出人工设计的特征工程方法，将数据转换为某种适合于浅层模型的形式。 然而，与一个算法自动执行的数百万个选择相比，人类通过特征工程所能完成的事情很少。 当深度学习开始时，这些特征抽取器被自动调整的滤波器所取代，产生了更高的精确度。  \n",
    "因此，深度学习的一个关键优势是它不仅取代了传统学习管道末端的浅层模型，而且还取代了劳动密集型的特征工程过程。 此外，通过取代大部分特定领域的预处理，深度学习消除了以前分隔计算机视觉、语音识别、自然语言处理、医学信息学和其他应用领域的许多界限，为解决各种问题提供了一套统一的工具。\n",
    "\n",
    "# 预备知识\n",
    "\n",
    "## 数据操作 {#sec-ndarray}\n",
    "\n",
    "### 入门"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:35.915445300Z",
     "start_time": "2023-07-31T11:37:19.019746600Z"
    }
   },
   "source": [
    "import torch"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量表示一个由数值组成的数组，这个数组可能有多个维度。 具有一个轴的张量对应数学上的向量（vector）； 具有两个轴的张量对应数学上的矩阵（matrix）； 具有两个轴以上的张量没有特殊的数学名称。\n",
    "\n",
    "除非额外指定，新的张量将存储在内存中，并采用基于CPU的计算。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:35.988797400Z",
     "start_time": "2023-07-31T11:37:35.919164600Z"
    }
   },
   "source": [
    "# 创建张量\n",
    "x = torch.arange(12)\n",
    "x"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:35.990135300Z",
     "start_time": "2023-07-31T11:37:35.954889100Z"
    }
   },
   "source": [
    "# 访问张量（沿每个轴的长度）的形状\n",
    "x.shape  "
   ],
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([12])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:35.990135300Z",
     "start_time": "2023-07-31T11:37:35.973004Z"
    }
   },
   "source": [
    "# 改变张量的形状\n",
    "X = x.reshape(3, 4)\n",
    "X"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要重点说明一下，虽然张量的形状发生了改变，但其元素值并没有变。 注意，通过改变张量的形状，张量的大小不会改变。\n",
    "\n",
    "此外，我们可以通过-1来调用自动计算出维度的功能。 即我们可以用`x.reshape(-1,4)`或`x.reshape(3,-1)`来取代`x.reshape(3,4)`。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:36.015836800Z",
     "start_time": "2023-07-31T11:37:35.986336300Z"
    }
   },
   "source": [
    "#| eval: false\n",
    "# 与 x.reshape(3, 4)效果一致\n",
    "x.reshape(-1, 4)\n",
    "x.reshape(3, -1)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:36.880327500Z",
     "start_time": "2023-07-31T11:37:35.998496Z"
    }
   },
   "source": [
    "# 创建形状为（2,3,4）的张量，其中所有元素都设置为0\n",
    "torch.zeros((2, 3, 4))"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:36.905984200Z",
     "start_time": "2023-07-31T11:37:36.039193700Z"
    }
   },
   "source": [
    "# 创建形状为（2,3,4）的张量，其中所有元素都设置为1\n",
    "torch.ones((2, 3, 4))"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]],\n\n        [[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:36.908063200Z",
     "start_time": "2023-07-31T11:37:36.056345300Z"
    }
   },
   "source": [
    "# 创建形状为（3,4）的张量，每个元素都从标准正态分布中随机采样\n",
    "torch.randn(3, 4)"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-1.1841, -1.5453,  0.7282,  1.2113],\n        [ 0.9225, -1.2678, -1.0426,  0.3869],\n        [ 0.5307,  0.3379,  1.4549,  1.1577]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:36.909222800Z",
     "start_time": "2023-07-31T11:37:36.070286Z"
    }
   },
   "source": [
    "# 通过 Python 列表为张量中的元素赋值\n",
    "torch.tensor([\n",
    "  [2, 1, 4, 3],\n",
    "  [1, 2, 3, 4],\n",
    "  [4, 3, 2, 1]\n",
    "])"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2, 1, 4, 3],\n        [1, 2, 3, 4],\n        [4, 3, 2, 1]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 运算符\n",
    "\n",
    "对于任意具有相同形状的张量， 常见的标准算术运算符（+、-、\\*、\\/和\\*\\*）都可以被升级为按元素运算。 我们可以在同一形状的任意两个张量上调用按元素操作。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:36.909222800Z",
     "start_time": "2023-07-31T11:37:36.083656400Z"
    }
   },
   "source": [
    "x = torch.tensor([1, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "\n",
    "# 使用逗号来表示一个具有5个元素的元组\n",
    "x + y, x - y, x * y, x / y, x ** y"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([ 3,  4,  6, 10]),\n tensor([-1,  0,  2,  6]),\n tensor([ 2,  4,  8, 16]),\n tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n tensor([ 1,  4, 16, 64]))"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“按元素”方式可以应用更多的计算，包括像求幂这样的一元运算符。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:36.946749100Z",
     "start_time": "2023-07-31T11:37:36.101051400Z"
    }
   },
   "source": [
    "torch.exp(x)"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以把多个张量连结（concatenate）在一起， 把它们端对端地叠起来形成一个更大的张量。 我们只需要提供张量列表，并给出沿哪个轴连结。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:36.947793400Z",
     "start_time": "2023-07-31T11:37:36.792209500Z"
    }
   },
   "source": [
    "X = torch.arange(12, dtype = torch.float32).reshape((3, 4))\n",
    "Y = torch.tensor([\n",
    "  [2.0, 1, 4, 3],\n",
    "  [1  , 2, 3, 4],\n",
    "  [4  , 3, 2, 1]\n",
    "])\n",
    "torch.cat((X, Y), dim = 0), torch.cat((X, Y), dim = 1)"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.],\n         [ 2.,  1.,  4.,  3.],\n         [ 1.,  2.,  3.,  4.],\n         [ 4.,  3.,  2.,  1.]]),\n tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过逻辑运算符构建二元张量："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:36.947793400Z",
     "start_time": "2023-07-31T11:37:36.792209500Z"
    }
   },
   "source": [
    "X == Y"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[False,  True, False,  True],\n        [False, False, False, False],\n        [False, False, False, False]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对张量中的所有元素进行求和，会产生一个单元素张量："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:36.948816600Z",
     "start_time": "2023-07-31T11:37:36.792209500Z"
    }
   },
   "source": [
    "X.sum()"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(66.)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 广播机制\n",
    "\n",
    "在上面的部分中，我们看到了如何在相同形状的两个张量上执行按元素操作。 在某些情况下，即使形状不同，我们仍然可以通过调用 *广播机制*（broadcasting mechanism）来执行按元素操作。 这种机制的工作方式如下：\n",
    "\n",
    "1. 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；\n",
    "1. 对生成的数组执行按元素操作。\n",
    "\n",
    "在大多数情况下，我们将沿着数组中长度为1的轴进行广播，如下例子："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:36.948816600Z",
     "start_time": "2023-07-31T11:37:36.792209500Z"
    }
   },
   "source": [
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "a, b"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0],\n         [1],\n         [2]]),\n tensor([[0, 1]]))"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于a和b分别是 $3\\times1$\n",
    "和 $1\\times2$\n",
    "矩阵，如果让它们相加，它们的形状不匹配。 我们将两个矩阵广播为一个更大的 $3\\times2$\n",
    "矩阵，如下所示：矩阵$\\mathbf{a}$将复制列， 矩阵$\\mathbf{b}$将复制行，然后再按元素相加。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:36.949817100Z",
     "start_time": "2023-07-31T11:37:36.792209500Z"
    }
   },
   "source": [
    "a + b"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 1],\n        [1, 2],\n        [2, 3]])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 索引和切片\n",
    "\n",
    "就像在任何其他Python数组中一样，张量中的元素可以通过索引访问。 与任何Python数组一样：第一个元素的索引是0，最后一个元素索引是-1； 可以指定范围以包含第一个元素和最后一个之前的元素。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:36.949817100Z",
     "start_time": "2023-07-31T11:37:36.793209900Z"
    }
   },
   "source": [
    "# 查看 X\n",
    "X"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.]])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:36.949817100Z",
     "start_time": "2023-07-31T11:37:36.793209900Z"
    }
   },
   "source": [
    "# 用[-1]选择最后一个元素，用[1:3]选择第二个和第三个元素\n",
    "X[-1], X[1:3]"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([ 8.,  9., 10., 11.]),\n tensor([[ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.]]))"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除读取外，我们还可以通过指定索引来将元素写入矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:36.949817100Z",
     "start_time": "2023-07-31T11:37:36.793209900Z"
    }
   },
   "source": [
    "X[1, 2] = 9\n",
    "X"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  9.,  7.],\n        [ 8.,  9., 10., 11.]])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们想为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值。 例如，`[0:2, :]`访问第1行和第2行，其中“`:`”代表沿轴1（列）的所有元素。 虽然我们讨论的是矩阵的索引，但这也适用于向量和超过2个维度的张量。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:36.950817Z",
     "start_time": "2023-07-31T11:37:36.793209900Z"
    }
   },
   "source": [
    "X[0:2, :] = 12\n",
    "X"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[12., 12., 12., 12.],\n        [12., 12., 12., 12.],\n        [ 8.,  9., 10., 11.]])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 节省内存\n",
    "\n",
    "运行一些操作可能会导致为新结果分配内存。 例如，如果我们用`Y = X + Y`，我们将取消引用`Y`指向的张量，而是指向新分配的内存处的张量。\n",
    "\n",
    "在下面的例子中，我们用Python的`id()`函数演示了这一点， 它给我们提供了内存中引用对象的确切地址。 运行`Y = Y + X`后，我们会发现`id(Y)`指向另一个位置。 这是因为Python首先计算`Y + X`，为结果分配新的内存，然后使`Y`指向内存中的这个新位置。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:36.950817Z",
     "start_time": "2023-07-31T11:37:36.793209900Z"
    }
   },
   "source": [
    "before = id(Y)\n",
    "Y = Y + X\n",
    "id(Y) == before"
   ],
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这可能是不可取的，原因有两个：\n",
    "\n",
    "1. 首先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新；\n",
    "1. 如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。\n",
    "\n",
    "幸运的是，执行原地操作非常简单。 我们可以使用切片表示法将操作的结果分配给先前分配的数组，例如`Y[:] = <expression>`。 为了说明这一点，我们首先创建一个新的矩阵`Z`，其形状与另一个`Y`相同， 使用`zeros_like`来分配一个全0的块。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:36.950817Z",
     "start_time": "2023-07-31T11:37:36.794656900Z"
    }
   },
   "source": [
    "Z = torch.zeros_like(Y)\n",
    "print('id(Z):', id(Z))\n",
    "Z[:] = X + Y\n",
    "print('id(Z):', id(Z))"
   ],
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 2205396151392\n",
      "id(Z): 2205396151392\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果在后续计算中没有重复使用`X`， 我们也可以使用`X[:] = X + Y`或`X += Y`来减少操作的内存开销。\n",
    "\n",
    "### 转换为其他 Python 对象\n",
    "\n",
    "将深度学习框架定义的张量转换为NumPy张量（ndarray）很容易，反之也同样容易。 torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:36.953044200Z",
     "start_time": "2023-07-31T11:37:36.794656900Z"
    }
   },
   "source": [
    "A = X.numpy()\n",
    "B = torch.tensor(A)\n",
    "type(A), type(B)"
   ],
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "(numpy.ndarray, torch.Tensor)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要将大小为1的张量转换为Python标量，我们可以调用`item`函数或Python的内置函数。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:36.953044200Z",
     "start_time": "2023-07-31T11:37:36.794656900Z"
    }
   },
   "source": [
    "a = torch.tensor([3.4])\n",
    "a, a.item(), float(a), int(a)"
   ],
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([3.4000]), 3.4000000953674316, 3.4000000953674316, 3)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "\n",
    "人工创建数据集："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:36.953044200Z",
     "start_time": "2023-07-31T11:37:36.794656900Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(os.path.join('.', 'data'), exist_ok = True)\n",
    "data_file = os.path.join('.', 'data', 'house_tiny.csv')\n",
    "with open(data_file, 'w') as f:\n",
    "  f.write('NumRooms,Allley,Price\\n') # 列名\n",
    "  f.write('NA,Pave,127500\\n')\n",
    "  f.write('2,NA,106000\\n')\n",
    "  f.write('4,NA,178100\\n')\n",
    "  f.write('NA,NA,140000\\n')"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 Pandas 的 `read_csv()` 函数读取："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:37.548008Z",
     "start_time": "2023-07-31T11:37:36.794656900Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(data_file)\n",
    "print(data)"
   ],
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Allley   Price\n",
      "0       NaN   Pave  127500\n",
      "1       2.0    NaN  106000\n",
      "2       4.0    NaN  178100\n",
      "3       NaN    NaN  140000\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，“`NaN`”项代表缺失值。 为了处理缺失的数据，典型的方法包括*插值法*和*删除法*， 其中插值法用一个替代值弥补缺失值，而删除法则直接忽略缺失值。 在这里，我们将考虑插值法。\n",
    "\n",
    "通过位置索引`iloc`，我们将`data`分成`inputs`和`outputs`， 其中前者为`data`的前两列，而后者为`data`的最后一列。 对于`inputs`中缺少的数值，我们用同一列的均值替换“`NaN`”项。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:37.579871200Z",
     "start_time": "2023-07-31T11:37:37.366715700Z"
    }
   },
   "source": [
    "inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]\n",
    "inputs.NumRooms = inputs.NumRooms.fillna(inputs.NumRooms.mean())\n",
    "print(inputs)"
   ],
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Allley\n",
      "0       3.0   Pave\n",
      "1       2.0    NaN\n",
      "2       4.0    NaN\n",
      "3       3.0    NaN\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于`inputs`中的类别值或离散值，我们将“`NaN`”视为一个类别。 由于“巷子类型”（“Alley”）列只接受两种类型的类别值“`Pave`”和“`NaN`”， pandas可以自动将此列转换为两列“`Alley_Pave`”和“`Alley_nan`”。 巷子类型为“`Pave`”的行会将“`Alley_Pave`”的值设置为1，“`Alley_nan`”的值设置为0。 缺少巷子类型的行会将“`Alley_Pave`”和“`Alley_nan`”分别设置为0和1。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:37.595799600Z",
     "start_time": "2023-07-31T11:37:37.375933700Z"
    }
   },
   "source": [
    "inputs = pd.get_dummies(inputs, dummy_na = True, dtype = float)\n",
    "print(inputs)"
   ],
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms  Allley_Pave  Allley_nan\n",
      "0       3.0          1.0         0.0\n",
      "1       2.0          0.0         1.0\n",
      "2       4.0          0.0         1.0\n",
      "3       3.0          0.0         1.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在`inputs`和`outputs`中的所有条目都是数值类型，它们可以转换为张量格式。 当数据采用张量格式后，可以通过在 @sec-ndarray 中引入的那些张量函数来进一步操作。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:37.597279200Z",
     "start_time": "2023-07-31T11:37:37.388066200Z"
    }
   },
   "source": [
    "X, y= torch.tensor(inputs.values), torch.tensor(outputs.values)\n",
    "X, y"
   ],
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[3., 1., 0.],\n         [2., 0., 1.],\n         [4., 0., 1.],\n         [3., 0., 1.]], dtype=torch.float64),\n tensor([127500, 106000, 178100, 140000]))"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性代数\n",
    "\n",
    "### 降维\n",
    "\n",
    "默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以指定张量沿哪一个轴来通过求和降低维度。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定`axis=0`。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:37.598305100Z",
     "start_time": "2023-07-31T11:37:37.400275100Z"
    }
   },
   "source": [
    "A = torch.arange(20, dtype = torch.float32).reshape(5, 4)\n",
    "A_sum_axis0 = A.sum(axis = 0)\n",
    "A_sum_axis0, A_sum_axis0.shape"
   ],
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([40., 45., 50., 55.]), torch.Size([4]))"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指定`axis=1`将通过汇总所有列的元素降维（轴1）。因此，输入轴1的维数在输出形状中消失。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:37.598305100Z",
     "start_time": "2023-07-31T11:37:37.406094900Z"
    }
   },
   "source": [
    "A_sum_axis1 = A.sum(axis=1)\n",
    "A_sum_axis1, A_sum_axis1.shape"
   ],
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:37.598305100Z",
     "start_time": "2023-07-31T11:37:37.413927100Z"
    }
   },
   "source": [
    "A.sum(axis = [0, 1])  # 结果和A.sum()相同"
   ],
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(190.)"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "非降维求和："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:37.598305100Z",
     "start_time": "2023-07-31T11:37:37.421126700Z"
    }
   },
   "source": [
    "# 保持轴数不变\n",
    "sum_A = A.sum(axis = 1, keepdims = True)\n",
    "sum_A"
   ],
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 6.],\n        [22.],\n        [38.],\n        [54.],\n        [70.]])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:37.599313700Z",
     "start_time": "2023-07-31T11:37:37.428991500Z"
    }
   },
   "source": [
    "# 通过广播将 A 除以 sum_A\n",
    "A / sum_A"
   ],
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n        [0.1818, 0.2273, 0.2727, 0.3182],\n        [0.2105, 0.2368, 0.2632, 0.2895],\n        [0.2222, 0.2407, 0.2593, 0.2778],\n        [0.2286, 0.2429, 0.2571, 0.2714]])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:37.599313700Z",
     "start_time": "2023-07-31T11:37:37.434599100Z"
    }
   },
   "source": [
    "# 求列和\n",
    "A.cumsum(axis = 0)"
   ],
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  6.,  8., 10.],\n        [12., 15., 18., 21.],\n        [24., 28., 32., 36.],\n        [40., 45., 50., 55.]])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 点积\n",
    "\n",
    "给定两个向量 $\\mathbf{x}, \\mathbf{y}\\in\\mathbb{R}^d$，\n",
    "它们的*点积*（dot product）$\\mathbf{x}^\\top\\mathbf{y}$\n",
    " （或 $\\langle\\mathbf{x},\\mathbf{y}\\rangle$）\n",
    " 是相同位置的按元素乘积的和：$\\mathbf{x}^\\top\\mathbf{y}=\\sum^d_{i=1} x_i y_i$。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:37.599313700Z",
     "start_time": "2023-07-31T11:37:37.443402500Z"
    }
   },
   "source": [
    "x = torch.arange(4, dtype = torch.float32)\n",
    "y = torch.ones(4, dtype = torch.float32)\n",
    "x, y, torch.dot(x, y)"
   ],
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵-向量积\n",
    "\n",
    "在代码中使用张量表示矩阵-向量积，我们使用`mv`函数。\n",
    "当我们为矩阵`A`和向量`x`调用`torch.mv(A, x)`时，会执行矩阵-向量积。\n",
    "注意，`A`的列维数（沿轴1的长度）必须与`x`的维数（其长度）相同。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:37.600342300Z",
     "start_time": "2023-07-31T11:37:37.454089200Z"
    }
   },
   "source": [
    "A.shape, x.shape, torch.mv(A, x)"
   ],
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵-矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:37.600342300Z",
     "start_time": "2023-07-31T11:37:37.464669600Z"
    }
   },
   "source": [
    "B = torch.ones(4, 3)\n",
    "torch.mm(A, B)"
   ],
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 6.,  6.,  6.],\n        [22., 22., 22.],\n        [38., 38., 38.],\n        [54., 54., 54.],\n        [70., 70., 70.]])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 范数\n",
    "\n",
    "线性代数中最有用的一些运算符是*范数*（norm）。\n",
    "非正式地说，向量的*范数*是表示一个向量有多大。\n",
    "这里考虑的*大小*（size）概念不涉及维度，而是分量的大小。\n",
    "\n",
    "在线性代数中，向量范数是将向量映射到标量的函数$f$。\n",
    "给定任意向量$\\mathbf{x}$，向量范数要满足一些属性。\n",
    "第一个性质是：如果我们按常数因子$\\alpha$缩放向量的所有元素，\n",
    "其范数也会按相同常数因子的*绝对值*缩放：\n",
    "\n",
    "$$\n",
    "f(\\alpha \\mathbf{x}) = |\\alpha| f(\\mathbf{x}).\n",
    "$$ {#eq-norm1}\n",
    "\n",
    "第二个性质是熟悉的三角不等式:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x} + \\mathbf{y}) \\leq f(\\mathbf{x}) + f(\\mathbf{y}).\n",
    "$$ {#eq-norm2}\n",
    "\n",
    "第三个性质简单地说范数必须是非负的:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) \\geq 0.\n",
    "$$ {#eq-norm3}\n",
    "\n",
    "这是有道理的。因为在大多数情况下，任何东西的最小的*大小*是0。\n",
    "最后一个性质要求范数最小为0，当且仅当向量全由0组成。\n",
    "\n",
    "$$\n",
    "\\forall i, [\\mathbf{x}]_i = 0 \\Leftrightarrow f(\\mathbf{x})=0.\n",
    "$$ {#eq-norm4}\n",
    "\n",
    "在深度学习中，我们经常试图解决优化问题：\n",
    "*最大化*分配给观测数据的概率;\n",
    "*最小化*预测和真实观测之间的距离。\n",
    "用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。\n",
    "**目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。**\n",
    "\n",
    "## 微积分\n",
    "\n",
    "在深度学习中，我们“训练”模型，不断更新它们，使它们在看到越来越多的数据时变得越来越好。\n",
    "通常情况下，变得更好意味着最小化一个*损失函数*（loss function），\n",
    "即一个衡量“模型有多糟糕”这个问题的分数。\n",
    "最终，我们真正关心的是生成一个模型，它能够在从未见过的数据上表现良好。\n",
    "但“训练”模型只能将模型与我们实际能看到的数据相拟合。\n",
    "因此，我们可以将拟合模型的任务分解为两个关键问题：\n",
    "\n",
    "- *优化*（optimization）：用模型拟合观测数据的过程；\n",
    "- *泛化*（generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:39.401133100Z",
     "start_time": "2023-07-31T11:37:37.474612900Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from matplotlib_inline import backend_inline"
   ],
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:39.417716400Z",
     "start_time": "2023-07-31T11:37:39.396118200Z"
    }
   },
   "source": [
    "# 定义 f(x)\n",
    "def f(x):\n",
    "  return 3 * x ** 2 - 4 * x\n",
    "\n",
    "# 定义瞬时变化率计算函数\n",
    "def numerical_lim(f, x, h):\n",
    "  return (f(x + h) - f(x)) / h\n",
    "\n",
    "h = 0.1\n",
    "for i in range(5):\n",
    "  print(f'h={h:.5f}, numerical limit={numerical_lim(f, 1, h):.5f}')\n",
    "  h *= 0.1"
   ],
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h=0.10000, numerical limit=2.30000\n",
      "h=0.01000, numerical limit=2.03000\n",
      "h=0.00100, numerical limit=2.00300\n",
      "h=0.00010, numerical limit=2.00030\n",
      "h=0.00001, numerical limit=2.00003\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:39.994131Z",
     "start_time": "2023-07-31T11:37:39.412929Z"
    }
   },
   "source": [
    "# 使用自定义的 d2l.py 中的函数\n",
    "import d2l"
   ],
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.201339300Z",
     "start_time": "2023-07-31T11:37:39.981930400Z"
    }
   },
   "source": [
    "#| label: fig-qiexian\n",
    "#| fig-cap: \"x=1处切线\"\n",
    "x = np.arange(0, 3, 0.1)\n",
    "d2l.plot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])"
   ],
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 350x250 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"243.529359pt\" height=\"183.35625pt\" viewBox=\"0 0 243.529359 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-07-31T19:37:40.157623</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 183.35625 \nL 243.529359 183.35625 \nL 243.529359 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 145.8 \nL 235.903125 145.8 \nL 235.903125 7.2 \nL 40.603125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 49.480398 145.8 \nL 49.480398 7.2 \n\" clip-path=\"url(#p384e9952c2)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"mb150eb0471\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mb150eb0471\" x=\"49.480398\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(46.299148 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 110.702968 145.8 \nL 110.702968 7.2 \n\" clip-path=\"url(#p384e9952c2)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#mb150eb0471\" x=\"110.702968\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1 -->\n      <g transform=\"translate(107.521718 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 171.925539 145.8 \nL 171.925539 7.2 \n\" clip-path=\"url(#p384e9952c2)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#mb150eb0471\" x=\"171.925539\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 2 -->\n      <g transform=\"translate(168.744289 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 233.148109 145.8 \nL 233.148109 7.2 \n\" clip-path=\"url(#p384e9952c2)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#mb150eb0471\" x=\"233.148109\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 3 -->\n      <g transform=\"translate(229.966859 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_5\">\n     <!-- x -->\n     <g transform=\"translate(135.29375 174.076563) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-78\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <path d=\"M 40.603125 116.769994 \nL 235.903125 116.769994 \n\" clip-path=\"url(#p384e9952c2)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <defs>\n       <path id=\"m42f3304d53\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m42f3304d53\" x=\"40.603125\" y=\"116.769994\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0 -->\n      <g transform=\"translate(27.240625 120.569213) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <path d=\"M 40.603125 78.886651 \nL 235.903125 78.886651 \n\" clip-path=\"url(#p384e9952c2)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m42f3304d53\" x=\"40.603125\" y=\"78.886651\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 5 -->\n      <g transform=\"translate(27.240625 82.685869) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_13\">\n      <path d=\"M 40.603125 41.003307 \nL 235.903125 41.003307 \n\" clip-path=\"url(#p384e9952c2)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m42f3304d53\" x=\"40.603125\" y=\"41.003307\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 10 -->\n      <g transform=\"translate(20.878125 44.802526) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- f(x) -->\n     <g transform=\"translate(14.798437 85.121094) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \nQ 1566 4138 1362 3434 \nQ 1159 2731 1159 2009 \nQ 1159 1288 1364 580 \nQ 1569 -128 1984 -844 \nL 1484 -844 \nQ 1016 -109 783 600 \nQ 550 1309 550 2009 \nQ 550 2706 781 3412 \nQ 1013 4119 1484 4856 \nL 1984 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \nL 1013 4856 \nQ 1481 4119 1714 3412 \nQ 1947 2706 1947 2009 \nQ 1947 1309 1714 600 \nQ 1481 -109 1013 -844 \nL 513 -844 \nQ 928 -128 1133 580 \nQ 1338 1288 1338 2009 \nQ 1338 2731 1133 3434 \nQ 928 4138 513 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-66\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path d=\"M 49.480398 116.769994 \nL 55.602655 119.573361 \nL 61.724912 121.922129 \nL 67.847169 123.816296 \nL 73.969426 125.255863 \nL 80.091683 126.24083 \nL 86.21394 126.771197 \nL 92.336197 126.846963 \nL 98.458454 126.46813 \nL 104.580711 125.634696 \nL 110.702968 124.346663 \nL 116.825225 122.604029 \nL 122.947482 120.406795 \nL 129.069739 117.754961 \nL 135.191996 114.648527 \nL 141.314254 111.087492 \nL 147.436511 107.071858 \nL 153.558768 102.601624 \nL 159.681025 97.676789 \nL 165.803282 92.297354 \nL 171.925539 86.463319 \nL 178.047796 80.174684 \nL 184.170053 73.431449 \nL 190.29231 66.233614 \nL 196.414567 58.581179 \nL 202.536824 50.474143 \nL 208.659081 41.912508 \nL 214.781338 32.896272 \nL 220.903595 23.425436 \nL 227.025852 13.5 \n\" clip-path=\"url(#p384e9952c2)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path d=\"M 49.480398 139.5 \nL 55.602655 137.984666 \nL 61.724912 136.469333 \nL 67.847169 134.953999 \nL 73.969426 133.438665 \nL 80.091683 131.923331 \nL 86.21394 130.407998 \nL 92.336197 128.892664 \nL 98.458454 127.37733 \nL 104.580711 125.861996 \nL 110.702968 124.346663 \nL 116.825225 122.831329 \nL 122.947482 121.315995 \nL 129.069739 119.800661 \nL 135.191996 118.285328 \nL 141.314254 116.769994 \nL 147.436511 115.25466 \nL 153.558768 113.739327 \nL 159.681025 112.223993 \nL 165.803282 110.708659 \nL 171.925539 109.193325 \nL 178.047796 107.677992 \nL 184.170053 106.162658 \nL 190.29231 104.647324 \nL 196.414567 103.13199 \nL 202.536824 101.616657 \nL 208.659081 100.101323 \nL 214.781338 98.585989 \nL 220.903595 97.070655 \nL 227.025852 95.555322 \n\" clip-path=\"url(#p384e9952c2)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 145.8 \nL 40.603125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 235.903125 145.8 \nL 235.903125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 145.8 \nL 235.903125 145.8 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 7.2 \nL 235.903125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 47.603125 44.55625 \nL 172.153125 44.55625 \nQ 174.153125 44.55625 174.153125 42.55625 \nL 174.153125 14.2 \nQ 174.153125 12.2 172.153125 12.2 \nL 47.603125 12.2 \nQ 45.603125 12.2 45.603125 14.2 \nL 45.603125 42.55625 \nQ 45.603125 44.55625 47.603125 44.55625 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_17\">\n     <path d=\"M 49.603125 20.298438 \nL 59.603125 20.298438 \nL 69.603125 20.298438 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_10\">\n     <!-- f(x) -->\n     <g transform=\"translate(77.603125 23.798438) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-66\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n     </g>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 49.603125 34.976562 \nL 59.603125 34.976562 \nL 69.603125 34.976562 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_11\">\n     <!-- Tangent line (x=1) -->\n     <g transform=\"translate(77.603125 38.476562) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-54\" d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-3d\" d=\"M 678 2906 \nL 4684 2906 \nL 4684 2381 \nL 678 2381 \nL 678 2906 \nz\nM 678 1631 \nL 4684 1631 \nL 4684 1100 \nL 678 1100 \nL 678 1631 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-54\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"44.583984\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"105.863281\"/>\n      <use xlink:href=\"#DejaVuSans-67\" x=\"169.242188\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"232.71875\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"294.242188\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"357.621094\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"396.830078\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"428.617188\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"456.400391\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"484.183594\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"547.5625\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"609.085938\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"640.873047\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"679.886719\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"739.066406\"/>\n      <use xlink:href=\"#DejaVuSans-31\" x=\"822.855469\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"886.478516\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p384e9952c2\">\n   <rect x=\"40.603125\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以连结一个多元函数对其所有变量的偏导数，以得到该函数的*梯度*（gradient）向量。\n",
    "具体而言，设函数$f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$的输入是\n",
    "一个$n$维向量$\\mathbf{x}=[x_1,x_2,\\ldots,x_n]^\\top$，并且输出是一个标量。\n",
    "函数$f(\\mathbf{x})$相对于$\\mathbf{x}$的梯度是一个包含$n$个偏导数的向量:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\bigg[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\bigg]^\\top,\n",
    "$$ {#eq-grandient}\n",
    "\n",
    "其中$\\nabla_{\\mathbf{x}} f(\\mathbf{x})$通常在没有歧义时被$\\nabla f(\\mathbf{x})$取代。\n",
    "\n",
    "假设$\\mathbf{x}$为$n$维向量，在微分多元函数时经常使用以下规则:\n",
    "\n",
    "- 对于所有$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$，都有$\\nabla_{\\mathbf{x}} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top$\n",
    "- 对于所有$\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$，都有$\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A}  = \\mathbf{A}$\n",
    "- 对于所有$\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$，都有$\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x}  = (\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}$\n",
    "- $\\nabla_{\\mathbf{x}} \\|\\mathbf{x} \\|^2 = \\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2\\mathbf{x}$\n",
    "\n",
    "同样，对于任何矩阵$\\mathbf{X}$，都有$\\nabla_{\\mathbf{X}} \\|\\mathbf{X} \\|_F^2 = 2\\mathbf{X}$。\n",
    "正如我们之后将看到的，梯度对于设计深度学习中的优化算法有很大用处。\n",
    "\n",
    "## 自动微分\n",
    "\n",
    "深度学习框架通过自动计算导数，即*自动微分*（automatic differentiation）来加快求导。\n",
    "实际中，根据设计好的模型，系统会构建一个*计算图*（computational graph），\n",
    "来跟踪计算是哪些数据通过哪些操作组合起来产生输出。\n",
    "自动微分使系统能够随后反向传播梯度。\n",
    "这里，*反向传播*（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。\n",
    "\n",
    "### 一个简单案例\n",
    "\n",
    "假设我们对函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量$\\mathbf{x}$求导。\n",
    "首先，我们创建变量`x`并为其分配一个初始值。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.202339200Z",
     "start_time": "2023-07-31T11:37:40.193734600Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x"
   ],
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0., 1., 2., 3.])"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我们计算$y$关于$\\mathbf{x}$的梯度之前，需要一个地方来存储梯度。\n",
    "重要的是，我们不会在每次对一个参数求导时都分配新的内存。\n",
    "因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。\n",
    "注意，一个标量函数关于向量$\\mathbf{x}$的梯度是向量，并且与$\\mathbf{x}$具有相同的形状。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.213168600Z",
     "start_time": "2023-07-31T11:37:40.202475200Z"
    }
   },
   "source": [
    "x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)\n",
    "x.grad  # 默认值是None"
   ],
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.287783700Z",
     "start_time": "2023-07-31T11:37:40.208165800Z"
    }
   },
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ],
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(28., grad_fn=<MulBackward0>)"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x`是一个长度为4的向量，计算`x`和`x`的点积，得到了我们赋值给`y`的标量输出。\n",
    "接下来，通过调用反向传播函数来自动计算`y`关于`x`每个分量的梯度，并打印这些梯度。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.289292Z",
     "start_time": "2023-07-31T11:37:40.221211100Z"
    }
   },
   "source": [
    "y.backward()\n",
    "x.grad"
   ],
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 0.,  4.,  8., 12.])"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于$\\mathbf{x}$的梯度应为$4\\mathbf{x}$。\n",
    "让我们快速验证这个梯度是否计算正确。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.289811800Z",
     "start_time": "2023-07-31T11:37:40.235906100Z"
    }
   },
   "source": [
    "x.grad == 4 * x"
   ],
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([True, True, True, True])"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在计算`x`的另一个函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.289811800Z",
     "start_time": "2023-07-31T11:37:40.243375600Z"
    }
   },
   "source": [
    "# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
    "x.grad.zero_()\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ],
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1., 1., 1., 1.])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分离计算\n",
    "\n",
    "有时，我们希望**将某些计算移动到记录的计算图之外**]。\n",
    "例如，假设`y`是作为`x`的函数计算的，而`z`则是作为`y`和`x`的函数计算的。\n",
    "想象一下，我们想计算`z`关于`x`的梯度，但由于某种原因，希望将`y`视为一个常数，\n",
    "并且只考虑到`x`在`y`被计算后发挥的作用。\n",
    "\n",
    "这里可以分离`y`来返回一个新变量`u`，该变量与`y`具有相同的值，\n",
    "但丢弃计算图中如何计算`y`的任何信息。\n",
    "换句话说，梯度不会向后流经`u`到`x`。\n",
    "因此，下面的反向传播函数计算`z=u*x`关于`x`的偏导数，同时将`u`作为常数处理，\n",
    "而不是`z=x*x*x`关于`x`的偏导数。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.289811800Z",
     "start_time": "2023-07-31T11:37:40.253843700Z"
    }
   },
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x"
   ],
   "execution_count": 49,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于记录了`y`的计算结果，我们可以随后在`y`上调用反向传播，\n",
    "得到`y=x*x`关于的`x`的导数，即`2*x`。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.289811800Z",
     "start_time": "2023-07-31T11:37:40.260900Z"
    }
   },
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ],
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([True, True, True, True])"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python控制流的梯度计算\n",
    "\n",
    "使用自动微分的一个好处是：\n",
    "即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度。\n",
    "在下面的代码中，`while`循环的迭代次数和`if`语句的结果都取决于输入`a`的值。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.289811800Z",
     "start_time": "2023-07-31T11:37:40.267946400Z"
    }
   },
   "source": [
    "def f(a):\n",
    "  b = a * 2\n",
    "  while b.norm() < 1000:\n",
    "    b = b * 2\n",
    "  if b.sum() > 0:\n",
    "    c = b\n",
    "  else:\n",
    "    c = 100 * b\n",
    "  return c"
   ],
   "execution_count": 51,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.289811800Z",
     "start_time": "2023-07-31T11:37:40.274269400Z"
    }
   },
   "source": [
    "# 计算梯度\n",
    "a = torch.randn(size = (), requires_grad = True)\n",
    "d = f(a)\n",
    "d.backward()"
   ],
   "execution_count": 52,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.314935200Z",
     "start_time": "2023-07-31T11:37:40.285696200Z"
    }
   },
   "source": [
    "a.grad == d / a"
   ],
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(True)"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概率\n",
    "\n",
    "现实生活中，对于我们从工厂收到的真实骰子，我们需要检查它是否有瑕疵。\n",
    "检查骰子的唯一方法是多次投掷并记录结果。\n",
    "对于每个骰子，我们将观察到$\\{1, \\ldots, 6\\}$中的一个值。\n",
    "对于每个值，一种自然的方法是将它出现的次数除以投掷的总次数，\n",
    "即此*事件*（event）概率的*估计值*。\n",
    "*大数定律*（law of large numbers）告诉我们：\n",
    "随着投掷次数的增加，这个估计值会越来越接近真实的潜在概率。\n",
    "让我们用代码试一试！"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.315943700Z",
     "start_time": "2023-07-31T11:37:40.294062700Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch.distributions import multinomial"
   ],
   "execution_count": 54,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.684390500Z",
     "start_time": "2023-07-31T11:37:40.299133700Z"
    }
   },
   "source": [
    "fair_probs = torch.ones([6]) / 6\n",
    "multinomial.Multinomial(1, fair_probs).sample()"
   ],
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0., 1., 0., 0., 0., 0.])"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在估计一个骰子的公平性时，我们希望从同一分布中生成多个样本。\n",
    "如果用Python的for循环来完成这个任务，速度会慢得惊人。\n",
    "因此我们使用深度学习框架的函数同时抽取多个样本，得到我们想要的任意形状的独立样本数组。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.710288400Z",
     "start_time": "2023-07-31T11:37:40.383967600Z"
    }
   },
   "source": [
    "multinomial.Multinomial(10, fair_probs).sample()"
   ],
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2., 1., 3., 3., 1., 0.])"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们知道如何对骰子进行采样，我们可以模拟1000次投掷。\n",
    "然后，我们可以统计1000次投掷后，每个数字被投中了多少次。\n",
    "具体来说，我们计算相对频率，以作为真实概率的估计。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.710288400Z",
     "start_time": "2023-07-31T11:37:40.388327Z"
    }
   },
   "source": [
    "# 将结果存储为32位浮点数以进行除法\n",
    "counts = multinomial.Multinomial(1000, fair_probs).sample()\n",
    "counts / 1000   # 相对频率作为估计值"
   ],
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.1580, 0.1690, 0.1850, 0.1440, 0.1690, 0.1750])"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性神经网络\n",
    "\n",
    "## 线性回归\n",
    "\n",
    "- *仿射变换*（affine transformation）的特点是通过加权和对特征进行*线性变换*（linear transformation），并通过偏置项来进行*平移*（translation）。\n",
    "\n",
    "- 给定训练数据特征$\\mathbf{X}$和对应的已知标签$\\mathbf{y}$，\n",
    "线性回归的目标是找到一组权重向量$\\mathbf{w}$和偏置$b$：\n",
    "当给定从$\\mathbf{X}$的同分布中取样的新样本特征时，\n",
    "这组权重向量和偏置能够使得新样本预测标签的误差尽可能小。\n",
    "\n",
    "- *损失函数*（loss function）能够量化目标的*实际*值与*预测*值之间的差距。\n",
    "通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。\n",
    "回归问题中最常用的损失函数是平方误差函数。\n",
    "\n",
    "### 线性回归的基本元素\n",
    "\n",
    "- 梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值）\n",
    "关于模型参数的导数（在这里也可以称为梯度）。\n",
    "但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。\n",
    "因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本，\n",
    "这种变体叫做*小批量随机梯度下降*（minibatch stochastic gradient descent）。\n",
    "\n",
    "- *批量大小*（batch size）和*学习率*（learning rate）的值通常是手动预先指定，而不是通过模型训练得到的。\n",
    "这些可以调整但不在训练过程中更新的参数称为*超参数*（hyperparameter）。\n",
    "*调参*（hyperparameter tuning）是选择超参数的过程。\n",
    "超参数通常是我们根据训练迭代结果来调整的，\n",
    "而训练迭代结果是在独立的*验证数据集*（validation dataset）上评估得到的。\n",
    "\n",
    "- 线性回归恰好是一个在整个域中只有一个最小值的学习问题。\n",
    "但是对像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。\n",
    "深度学习实践者很少会去花费大力气寻找这样一组参数，使得在*训练集*上的损失达到最小。\n",
    "事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失，\n",
    "这一挑战被称为*泛化*（generalization）。\n",
    "\n",
    "### 矢量化加速"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.710288400Z",
     "start_time": "2023-07-31T11:37:40.400441600Z"
    }
   },
   "source": [
    "#| include: false \n",
    "\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import d2l\n",
    "import random"
   ],
   "execution_count": 58,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.711390600Z",
     "start_time": "2023-07-31T11:37:40.406672200Z"
    }
   },
   "source": [
    "n = 10000\n",
    "a = torch.ones([n])\n",
    "b = torch.ones([n])"
   ],
   "execution_count": 59,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们使用for循环，每次执行一位的加法。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.711390600Z",
     "start_time": "2023-07-31T11:37:40.413262200Z"
    }
   },
   "source": [
    "c = torch.zeros([n])\n",
    "timer = d2l.Timer()\n",
    "for i in range(n):\n",
    "  c[i] = a[i] + b[i]\n",
    "f'{timer.stop():.5f} sec'"
   ],
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "'0.18561 sec'"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者，我们使用重载的`+`运算符来计算按元素的和。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.712486200Z",
     "start_time": "2023-07-31T11:37:40.622696100Z"
    }
   },
   "source": [
    "timer.start()\n",
    "d = a + b\n",
    "f'{timer.stop():.5f} sec'"
   ],
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "'0.00100 sec'"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果很明显，第二种方法比第一种方法快得多。\n",
    "矢量化代码通常会带来数量级的加速。\n",
    "\n",
    "## 线性回归的从零开始实现"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.712486200Z",
     "start_time": "2023-07-31T11:37:40.623250700Z"
    }
   },
   "source": [
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = d2l.synthetic_data(true_w, true_b, 1000)"
   ],
   "execution_count": 62,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.713566200Z",
     "start_time": "2023-07-31T11:37:40.627760600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.713566200Z",
     "start_time": "2023-07-31T11:37:40.632461200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.713566200Z",
     "start_time": "2023-07-31T11:37:40.637401500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:40.994360200Z",
     "start_time": "2023-07-31T11:37:40.641872900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:41.010514600Z",
     "start_time": "2023-07-31T11:37:40.659787100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:41.057580100Z",
     "start_time": "2023-07-31T11:37:40.662529100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:41.058497200Z",
     "start_time": "2023-07-31T11:37:40.685307700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:41.058497200Z",
     "start_time": "2023-07-31T11:37:40.685307700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T11:37:41.058497200Z",
     "start_time": "2023-07-31T11:37:40.685307700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-tip title=\"To be continued\"}\n",
    "- <https://zh-v2.d2l.ai/chapter_linear-networks/linear-regression.html>\n",
    ":::\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
