{
  "hash": "ffa00a5f52bd7051b595dfcaa410348c",
  "result": {
    "markdown": "---\ntitle: \"Applied Machine Learning Using mlr3 in R\"\ndate: \"2023-08-16\"\ndate-modified: \"2023-09-22\"\nimage: \"logo.png\"\ncategories: \n  - Machine Learning\n  - R\n  - mlr3\n---\n\n\n\n\n::: {.callout-note title='Progress'}\nLearning Progress: 20.67%.\n:::\n\n::: {.callout-tip title=\"Learning Source\"}\n- <https://mlr3book.mlr-org.com/>\n- 中文翻译由 ChatGPT 3.5 提供\n:::\n\n# Getting Started {.unnumbered}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\nlibrary(mlr3pipelines)\nlibrary(mlr3benchmark)\nlibrary(ggplot2)\n```\n:::\n\n\n\n\n# Introduction and Overview\n\n`mlr3` by Example:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\n\ntask = tsk(\"penguins\")\nsplit = partition(task)\nlearner = lrn(\"classif.rpart\")\n\nlearner$train(task, row_ids = split$train)\nlearner$model\n#> n= 231 \n#> \n#> node), split, n, loss, yval, (yprob)\n#>       * denotes terminal node\n#> \n#> 1) root 231 129 Adelie (0.441558442 0.199134199 0.359307359)  \n#>   2) flipper_length< 206.5 144  44 Adelie (0.694444444 0.298611111 0.006944444)  \n#>     4) bill_length< 43.05 98   3 Adelie (0.969387755 0.030612245 0.000000000) *\n#>     5) bill_length>=43.05 46   6 Chinstrap (0.108695652 0.869565217 0.021739130) *\n#>   3) flipper_length>=206.5 87   5 Gentoo (0.022988506 0.034482759 0.942528736) *\n\nprediction = learner$predict(task, row_ids = split$test)\nprediction\n#> <PredictionClassif> for 113 observations:\n#>     row_ids     truth  response\n#>           1    Adelie    Adelie\n#>           2    Adelie    Adelie\n#>           3    Adelie    Adelie\n#> ---                            \n#>         328 Chinstrap Chinstrap\n#>         331 Chinstrap    Adelie\n#>         339 Chinstrap Chinstrap\n\nprediction$score(msr(\"classif.acc\"))\n#> classif.acc \n#>   0.9557522\n```\n:::\n\n\nThe `mlr3` interface also lets you run more complicated experiments in just a few lines of code:\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\nWe use dictionaries to group large collections of relevant objects so they can be listed and retrieved easily.\nFor example, you can see an overview of available learners (that are in loaded packages) and their properties with `as.data.table(mlr_learners)` or by calling the sugar function without any arguments, e.g. `lrn()`.\n\n> 我们使用字典来分组大量相关对象，以便可以轻松地列出和检索它们。例如，您可以通过 `as.data.table(mlr_learners)` 查看可用学习器（位于加载的包中）及其属性的概述，或者通过调用糖函数而不带任何参数，例如 `lrn()`。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(mlr_learners)[1:3]\n#>                    key                              label task_type\n#> 1:   classif.cv_glmnet                               <NA>   classif\n#> 2:       classif.debug   Debug Learner for Classification   classif\n#> 3: classif.featureless Featureless Classification Learner   classif\n#>                                           feature_types\n#> 1:                              logical,integer,numeric\n#> 2:     logical,integer,numeric,character,factor,ordered\n#> 3: logical,integer,numeric,character,factor,ordered,...\n#>                    packages\n#> 1: mlr3,mlr3learners,glmnet\n#> 2:                     mlr3\n#> 3:                     mlr3\n#>                                                               properties\n#> 1:                         multiclass,selected_features,twoclass,weights\n#> 2:                         hotstart_forward,missings,multiclass,twoclass\n#> 3: featureless,importance,missings,multiclass,selected_features,twoclass\n#>    predict_types\n#> 1: response,prob\n#> 2: response,prob\n#> 3: response,prob\n```\n:::\n\n\n# Fundamentals {.unnumbered}\n\n# Data and Basic Modeling\n\n## Tasks\n\n### Constructing Tasks\n\n`mlr3` includes a few predefined machine learning tasks in the `mlr_tasks` Dictionary.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlr_tasks\n#> <DictionaryTask> with 21 stored values\n#> Keys: ames_housing, bike_sharing, boston_housing, breast_cancer,\n#>   german_credit, ilpd, iris, kc_housing, moneyball, mtcars, optdigits,\n#>   penguins, penguins_simple, pima, ruspini, sonar, spam, titanic,\n#>   usarrests, wine, zoo\n# the same as \n# tsk()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_mtcars = tsk(\"mtcars\")\ntsk_mtcars\n#> <TaskRegr:mtcars> (32 x 11): Motor Trends\n#> * Target: mpg\n#> * Properties: -\n#> * Features (10):\n#>   - dbl (10): am, carb, cyl, disp, drat, gear, hp, qsec, vs, wt\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# create my own regression task\ndata(\"mtcars\", package = \"datasets\")\nmtcars_subset = subset(mtcars, select = c(\"mpg\", \"cyl\", \"disp\"))\ntsk_mtcars = as_task_regr(mtcars_subset, target = \"mpg\", id = \"cars\")\ntsk_mtcars\n#> <TaskRegr:cars> (32 x 3)\n#> * Target: mpg\n#> * Properties: -\n#> * Features (2):\n#>   - dbl (2): cyl, disp\n```\n:::\n\n\nThe `id` argument is optional and specifies an identifier for the task that is used in plots and summaries; if omitted the variable name of the data will be used as the `id`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3viz)\nautoplot(tsk_mtcars, type = \"pairs\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n### Retrieving Data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nc(tsk_mtcars$nrow, tsk_mtcars$ncol)\n#> [1] 32  3\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nc(Features = tsk_mtcars$feature_names,\n  Target = tsk_mtcars$target_names)\n#> Features1 Features2    Target \n#>     \"cyl\"    \"disp\"     \"mpg\"\n```\n:::\n\n\nRow IDs are not used as features when training or predicting but are metadata that allow access to individual observations. Note that row IDs are not the same as row numbers.\n\nThis design decision allows tasks and learners to transparently operate on real database management systems, where primary keys are required to be unique, but not necessarily consecutive.\n\n> 行ID在训练或预测时不作为特征使用，而是元数据，用于访问个别观测数据。需要注意的是，行ID与行号不同。\n>\n> 这种设计决策使得任务和学习器能够透明地在真实的数据库管理系统上运行，其中要求主键是唯一的，但不一定连续。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = as_task_regr(data.frame(x = runif(5), y = runif(5)),\n                    target = \"y\")\ntask$row_ids\n#> [1] 1 2 3 4 5\n\ntask$filter(c(4, 1, 3))\ntask$row_ids\n#> [1] 1 3 4\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_mtcars$data()[1:3]\n#>     mpg cyl disp\n#> 1: 21.0   6  160\n#> 2: 21.0   6  160\n#> 3: 22.8   4  108\ntsk_mtcars$data(rows = c(1, 5, 10), cols = tsk_mtcars$feature_names)\n#>    cyl  disp\n#> 1:   6 160.0\n#> 2:   8 360.0\n#> 3:   6 167.6\n```\n:::\n\n\n### Task Mutators\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_mtcars_small = tsk(\"mtcars\")\ntsk_mtcars_small$select(\"cyl\")\ntsk_mtcars_small$filter(2:3)\ntsk_mtcars_small$data()\n#>     mpg cyl\n#> 1: 21.0   6\n#> 2: 22.8   4\n```\n:::\n\n\nAs `R6` uses reference semantics, you need to use `$clone()` if you want to modify a task while keeping the original object intact.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_mtcars = tsk(\"mtcars\")\ntsk_mtcars_clone = tsk_mtcars$clone()\ntsk_mtcars_clone$filter(1:2)\ntsk_mtcars_clone$head()\n#>    mpg am carb cyl disp drat gear  hp  qsec vs    wt\n#> 1:  21  1    4   6  160  3.9    4 110 16.46  0 2.620\n#> 2:  21  1    4   6  160  3.9    4 110 17.02  0 2.875\n```\n:::\n\n\nTo add extra rows and columns to a task, you can use `$rbind()` and `$cbind()` respectively:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_mtcars_small\n#> <TaskRegr:mtcars> (2 x 2): Motor Trends\n#> * Target: mpg\n#> * Properties: -\n#> * Features (1):\n#>   - dbl (1): cyl\ntsk_mtcars_small$cbind(data.frame(disp = c(150, 160)))\ntsk_mtcars_small$rbind(data.frame(mpg = 23, cyl = 5, disp = 170))\ntsk_mtcars_small$data()\n#>     mpg cyl disp\n#> 1: 21.0   6  150\n#> 2: 22.8   4  160\n#> 3: 23.0   5  170\n```\n:::\n\n\n## Learners\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# all the learners available in mlr3\nmlr_learners\n#> <DictionaryLearner> with 46 stored values\n#> Keys: classif.cv_glmnet, classif.debug, classif.featureless,\n#>   classif.glmnet, classif.kknn, classif.lda, classif.log_reg,\n#>   classif.multinom, classif.naive_bayes, classif.nnet, classif.qda,\n#>   classif.ranger, classif.rpart, classif.svm, classif.xgboost,\n#>   clust.agnes, clust.ap, clust.cmeans, clust.cobweb, clust.dbscan,\n#>   clust.diana, clust.em, clust.fanny, clust.featureless, clust.ff,\n#>   clust.hclust, clust.kkmeans, clust.kmeans, clust.MBatchKMeans,\n#>   clust.mclust, clust.meanshift, clust.pam, clust.SimpleKMeans,\n#>   clust.xmeans, regr.cv_glmnet, regr.debug, regr.featureless,\n#>   regr.glmnet, regr.kknn, regr.km, regr.lm, regr.nnet, regr.ranger,\n#>   regr.rpart, regr.svm, regr.xgboost\n# lrns()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn(\"regr.rpart\")\n#> <LearnerRegrRpart:regr.rpart>: Regression Tree\n#> * Model: -\n#> * Parameters: xval=0\n#> * Packages: mlr3, rpart\n#> * Predict Types:  [response]\n#> * Feature Types: logical, integer, numeric, factor, ordered\n#> * Properties: importance, missings, selected_features, weights\n```\n:::\n\n\nAll `Learner` objects include the following metadata, which can be seen in the output above:\n\n- `$feature_types`: the type of features the learner can handle.\n\n- `$packages`: the packages required to be installed to use the learner.\n\n- `$properties`: the properties of the learner. For example, the “missings” properties means a model can handle missing data, and “importance” means it can compute the relative importance of each feature.\n\n- `$predict_types`: the types of prediction that the model can make.\n\n- `$param_set`: the set of available hyperparameters.\n\n### Training\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load mtcars task\ntsk_mtcars = tsk(\"mtcars\")\n\n# load a regression tree\nlrn_rpart = lrn(\"regr.rpart\")\n\n# pass the task to the learner via $train()\nlrn_rpart$train(tsk_mtcars)\n```\n:::\n\n\nAfter training, the fitted model is stored in the `$model` field for future inspection and prediction:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_rpart$model\n#> n= 32 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#> 1) root 32 1126.04700 20.09062  \n#>   2) cyl>=5 21  198.47240 16.64762  \n#>     4) hp>=192.5 7   28.82857 13.41429 *\n#>     5) hp< 192.5 14   59.87214 18.26429 *\n#>   3) cyl< 5 11  203.38550 26.66364 *\n\nsplits = partition(tsk_mtcars)\nsplits\n#> $train\n#>  [1]  1  2  3  4  5 21 25 27 32  7 13 15 16 17 22 23 29 31 18 26 28\n#> \n#> $test\n#>  [1]  8  9 10 30  6 11 12 14 24 19 20\n\nlrn_rpart$train(tsk_mtcars, row_ids = splits$train)\n```\n:::\n\n\n### Predicting\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction = lrn_rpart$predict(tsk_mtcars, row_ids = splits$test)\nprediction\n#> <PredictionRegr> for 11 observations:\n#>     row_ids truth response\n#>           8  24.4 24.52000\n#>           9  22.8 24.52000\n#>          10  19.2 24.52000\n#> ---                       \n#>          24  13.3 15.13636\n#>          19  30.4 24.52000\n#>          20  33.9 24.52000\n\nautoplot(prediction)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmtcars_new = data.table(cyl = c(5, 6), disp = c(100, 120),\n  hp = c(100, 150), drat = c(4, 3.9), wt = c(3.8, 4.1),\n  qsec = c(18, 19.5), vs = c(1, 0), am = c(1, 1),\n  gear = c(6, 4), carb = c(3, 5))\nprediction = lrn_rpart$predict_newdata(mtcars_new)\nprediction\n#> <PredictionRegr> for 2 observations:\n#>  row_ids truth response\n#>        1    NA    24.52\n#>        2    NA    24.52\n```\n:::\n\n\n### Hyperparameters\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_rpart$param_set\n#> <ParamSet>\n#>                 id    class lower upper nlevels        default value\n#>  1:             cp ParamDbl     0     1     Inf           0.01      \n#>  2:     keep_model ParamLgl    NA    NA       2          FALSE      \n#>  3:     maxcompete ParamInt     0   Inf     Inf              4      \n#>  4:       maxdepth ParamInt     1    30      30             30      \n#>  5:   maxsurrogate ParamInt     0   Inf     Inf              5      \n#>  6:      minbucket ParamInt     1   Inf     Inf <NoDefault[3]>      \n#>  7:       minsplit ParamInt     1   Inf     Inf             20      \n#>  8: surrogatestyle ParamInt     0     1       2              0      \n#>  9:   usesurrogate ParamInt     0     2       3              2      \n#> 10:           xval ParamInt     0   Inf     Inf             10     0\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# change hyperparameter\nlrn_rpart = lrn(\"regr.rpart\", maxdepth = 1)\n\nlrn_rpart$param_set$values\n#> $xval\n#> [1] 0\n#> \n#> $maxdepth\n#> [1] 1\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# learned regression tree\nlrn_rpart$train(tsk(\"mtcars\"))$model\n#> n= 32 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#> 1) root 32 1126.0470 20.09062  \n#>   2) cyl>=5 21  198.4724 16.64762 *\n#>   3) cyl< 5 11  203.3855 26.66364 *\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# another way to update hyperparameters\nlrn_rpart$param_set$values$maxdepth = 2\nlrn_rpart$param_set$values\n#> $xval\n#> [1] 0\n#> \n#> $maxdepth\n#> [1] 2\n\n# now with depth 2\nlrn_rpart$train(tsk(\"mtcars\"))$model\n#> n= 32 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#> 1) root 32 1126.04700 20.09062  \n#>   2) cyl>=5 21  198.47240 16.64762  \n#>     4) hp>=192.5 7   28.82857 13.41429 *\n#>     5) hp< 192.5 14   59.87214 18.26429 *\n#>   3) cyl< 5 11  203.38550 26.66364 *\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# or with set_values()\nlrn_rpart$param_set$set_values(xval = 2, cp = .5)\nlrn_rpart$param_set$values\n#> $xval\n#> [1] 2\n#> \n#> $maxdepth\n#> [1] 2\n#> \n#> $cp\n#> [1] 0.5\n```\n:::\n\n\n### Baseline Learners\n\nBaselines are useful in model comparison and as fallback learners. For regression, we have implemented the baseline `lrn(\"regr.featureless\")`, which always predicts new values to be the mean (or median, if the `robust` hyperparameter is set to `TRUE`) of the target in the training data:\n\n基线在模型比较和作为备用学习器中非常有用。对于回归问题，我们已经实现了名为 `lrn(\"regr.featureless\")` 的基线，它总是预测新值为训练数据中目标的均值（如果鲁棒性参数设置为 `TRUE`，则为中位数）：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = as_task_regr(data.frame(x = runif(1000), y = rnorm(1000, 2, 1)),\n                    target = \"y\")\nlrn(\"regr.featureless\")$train(task, 1:995)$predict(task, 996:1000)\n#> <PredictionRegr> for 5 observations:\n#>  row_ids    truth response\n#>      996 1.484589 2.034983\n#>      997 3.012537 2.034983\n#>      998 1.964060 2.034983\n#>      999 1.332658 2.034983\n#>     1000 2.923380 2.034983\n```\n:::\n\n\nIt is good practice to test all new models against a baseline, and also to include baselines in experiments with multiple other models. In general, a model that does not outperform a baseline is a ‘bad’ model, on the other hand, a model is not necessarily ‘good’ if it outperforms the baseline.\n\n> 在实践中，对所有新模型进行与基线的测试是一个良好的做法，同时在与多个其他模型进行实验时也要包括基线。通常情况下，如果一个模型无法超越基线，那么它可以被视为是一个不好的模型；另一方面，如果一个模型超越了基线，也不一定就是一个好模型。\n\n## Evaluation\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_rpart = lrn(\"regr.rpart\")\ntsk_mtcars = tsk(\"mtcars\")\nsplits = partition(tsk_mtcars)\nlrn_rpart$train(tsk_mtcars, splits$train)\nprediction = lrn_rpart$predict(tsk_mtcars, splits$test)\n```\n:::\n\n\n### Measures\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(msr())[1:3]\n#>            key                          label task_type          packages\n#> 1:         aic   Akaike Information Criterion      <NA>              mlr3\n#> 2:         bic Bayesian Information Criterion      <NA>              mlr3\n#> 3: classif.acc        Classification Accuracy   classif mlr3,mlr3measures\n#>    predict_type task_properties\n#> 1:         <NA>                \n#> 2:         <NA>                \n#> 3:     response\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmeasure = msr(\"regr.mae\")\nmeasure\n#> <MeasureRegrSimple:regr.mae>: Mean Absolute Error\n#> * Packages: mlr3, mlr3measures\n#> * Range: [0, Inf]\n#> * Minimize: TRUE\n#> * Average: macro\n#> * Parameters: list()\n#> * Properties: -\n#> * Predict type: response\n```\n:::\n\n\n### Scoring Predictions\n\nNote that all task types have default measures that are used if the argument to `$score()` is omitted, for regression this is the mean squared error (`msr(\"regr.mse\")`).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction$score()\n#> regr.mse \n#> 18.44327\nprediction$score(measure)\n#> regr.mae \n#> 3.832168\nprediction$score(msrs(c(\"regr.mse\", \"regr.mae\")))\n#>  regr.mse  regr.mae \n#> 18.443271  3.832168\n```\n:::\n\n\n### Technical Measures\n\n`mlr3` also provides measures that do not quantify the quality of the predictions of a model, but instead provide ‘meta’-information about the model. These include:\n\n- `msr(\"time_train\")`: The time taken to train a model.\n\n- `msr(\"time_predict\")`: The time taken for the model to make predictions.\n\n- `msr(\"time_both\")`: The total time taken to train the model and then make predictions.\n\n- `msr(\"selected_features\")`: The number of features selected by a model, which can only be used if the model has the “selected_features” property.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmeasures = msrs(c(\"time_train\", \"time_predict\", \"time_both\"))\nprediction$score(measures, learner = lrn_rpart)\n#>   time_train time_predict    time_both \n#>            0            0            0\n```\n:::\n\n\nThese can be used after model training and predicting because we automatically store model run times whenever `$train()` and `$predict()` are called, so the measures above are equivalent to:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nc(lrn_rpart$timings, both = sum(lrn_rpart$timings))\n#>   train predict    both \n#>       0       0       0\n```\n:::\n\n\nThe `selected_features` measure calculates how many features were used in the fitted model.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmsr_sf = msr(\"selected_features\")\nmsr_sf\n#> <MeasureSelectedFeatures:selected_features>: Absolute or Relative Frequency of Selected Features\n#> * Packages: mlr3\n#> * Range: [0, Inf]\n#> * Minimize: TRUE\n#> * Average: macro\n#> * Parameters: normalize=FALSE\n#> * Properties: requires_task, requires_learner, requires_model\n#> * Predict type: NA\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# accessed hyperparameters with `$param_set`\nmsr_sf$param_set\n#> <ParamSet>\n#>           id    class lower upper nlevels default value\n#> 1: normalize ParamLgl    NA    NA       2   FALSE FALSE\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmsr_sf$param_set$values$normalize = TRUE\nprediction$score(msr_sf, task = tsk_mtcars, learner = lrn_rpart)\n#> selected_features \n#>               0.1\n```\n:::\n\n\nNote that we passed the task and learner as the measure has the `requires_task` and `requires_learner` properties.\n\n## Our First Regression Experiment\n\nWe have now seen how to train a model, make predictions and score them. What we have not yet attempted is to ascertain if our predictions are any ‘good’. So before look at how the building blocks of `mlr3` extend to classification, we will take a brief pause to put together everything above in a short experiment to assess the quality of our predictions. We will do this by comparing the performance of a featureless regression learner to a decision tree with changed hyperparameters.\n\n> 我们已经了解了如何训练模型、进行预测并对其进行评分。但是，我们尚未尝试确定我们的预测是否“好”。因此，在深入研究 `mlr3` 的构建模块如何扩展到分类之前，我们将简要停顿一下，通过一个简短的实验来评估我们预测的质量。我们将通过比较无特征的回归学习器与更改超参数的决策树的性能来进行评估。\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(349)\ntsk_mtcars = tsk(\"mtcars\")\nsplits = partition(tsk_mtcars)\nlrn_featureless = lrn(\"regr.featureless\")\nlrn_rpart = lrn(\"regr.rpart\", cp = .2, maxdepth = 5)\nmeasures = msrs(c(\"regr.mse\", \"regr.mae\"))\n\n# train learners\nlrn_featureless$train(tsk_mtcars, splits$train)\nlrn_rpart$train(tsk_mtcars, splits$train)\n# make and score predictions\nlrn_featureless$predict(tsk_mtcars, splits$test)$score(measures)\n#>  regr.mse  regr.mae \n#> 26.726772  4.512987\nlrn_rpart$predict(tsk_mtcars, splits$test)$score(measures)\n#> regr.mse regr.mae \n#> 6.932709 2.206494\n```\n:::\n\n\n## Classification\n\n### Our First Classification Experiment\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(349)\ntsk_penguins = tsk(\"penguins\")\nsplits = partition(tsk_penguins)\nlrn_featureless = lrn(\"classif.featureless\")\nlrn_rpart = lrn(\"classif.rpart\", cp = .2, maxdepth = 5)\nmeasure = msr(\"classif.acc\")\n\n# train learners\nlrn_featureless$train(tsk_penguins, splits$train)\nlrn_rpart$train(tsk_penguins, splits$train)\n\n# make and score predictions\nlrn_featureless$predict(tsk_penguins, splits$test)$score(measure)\n#> classif.acc \n#>   0.4424779\nlrn_rpart$predict(tsk_penguins, splits$test)$score(measure)\n#> classif.acc \n#>   0.9469027\n```\n:::\n\n\n### TaskClassif\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(tsks())[task_type == \"classif\"]\n#>                 key                                     label task_type nrow\n#>  1:   breast_cancer                   Wisconsin Breast Cancer   classif  683\n#>  2:   german_credit                             German Credit   classif 1000\n#>  3:            ilpd                 Indian Liver Patient Data   classif  583\n#>  4:            iris                              Iris Flowers   classif  150\n#>  5:       optdigits Optical Recognition of Handwritten Digits   classif 5620\n#>  6:        penguins                           Palmer Penguins   classif  344\n#>  7: penguins_simple                Simplified Palmer Penguins   classif  333\n#>  8:            pima                      Pima Indian Diabetes   classif  768\n#>  9:           sonar                    Sonar: Mines vs. Rocks   classif  208\n#> 10:            spam                         HP Spam Detection   classif 4601\n#> 11:         titanic                                   Titanic   classif 1309\n#> 12:            wine                              Wine Regions   classif  178\n#> 13:             zoo                               Zoo Animals   classif  101\n#>     ncol properties lgl int dbl chr fct ord pxc\n#>  1:   10   twoclass   0   0   0   0   0   9   0\n#>  2:   21   twoclass   0   3   0   0  14   3   0\n#>  3:   11   twoclass   0   4   5   0   1   0   0\n#>  4:    5 multiclass   0   0   4   0   0   0   0\n#>  5:   65   twoclass   0  64   0   0   0   0   0\n#>  6:    8 multiclass   0   3   2   0   2   0   0\n#>  7:   11 multiclass   0   3   7   0   0   0   0\n#>  8:    9   twoclass   0   0   8   0   0   0   0\n#>  9:   61   twoclass   0   0  60   0   0   0   0\n#> 10:   58   twoclass   0   0  57   0   0   0   0\n#> 11:   11   twoclass   0   2   2   3   2   1   0\n#> 12:   14 multiclass   0   2  11   0   0   0   0\n#> 13:   17 multiclass  15   1   0   0   0   0   0\n```\n:::\n\n\nThe `sonar` task is an example of a binary classification problem, as the target can only take two different values, in `mlr3` terminology it has the “twoclass” property:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_sonar = tsk(\"sonar\")\ntsk_sonar\n#> <TaskClassif:sonar> (208 x 61): Sonar: Mines vs. Rocks\n#> * Target: Class\n#> * Properties: twoclass\n#> * Features (60):\n#>   - dbl (60): V1, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V2,\n#>     V20, V21, V22, V23, V24, V25, V26, V27, V28, V29, V3, V30, V31,\n#>     V32, V33, V34, V35, V36, V37, V38, V39, V4, V40, V41, V42, V43,\n#>     V44, V45, V46, V47, V48, V49, V5, V50, V51, V52, V53, V54, V55,\n#>     V56, V57, V58, V59, V6, V60, V7, V8, V9\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_sonar$class_names\n#> [1] \"M\" \"R\"\n```\n:::\n\n\nIn contrast, `tsk(\"penguins\")` is a multiclass problem as there are more than two species of penguins; it has the “multiclass” property:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_penguins = tsk(\"penguins\")\ntsk_penguins$properties\n#> [1] \"multiclass\"\ntsk_penguins$class_names\n#> [1] \"Adelie\"    \"Chinstrap\" \"Gentoo\"\n```\n:::\n\n\nA further difference between these tasks is that binary classification tasks have an extra field called `$positive`, which defines the ‘positive’ class. In binary classification, as there are only two possible class types, by convention one of these is known as the ‘positive’ class, and the other as the ‘negative’ class. It is arbitrary which is which, though often the more ‘important’ (and often smaller) class is set as the positive class. You can set the positive class during or after construction. If no positive class is specified then `mlr3` assumes the first level in the `target` column is the positive class, which can lead to misleading results.\n\n> 这两种任务之间的另一个区别是，二分类任务有一个额外的字段称为 `$positive`，它定义了“正类”（positive class）。在二分类问题中，由于只有两种可能的类别类型，按照惯例，其中一种被称为“正类”，另一种被称为“负类”。哪个是哪个是任意的，尽管通常更“重要”（通常更小）的类别被设置为正类。您可以在构建期间或之后设置正类。如果未指定正类，则 `mlr3` 假定目标列中的第一个级别是正类，这可能导致误导性的结果。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nSonar = tsk_sonar$data()\ntsk_classif = as_task_classif(Sonar, target = \"Class\", positive = \"R\")\ntsk_classif$positive\n#> [1] \"R\"\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# changing after construction\ntsk_classif$positive = \"M\"\ntsk_classif$positive\n#> [1] \"M\"\n```\n:::\n\n\n### LearnerClassif and MeasureClassif\n\nClassification learners, which inherit from `LearnerClassif`, have nearly the same interface as regression learners. However, a key difference is that the possible predictions in classification are either `\"response\"` – predicting an observation’s class (a penguin’s species in our example, this is sometimes called “hard labeling”) – or `\"prob\"` – predicting a vector of probabilities, also called “posterior probabilities”, of an observation belonging to each class. In classification, the latter can be more useful as it provides information about the confidence of the predictions:\n\n> 分类学习器（继承自 `LearnerClassif`）几乎具有与回归学习器相同的接口。然而，分类中的一个关键区别是，分类问题中可能的预测结果要么是 `\"response\"` （预测观测的类别，例如我们示例中的企鹅物种，有时称为“硬标签”），要么是 `\"prob\"` （预测属于每个类别的概率向量，也称为“后验概率”）。在分类中，后者可能更有用，因为它提供了有关预测的置信度信息：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_rpart = lrn(\"classif.rpart\", predict_type = \"prob\")\nlrn_rpart$train(tsk_penguins, splits$train)\nprediction = lrn_rpart$predict(tsk_penguins, splits$test)\nprediction\n#> <PredictionClassif> for 113 observations:\n#>     row_ids     truth  response prob.Adelie prob.Chinstrap prob.Gentoo\n#>           2    Adelie    Adelie  0.97029703     0.02970297  0.00000000\n#>           4    Adelie    Adelie  0.97029703     0.02970297  0.00000000\n#>           7    Adelie    Adelie  0.97029703     0.02970297  0.00000000\n#> ---                                                                   \n#>         338 Chinstrap Chinstrap  0.04651163     0.93023256  0.02325581\n#>         341 Chinstrap    Adelie  0.97029703     0.02970297  0.00000000\n#>         344 Chinstrap Chinstrap  0.04651163     0.93023256  0.02325581\n```\n:::\n\n\nAlso, the interface for classification measures, which are of class `MeasureClassif`, is identical to regression measures. The key difference in usage is that you will need to ensure your selected measure evaluates the prediction type of interest. To evaluate \"response\" predictions, you will need measures with `predict_type = \"response\"`, or to evaluate probability predictions you will need `predict_type = \"prob\"`. The easiest way to find these measures is by filtering the `mlr_measures` dictionary:\n\n> 此外，分类度量标准的接口，其类别为 `MeasureClassif`，与回归度量标准完全相同。在使用上的主要区别在于，您需要确保所选的度量标准评估感兴趣的预测类型。要评估 `“response”` 预测，您需要使用 `predict_type = \"response\"` 的度量标准，或者要评估概率预测，您需要使用 `predict_type = \"prob\"` 的度量标准。查找这些度量标准的最简单方法是通过筛选 `mlr_measures` 字典：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(msr())[\n  task_type == \"classif\" & predict_type == \"prob\" &\n  !sapply(task_properties, \\(x) \"twoclass\" %in% x)\n]\n#>                  key                                      label task_type\n#> 1:   classif.logloss                                   Log Loss   classif\n#> 2: classif.mauc_au1p    Weighted average 1 vs. 1 multiclass AUC   classif\n#> 3: classif.mauc_au1u             Average 1 vs. 1 multiclass AUC   classif\n#> 4: classif.mauc_aunp Weighted average 1 vs. rest multiclass AUC   classif\n#> 5: classif.mauc_aunu          Average 1 vs. rest multiclass AUC   classif\n#> 6:    classif.mbrier                     Multiclass Brier Score   classif\n#>             packages predict_type task_properties\n#> 1: mlr3,mlr3measures         prob                \n#> 2: mlr3,mlr3measures         prob                \n#> 3: mlr3,mlr3measures         prob                \n#> 4: mlr3,mlr3measures         prob                \n#> 5: mlr3,mlr3measures         prob                \n#> 6: mlr3,mlr3measures         prob\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmeasures = msrs(c(\"classif.mbrier\", \"classif.logloss\", \"classif.acc\"))\nprediction$score(measures)\n#>  classif.mbrier classif.logloss     classif.acc \n#>       0.1016821       0.2291407       0.9469027\n```\n:::\n\n\n### PredictionClassif, Confusion Matrix, and Thresholding\n\n`PredictionClassif` objects have two important differences from their regression analog. Firstly, the added field `$confusion`, and secondly the added method `$set_threshold()`.\n\n> `PredictionClassif` 对象与其回归模型的预测对象有两个重要的区别。首先是新增的字段 `$confusion`，其次是新增的方法 `$set_threshold()`。\n\n#### Confusion Matrix\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction$confusion\n#>            truth\n#> response    Adelie Chinstrap Gentoo\n#>   Adelie        49         3      0\n#>   Chinstrap      1        18      1\n#>   Gentoo         0         1     40\n```\n:::\n\n\nThe rows in a confusion matrix are the predicted class and the columns are the true class. All off-diagonal entries are incorrectly classified observations, and all diagonal entries are correctly classified. In this case, the classifier does fairly well classifying all penguins, but we could have found that it only classifies the Adelie species well but often conflates Chinstrap and Gentoo, for example.\n\n> 混淆矩阵中的行表示预测的类别，列表示真实的类别。所有非对角线条目都是被错误分类的观测值，而所有对角线条目都是被正确分类的。在这种情况下，分类器在对所有企鹅进行分类时表现得相当不错，但我们也可能发现它只能很好地对 Adelie 物种进行分类，但经常将 Chinstrap 和 Gentoo 混为一谈。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(prediction)\n```\n\n::: {.cell-output-display}\n![Counts of each class label in the ground truth data (left) and predictions (right).](index_files/figure-html/fig-confusion_matrix-1.png){#fig-confusion_matrix fig-align='center' width=70%}\n:::\n:::\n\n\nIn the binary classification case, the top left entry corresponds to true positives, the top right to false positives, the bottom left to false negatives and the bottom right to true negatives. Taking `tsk_sonar` as an example with `M` as the positive class:\n\n> 在二分类情况下，左上角的条目对应于真正例（true positives），右上角对应于假正例（false positives），左下角对应于假负例（false negatives），右下角对应于真负例（true negatives）。以 `tsk_sonar` 为例，`M` 为正类：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsplits = partition(tsk_sonar)\nlrn_rpart$\n  train(tsk_sonar, splits$train)$\n  predict(tsk_sonar, splits$test)$\n  confusion\n#>         truth\n#> response  M  R\n#>        M 27 10\n#>        R 10 22\n```\n:::\n\n\n#### Thresholding\n\n**阈值化**\n\nThis 50% value is known as the threshold and it can be useful to change this threshold if there is class imbalance (when one class is over- or under-represented in a dataset), or if there are different costs associated with classes, or simply if there is a preference to ‘over’-predict one class. As an example, let us take `tsk(\"german_credit\")` in which 700 customers have good credit and 300 have bad. Now we could easily build a model with around “70%” accuracy simply by always predicting a customer will have good credit:\n\n> 这个 50% 的值被称为阈值，如果数据集中存在类别不平衡（即一个类别在数据集中过多或过少出现），或者不同的类别具有不同的成本，或者只是有一种“过度”预测一种类别的倾向，那么更改这个阈值可能会很有用。举个例子，让我们看看 `tsk(\"german_credit\")`，其中有 700 个客户信用良好，300 个客户信用不良。现在，我们可以很容易地构建一个模型，总是预测客户会有良好的信用，从而获得 “70%” 左右的准确性：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask_credit = tsk(\"german_credit\")\nlrn_featureless = lrn(\"classif.featureless\", predict_type = \"prob\")\nsplits = partition(task_credit)\nlrn_featureless$train(task_credit, splits$train)\nprediction = lrn_featureless$predict(task_credit, splits$test)\nprediction$score(msr(\"classif.acc\"))\n#> classif.acc \n#>         0.7\n```\n:::\n\n\n::: {.callout-caution}\nTODO：等待后续添加交叉引用\n:::\n\nWhile this model may appear to have good performance on the surface, in fact, it just ignores all ‘bad’ customers – this can create big problems in this finance example, as well as in healthcare tasks and other settings where false positives cost more than false negatives (see Section 13.1 for cost-sensitive classification).\n\nThresholding allows classes to be selected with a different probability threshold, so instead of predicting that a customer has bad credit if P(good) < 50%, we might predict bad credit if P(good) < 70% – notice how we write this in terms of the positive class, which in this task is ‘good’. Let us see this in practice:\n\n> 虽然这个模型表面上看起来性能不错，但实际上它只是忽略了所有“不良”的客户 - 这在金融示例以及在医疗任务和其他一些情况下可能会带来很大问题，特别是在假阳性的成本高于假阴性的情况下（请参见第13.1节的成本敏感分类）。\n>\n> 阈值化允许使用不同的概率阈值选择类别，因此，与其在P(好) < 50%时预测客户信用不良，我们可以在P(好) < 70%时预测客户信用不良。请注意，我们是根据正类别来表示这一点，而在这个任务中正类别是“好”。让我们看看实际应用中的情况：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction$set_threshold(0.7)\nprediction$score(msr(\"classif.acc\"))\n#> classif.acc \n#>   0.5393939\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_rpart = lrn(\"classif.rpart\", predict_type = \"prob\")\nlrn_rpart$train(task_credit, splits$train)\nprediction = lrn_rpart$predict(task_credit, splits$test)\nprediction$score(msr(\"classif.acc\"))\n#> classif.acc \n#>   0.6939394\nprediction$confusion\n#>         truth\n#> response good bad\n#>     good  194  64\n#>     bad    37  35\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction$set_threshold(0.7)\nprediction$score(msr(\"classif.acc\"))\n#> classif.acc \n#>   0.6878788\nprediction$confusion\n#>         truth\n#> response good bad\n#>     good  181  53\n#>     bad    50  46\n```\n:::\n\n\n# Evaluation and Benchmarking\n\n**Resampling Does Not Avoid Model Overfitting**: \nA common **misunderstanding** is that holdout and other more advanced resampling strategies can prevent model overfitting. In fact, these methods just make overfitting visible as we can separately evaluate train/test performance. Resampling strategies also allow us to make (nearly) unbiased estimations of the generalization error.\n\n> **重采样不能避免模型过拟合**：一个常见的误解是，留出策略和其他更高级的重采样策略可以防止模型过拟合。实际上，这些方法只是使过拟合问题更加显而易见，因为我们可以单独评估训练/测试性能。重采样策略还允许我们对泛化误差进行（几乎）无偏估计。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.callout-tip title=\"To be continued\"}\n- <https://mlr3book.mlr-org.com/chapters/chapter3/evaluation_and_benchmarking.html#sec-holdout-scoring>\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}