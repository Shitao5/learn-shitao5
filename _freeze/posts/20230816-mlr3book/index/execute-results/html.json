{
  "hash": "702f3ddb31daab15588f0bac04d93fc1",
  "result": {
    "markdown": "---\ntitle: \"Applied Machine Learning Using mlr3 in R\"\ndate: \"2023-08-16\"\ndate-modified: \"2023-10-11\"\nimage: \"logo.png\"\ncategories: \n  - Machine Learning\n  - R\n  - mlr3\n---\n\n\n\n\n::: {.callout-note title='Progress'}\nLearning Progress: 34.67%.\n:::\n\n::: {.callout-tip title=\"Learning Source\"}\n- <https://mlr3book.mlr-org.com/>\n- 中文翻译由 ChatGPT 3.5 提供\n:::\n\n# Getting Started {.unnumbered}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\nlibrary(mlr3pipelines)\nlibrary(mlr3benchmark)\nlibrary(ggplot2)\nlibrary(patchwork)\n```\n:::\n\n\n\n\n# Introduction and Overview\n\n`mlr3` by Example:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\n\ntask = tsk(\"penguins\")\nsplit = partition(task)\nlearner = lrn(\"classif.rpart\")\n\nlearner$train(task, row_ids = split$train)\nlearner$model\n#> n= 231 \n#> \n#> node), split, n, loss, yval, (yprob)\n#>       * denotes terminal node\n#> \n#> 1) root 231 129 Adelie (0.441558442 0.199134199 0.359307359)  \n#>   2) flipper_length< 206.5 144  44 Adelie (0.694444444 0.298611111 0.006944444)  \n#>     4) bill_length< 43.05 98   3 Adelie (0.969387755 0.030612245 0.000000000) *\n#>     5) bill_length>=43.05 46   6 Chinstrap (0.108695652 0.869565217 0.021739130) *\n#>   3) flipper_length>=206.5 87   5 Gentoo (0.022988506 0.034482759 0.942528736) *\n\nprediction = learner$predict(task, row_ids = split$test)\nprediction\n#> <PredictionClassif> for 113 observations:\n#>     row_ids     truth  response\n#>           1    Adelie    Adelie\n#>           2    Adelie    Adelie\n#>           3    Adelie    Adelie\n#> ---                            \n#>         328 Chinstrap Chinstrap\n#>         331 Chinstrap    Adelie\n#>         339 Chinstrap Chinstrap\n\nprediction$score(msr(\"classif.acc\"))\n#> classif.acc \n#>   0.9557522\n```\n:::\n\n\nThe `mlr3` interface also lets you run more complicated experiments in just a few lines of code:\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\nWe use dictionaries to group large collections of relevant objects so they can be listed and retrieved easily.\nFor example, you can see an overview of available learners (that are in loaded packages) and their properties with `as.data.table(mlr_learners)` or by calling the sugar function without any arguments, e.g. `lrn()`.\n\n> 我们使用字典来分组大量相关对象，以便可以轻松地列出和检索它们。例如，您可以通过 `as.data.table(mlr_learners)` 查看可用学习器（位于加载的包中）及其属性的概述，或者通过调用糖函数而不带任何参数，例如 `lrn()`。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(mlr_learners)[1:3]\n#>                    key                              label task_type\n#> 1:   classif.cv_glmnet                               <NA>   classif\n#> 2:       classif.debug   Debug Learner for Classification   classif\n#> 3: classif.featureless Featureless Classification Learner   classif\n#>                                           feature_types\n#> 1:                              logical,integer,numeric\n#> 2:     logical,integer,numeric,character,factor,ordered\n#> 3: logical,integer,numeric,character,factor,ordered,...\n#>                    packages\n#> 1: mlr3,mlr3learners,glmnet\n#> 2:                     mlr3\n#> 3:                     mlr3\n#>                                                               properties\n#> 1:                         multiclass,selected_features,twoclass,weights\n#> 2:                         hotstart_forward,missings,multiclass,twoclass\n#> 3: featureless,importance,missings,multiclass,selected_features,twoclass\n#>    predict_types\n#> 1: response,prob\n#> 2: response,prob\n#> 3: response,prob\n```\n:::\n\n\n# Fundamentals {.unnumbered}\n\n# Data and Basic Modeling\n\n## Tasks\n\n### Constructing Tasks\n\n`mlr3` includes a few predefined machine learning tasks in the `mlr_tasks` Dictionary.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlr_tasks\n#> <DictionaryTask> with 21 stored values\n#> Keys: ames_housing, bike_sharing, boston_housing, breast_cancer,\n#>   german_credit, ilpd, iris, kc_housing, moneyball, mtcars, optdigits,\n#>   penguins, penguins_simple, pima, ruspini, sonar, spam, titanic,\n#>   usarrests, wine, zoo\n# the same as \n# tsk()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_mtcars = tsk(\"mtcars\")\ntsk_mtcars\n#> <TaskRegr:mtcars> (32 x 11): Motor Trends\n#> * Target: mpg\n#> * Properties: -\n#> * Features (10):\n#>   - dbl (10): am, carb, cyl, disp, drat, gear, hp, qsec, vs, wt\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# create my own regression task\ndata(\"mtcars\", package = \"datasets\")\nmtcars_subset = subset(mtcars, select = c(\"mpg\", \"cyl\", \"disp\"))\ntsk_mtcars = as_task_regr(mtcars_subset, target = \"mpg\", id = \"cars\")\ntsk_mtcars\n#> <TaskRegr:cars> (32 x 3)\n#> * Target: mpg\n#> * Properties: -\n#> * Features (2):\n#>   - dbl (2): cyl, disp\n```\n:::\n\n\nThe `id` argument is optional and specifies an identifier for the task that is used in plots and summaries; if omitted the variable name of the data will be used as the `id`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3viz)\nautoplot(tsk_mtcars, type = \"pairs\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n### Retrieving Data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nc(tsk_mtcars$nrow, tsk_mtcars$ncol)\n#> [1] 32  3\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nc(Features = tsk_mtcars$feature_names,\n  Target = tsk_mtcars$target_names)\n#> Features1 Features2    Target \n#>     \"cyl\"    \"disp\"     \"mpg\"\n```\n:::\n\n\nRow IDs are not used as features when training or predicting but are metadata that allow access to individual observations. Note that row IDs are not the same as row numbers.\n\nThis design decision allows tasks and learners to transparently operate on real database management systems, where primary keys are required to be unique, but not necessarily consecutive.\n\n> 行ID在训练或预测时不作为特征使用，而是元数据，用于访问个别观测数据。需要注意的是，行ID与行号不同。\n>\n> 这种设计决策使得任务和学习器能够透明地在真实的数据库管理系统上运行，其中要求主键是唯一的，但不一定连续。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = as_task_regr(data.frame(x = runif(5), y = runif(5)),\n                    target = \"y\")\ntask$row_ids\n#> [1] 1 2 3 4 5\n\ntask$filter(c(4, 1, 3))\ntask$row_ids\n#> [1] 1 3 4\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_mtcars$data()[1:3]\n#>     mpg cyl disp\n#> 1: 21.0   6  160\n#> 2: 21.0   6  160\n#> 3: 22.8   4  108\ntsk_mtcars$data(rows = c(1, 5, 10), cols = tsk_mtcars$feature_names)\n#>    cyl  disp\n#> 1:   6 160.0\n#> 2:   8 360.0\n#> 3:   6 167.6\n```\n:::\n\n\n### Task Mutators\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_mtcars_small = tsk(\"mtcars\")\ntsk_mtcars_small$select(\"cyl\")\ntsk_mtcars_small$filter(2:3)\ntsk_mtcars_small$data()\n#>     mpg cyl\n#> 1: 21.0   6\n#> 2: 22.8   4\n```\n:::\n\n\nAs `R6` uses reference semantics, you need to use `$clone()` if you want to modify a task while keeping the original object intact.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_mtcars = tsk(\"mtcars\")\ntsk_mtcars_clone = tsk_mtcars$clone()\ntsk_mtcars_clone$filter(1:2)\ntsk_mtcars_clone$head()\n#>    mpg am carb cyl disp drat gear  hp  qsec vs    wt\n#> 1:  21  1    4   6  160  3.9    4 110 16.46  0 2.620\n#> 2:  21  1    4   6  160  3.9    4 110 17.02  0 2.875\n```\n:::\n\n\nTo add extra rows and columns to a task, you can use `$rbind()` and `$cbind()` respectively:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_mtcars_small\n#> <TaskRegr:mtcars> (2 x 2): Motor Trends\n#> * Target: mpg\n#> * Properties: -\n#> * Features (1):\n#>   - dbl (1): cyl\ntsk_mtcars_small$cbind(data.frame(disp = c(150, 160)))\ntsk_mtcars_small$rbind(data.frame(mpg = 23, cyl = 5, disp = 170))\ntsk_mtcars_small$data()\n#>     mpg cyl disp\n#> 1: 21.0   6  150\n#> 2: 22.8   4  160\n#> 3: 23.0   5  170\n```\n:::\n\n\n## Learners\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# all the learners available in mlr3\nmlr_learners\n#> <DictionaryLearner> with 46 stored values\n#> Keys: classif.cv_glmnet, classif.debug, classif.featureless,\n#>   classif.glmnet, classif.kknn, classif.lda, classif.log_reg,\n#>   classif.multinom, classif.naive_bayes, classif.nnet, classif.qda,\n#>   classif.ranger, classif.rpart, classif.svm, classif.xgboost,\n#>   clust.agnes, clust.ap, clust.cmeans, clust.cobweb, clust.dbscan,\n#>   clust.diana, clust.em, clust.fanny, clust.featureless, clust.ff,\n#>   clust.hclust, clust.kkmeans, clust.kmeans, clust.MBatchKMeans,\n#>   clust.mclust, clust.meanshift, clust.pam, clust.SimpleKMeans,\n#>   clust.xmeans, regr.cv_glmnet, regr.debug, regr.featureless,\n#>   regr.glmnet, regr.kknn, regr.km, regr.lm, regr.nnet, regr.ranger,\n#>   regr.rpart, regr.svm, regr.xgboost\n# lrns()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn(\"regr.rpart\")\n#> <LearnerRegrRpart:regr.rpart>: Regression Tree\n#> * Model: -\n#> * Parameters: xval=0\n#> * Packages: mlr3, rpart\n#> * Predict Types:  [response]\n#> * Feature Types: logical, integer, numeric, factor, ordered\n#> * Properties: importance, missings, selected_features, weights\n```\n:::\n\n\nAll `Learner` objects include the following metadata, which can be seen in the output above:\n\n- `$feature_types`: the type of features the learner can handle.\n\n- `$packages`: the packages required to be installed to use the learner.\n\n- `$properties`: the properties of the learner. For example, the “missings” properties means a model can handle missing data, and “importance” means it can compute the relative importance of each feature.\n\n- `$predict_types`: the types of prediction that the model can make.\n\n- `$param_set`: the set of available hyperparameters.\n\n### Training\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load mtcars task\ntsk_mtcars = tsk(\"mtcars\")\n\n# load a regression tree\nlrn_rpart = lrn(\"regr.rpart\")\n\n# pass the task to the learner via $train()\nlrn_rpart$train(tsk_mtcars)\n```\n:::\n\n\nAfter training, the fitted model is stored in the `$model` field for future inspection and prediction:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_rpart$model\n#> n= 32 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#> 1) root 32 1126.04700 20.09062  \n#>   2) cyl>=5 21  198.47240 16.64762  \n#>     4) hp>=192.5 7   28.82857 13.41429 *\n#>     5) hp< 192.5 14   59.87214 18.26429 *\n#>   3) cyl< 5 11  203.38550 26.66364 *\n\nsplits = partition(tsk_mtcars)\nsplits\n#> $train\n#>  [1]  1  2  3  4  5 21 25 27 32  7 13 15 16 17 22 23 29 31 18 26 28\n#> \n#> $test\n#>  [1]  8  9 10 30  6 11 12 14 24 19 20\n\nlrn_rpart$train(tsk_mtcars, row_ids = splits$train)\n```\n:::\n\n\n### Predicting\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction = lrn_rpart$predict(tsk_mtcars, row_ids = splits$test)\nprediction\n#> <PredictionRegr> for 11 observations:\n#>     row_ids truth response\n#>           8  24.4 24.52000\n#>           9  22.8 24.52000\n#>          10  19.2 24.52000\n#> ---                       \n#>          24  13.3 15.13636\n#>          19  30.4 24.52000\n#>          20  33.9 24.52000\n\nautoplot(prediction)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmtcars_new = data.table(cyl = c(5, 6), disp = c(100, 120),\n  hp = c(100, 150), drat = c(4, 3.9), wt = c(3.8, 4.1),\n  qsec = c(18, 19.5), vs = c(1, 0), am = c(1, 1),\n  gear = c(6, 4), carb = c(3, 5))\nprediction = lrn_rpart$predict_newdata(mtcars_new)\nprediction\n#> <PredictionRegr> for 2 observations:\n#>  row_ids truth response\n#>        1    NA    24.52\n#>        2    NA    24.52\n```\n:::\n\n\n### Hyperparameters\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_rpart$param_set\n#> <ParamSet>\n#>                 id    class lower upper nlevels        default value\n#>  1:             cp ParamDbl     0     1     Inf           0.01      \n#>  2:     keep_model ParamLgl    NA    NA       2          FALSE      \n#>  3:     maxcompete ParamInt     0   Inf     Inf              4      \n#>  4:       maxdepth ParamInt     1    30      30             30      \n#>  5:   maxsurrogate ParamInt     0   Inf     Inf              5      \n#>  6:      minbucket ParamInt     1   Inf     Inf <NoDefault[3]>      \n#>  7:       minsplit ParamInt     1   Inf     Inf             20      \n#>  8: surrogatestyle ParamInt     0     1       2              0      \n#>  9:   usesurrogate ParamInt     0     2       3              2      \n#> 10:           xval ParamInt     0   Inf     Inf             10     0\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# change hyperparameter\nlrn_rpart = lrn(\"regr.rpart\", maxdepth = 1)\n\nlrn_rpart$param_set$values\n#> $xval\n#> [1] 0\n#> \n#> $maxdepth\n#> [1] 1\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# learned regression tree\nlrn_rpart$train(tsk(\"mtcars\"))$model\n#> n= 32 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#> 1) root 32 1126.0470 20.09062  \n#>   2) cyl>=5 21  198.4724 16.64762 *\n#>   3) cyl< 5 11  203.3855 26.66364 *\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# another way to update hyperparameters\nlrn_rpart$param_set$values$maxdepth = 2\nlrn_rpart$param_set$values\n#> $xval\n#> [1] 0\n#> \n#> $maxdepth\n#> [1] 2\n\n# now with depth 2\nlrn_rpart$train(tsk(\"mtcars\"))$model\n#> n= 32 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#> 1) root 32 1126.04700 20.09062  \n#>   2) cyl>=5 21  198.47240 16.64762  \n#>     4) hp>=192.5 7   28.82857 13.41429 *\n#>     5) hp< 192.5 14   59.87214 18.26429 *\n#>   3) cyl< 5 11  203.38550 26.66364 *\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# or with set_values()\nlrn_rpart$param_set$set_values(xval = 2, cp = .5)\nlrn_rpart$param_set$values\n#> $xval\n#> [1] 2\n#> \n#> $maxdepth\n#> [1] 2\n#> \n#> $cp\n#> [1] 0.5\n```\n:::\n\n\n### Baseline Learners\n\nBaselines are useful in model comparison and as fallback learners. For regression, we have implemented the baseline `lrn(\"regr.featureless\")`, which always predicts new values to be the mean (or median, if the `robust` hyperparameter is set to `TRUE`) of the target in the training data:\n\n基线在模型比较和作为备用学习器中非常有用。对于回归问题，我们已经实现了名为 `lrn(\"regr.featureless\")` 的基线，它总是预测新值为训练数据中目标的均值（如果鲁棒性参数设置为 `TRUE`，则为中位数）：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = as_task_regr(data.frame(x = runif(1000), y = rnorm(1000, 2, 1)),\n                    target = \"y\")\nlrn(\"regr.featureless\")$train(task, 1:995)$predict(task, 996:1000)\n#> <PredictionRegr> for 5 observations:\n#>  row_ids    truth response\n#>      996 1.484589 2.034983\n#>      997 3.012537 2.034983\n#>      998 1.964060 2.034983\n#>      999 1.332658 2.034983\n#>     1000 2.923380 2.034983\n```\n:::\n\n\nIt is good practice to test all new models against a baseline, and also to include baselines in experiments with multiple other models. In general, a model that does not outperform a baseline is a ‘bad’ model, on the other hand, a model is not necessarily ‘good’ if it outperforms the baseline.\n\n> 在实践中，对所有新模型进行与基线的测试是一个良好的做法，同时在与多个其他模型进行实验时也要包括基线。通常情况下，如果一个模型无法超越基线，那么它可以被视为是一个不好的模型；另一方面，如果一个模型超越了基线，也不一定就是一个好模型。\n\n## Evaluation\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_rpart = lrn(\"regr.rpart\")\ntsk_mtcars = tsk(\"mtcars\")\nsplits = partition(tsk_mtcars)\nlrn_rpart$train(tsk_mtcars, splits$train)\nprediction = lrn_rpart$predict(tsk_mtcars, splits$test)\n```\n:::\n\n\n### Measures\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(msr())[1:3]\n#>            key                          label task_type          packages\n#> 1:         aic   Akaike Information Criterion      <NA>              mlr3\n#> 2:         bic Bayesian Information Criterion      <NA>              mlr3\n#> 3: classif.acc        Classification Accuracy   classif mlr3,mlr3measures\n#>    predict_type task_properties\n#> 1:         <NA>                \n#> 2:         <NA>                \n#> 3:     response\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmeasure = msr(\"regr.mae\")\nmeasure\n#> <MeasureRegrSimple:regr.mae>: Mean Absolute Error\n#> * Packages: mlr3, mlr3measures\n#> * Range: [0, Inf]\n#> * Minimize: TRUE\n#> * Average: macro\n#> * Parameters: list()\n#> * Properties: -\n#> * Predict type: response\n```\n:::\n\n\n### Scoring Predictions\n\nNote that all task types have default measures that are used if the argument to `$score()` is omitted, for regression this is the mean squared error (`msr(\"regr.mse\")`).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction$score()\n#> regr.mse \n#> 18.44327\nprediction$score(measure)\n#> regr.mae \n#> 3.832168\nprediction$score(msrs(c(\"regr.mse\", \"regr.mae\")))\n#>  regr.mse  regr.mae \n#> 18.443271  3.832168\n```\n:::\n\n\n### Technical Measures\n\n`mlr3` also provides measures that do not quantify the quality of the predictions of a model, but instead provide ‘meta’-information about the model. These include:\n\n- `msr(\"time_train\")`: The time taken to train a model.\n\n- `msr(\"time_predict\")`: The time taken for the model to make predictions.\n\n- `msr(\"time_both\")`: The total time taken to train the model and then make predictions.\n\n- `msr(\"selected_features\")`: The number of features selected by a model, which can only be used if the model has the “selected_features” property.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmeasures = msrs(c(\"time_train\", \"time_predict\", \"time_both\"))\nprediction$score(measures, learner = lrn_rpart)\n#>   time_train time_predict    time_both \n#>         0.01         0.00         0.01\n```\n:::\n\n\nThese can be used after model training and predicting because we automatically store model run times whenever `$train()` and `$predict()` are called, so the measures above are equivalent to:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nc(lrn_rpart$timings, both = sum(lrn_rpart$timings))\n#>   train predict    both \n#>    0.01    0.00    0.01\n```\n:::\n\n\nThe `selected_features` measure calculates how many features were used in the fitted model.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmsr_sf = msr(\"selected_features\")\nmsr_sf\n#> <MeasureSelectedFeatures:selected_features>: Absolute or Relative Frequency of Selected Features\n#> * Packages: mlr3\n#> * Range: [0, Inf]\n#> * Minimize: TRUE\n#> * Average: macro\n#> * Parameters: normalize=FALSE\n#> * Properties: requires_task, requires_learner, requires_model\n#> * Predict type: NA\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# accessed hyperparameters with `$param_set`\nmsr_sf$param_set\n#> <ParamSet>\n#>           id    class lower upper nlevels default value\n#> 1: normalize ParamLgl    NA    NA       2   FALSE FALSE\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmsr_sf$param_set$values$normalize = TRUE\nprediction$score(msr_sf, task = tsk_mtcars, learner = lrn_rpart)\n#> selected_features \n#>               0.1\n```\n:::\n\n\nNote that we passed the task and learner as the measure has the `requires_task` and `requires_learner` properties.\n\n## Our First Regression Experiment\n\nWe have now seen how to train a model, make predictions and score them. What we have not yet attempted is to ascertain if our predictions are any ‘good’. So before look at how the building blocks of `mlr3` extend to classification, we will take a brief pause to put together everything above in a short experiment to assess the quality of our predictions. We will do this by comparing the performance of a featureless regression learner to a decision tree with changed hyperparameters.\n\n> 我们已经了解了如何训练模型、进行预测并对其进行评分。但是，我们尚未尝试确定我们的预测是否“好”。因此，在深入研究 `mlr3` 的构建模块如何扩展到分类之前，我们将简要停顿一下，通过一个简短的实验来评估我们预测的质量。我们将通过比较无特征的回归学习器与更改超参数的决策树的性能来进行评估。\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(349)\ntsk_mtcars = tsk(\"mtcars\")\nsplits = partition(tsk_mtcars)\nlrn_featureless = lrn(\"regr.featureless\")\nlrn_rpart = lrn(\"regr.rpart\", cp = .2, maxdepth = 5)\nmeasures = msrs(c(\"regr.mse\", \"regr.mae\"))\n\n# train learners\nlrn_featureless$train(tsk_mtcars, splits$train)\nlrn_rpart$train(tsk_mtcars, splits$train)\n# make and score predictions\nlrn_featureless$predict(tsk_mtcars, splits$test)$score(measures)\n#>  regr.mse  regr.mae \n#> 26.726772  4.512987\nlrn_rpart$predict(tsk_mtcars, splits$test)$score(measures)\n#> regr.mse regr.mae \n#> 6.932709 2.206494\n```\n:::\n\n\n## Classification\n\n### Our First Classification Experiment\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(349)\ntsk_penguins = tsk(\"penguins\")\nsplits = partition(tsk_penguins)\nlrn_featureless = lrn(\"classif.featureless\")\nlrn_rpart = lrn(\"classif.rpart\", cp = .2, maxdepth = 5)\nmeasure = msr(\"classif.acc\")\n\n# train learners\nlrn_featureless$train(tsk_penguins, splits$train)\nlrn_rpart$train(tsk_penguins, splits$train)\n\n# make and score predictions\nlrn_featureless$predict(tsk_penguins, splits$test)$score(measure)\n#> classif.acc \n#>   0.4424779\nlrn_rpart$predict(tsk_penguins, splits$test)$score(measure)\n#> classif.acc \n#>   0.9469027\n```\n:::\n\n\n### TaskClassif\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(tsks())[task_type == \"classif\"]\n#>                 key                                     label task_type nrow\n#>  1:   breast_cancer                   Wisconsin Breast Cancer   classif  683\n#>  2:   german_credit                             German Credit   classif 1000\n#>  3:            ilpd                 Indian Liver Patient Data   classif  583\n#>  4:            iris                              Iris Flowers   classif  150\n#>  5:       optdigits Optical Recognition of Handwritten Digits   classif 5620\n#>  6:        penguins                           Palmer Penguins   classif  344\n#>  7: penguins_simple                Simplified Palmer Penguins   classif  333\n#>  8:            pima                      Pima Indian Diabetes   classif  768\n#>  9:           sonar                    Sonar: Mines vs. Rocks   classif  208\n#> 10:            spam                         HP Spam Detection   classif 4601\n#> 11:         titanic                                   Titanic   classif 1309\n#> 12:            wine                              Wine Regions   classif  178\n#> 13:             zoo                               Zoo Animals   classif  101\n#>     ncol properties lgl int dbl chr fct ord pxc\n#>  1:   10   twoclass   0   0   0   0   0   9   0\n#>  2:   21   twoclass   0   3   0   0  14   3   0\n#>  3:   11   twoclass   0   4   5   0   1   0   0\n#>  4:    5 multiclass   0   0   4   0   0   0   0\n#>  5:   65   twoclass   0  64   0   0   0   0   0\n#>  6:    8 multiclass   0   3   2   0   2   0   0\n#>  7:   11 multiclass   0   3   7   0   0   0   0\n#>  8:    9   twoclass   0   0   8   0   0   0   0\n#>  9:   61   twoclass   0   0  60   0   0   0   0\n#> 10:   58   twoclass   0   0  57   0   0   0   0\n#> 11:   11   twoclass   0   2   2   3   2   1   0\n#> 12:   14 multiclass   0   2  11   0   0   0   0\n#> 13:   17 multiclass  15   1   0   0   0   0   0\n```\n:::\n\n\nThe `sonar` task is an example of a binary classification problem, as the target can only take two different values, in `mlr3` terminology it has the “twoclass” property:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_sonar = tsk(\"sonar\")\ntsk_sonar\n#> <TaskClassif:sonar> (208 x 61): Sonar: Mines vs. Rocks\n#> * Target: Class\n#> * Properties: twoclass\n#> * Features (60):\n#>   - dbl (60): V1, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V2,\n#>     V20, V21, V22, V23, V24, V25, V26, V27, V28, V29, V3, V30, V31,\n#>     V32, V33, V34, V35, V36, V37, V38, V39, V4, V40, V41, V42, V43,\n#>     V44, V45, V46, V47, V48, V49, V5, V50, V51, V52, V53, V54, V55,\n#>     V56, V57, V58, V59, V6, V60, V7, V8, V9\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_sonar$class_names\n#> [1] \"M\" \"R\"\n```\n:::\n\n\nIn contrast, `tsk(\"penguins\")` is a multiclass problem as there are more than two species of penguins; it has the “multiclass” property:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_penguins = tsk(\"penguins\")\ntsk_penguins$properties\n#> [1] \"multiclass\"\ntsk_penguins$class_names\n#> [1] \"Adelie\"    \"Chinstrap\" \"Gentoo\"\n```\n:::\n\n\nA further difference between these tasks is that binary classification tasks have an extra field called `$positive`, which defines the ‘positive’ class. In binary classification, as there are only two possible class types, by convention one of these is known as the ‘positive’ class, and the other as the ‘negative’ class. It is arbitrary which is which, though often the more ‘important’ (and often smaller) class is set as the positive class. You can set the positive class during or after construction. If no positive class is specified then `mlr3` assumes the first level in the `target` column is the positive class, which can lead to misleading results.\n\n> 这两种任务之间的另一个区别是，二分类任务有一个额外的字段称为 `$positive`，它定义了“正类”（positive class）。在二分类问题中，由于只有两种可能的类别类型，按照惯例，其中一种被称为“正类”，另一种被称为“负类”。哪个是哪个是任意的，尽管通常更“重要”（通常更小）的类别被设置为正类。您可以在构建期间或之后设置正类。如果未指定正类，则 `mlr3` 假定目标列中的第一个级别是正类，这可能导致误导性的结果。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nSonar = tsk_sonar$data()\ntsk_classif = as_task_classif(Sonar, target = \"Class\", positive = \"R\")\ntsk_classif$positive\n#> [1] \"R\"\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# changing after construction\ntsk_classif$positive = \"M\"\ntsk_classif$positive\n#> [1] \"M\"\n```\n:::\n\n\n### LearnerClassif and MeasureClassif\n\nClassification learners, which inherit from `LearnerClassif`, have nearly the same interface as regression learners. However, a key difference is that the possible predictions in classification are either `\"response\"` – predicting an observation’s class (a penguin’s species in our example, this is sometimes called “hard labeling”) – or `\"prob\"` – predicting a vector of probabilities, also called “posterior probabilities”, of an observation belonging to each class. In classification, the latter can be more useful as it provides information about the confidence of the predictions:\n\n> 分类学习器（继承自 `LearnerClassif`）几乎具有与回归学习器相同的接口。然而，分类中的一个关键区别是，分类问题中可能的预测结果要么是 `\"response\"` （预测观测的类别，例如我们示例中的企鹅物种，有时称为“硬标签”），要么是 `\"prob\"` （预测属于每个类别的概率向量，也称为“后验概率”）。在分类中，后者可能更有用，因为它提供了有关预测的置信度信息：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_rpart = lrn(\"classif.rpart\", predict_type = \"prob\")\nlrn_rpart$train(tsk_penguins, splits$train)\nprediction = lrn_rpart$predict(tsk_penguins, splits$test)\nprediction\n#> <PredictionClassif> for 113 observations:\n#>     row_ids     truth  response prob.Adelie prob.Chinstrap prob.Gentoo\n#>           2    Adelie    Adelie  0.97029703     0.02970297  0.00000000\n#>           4    Adelie    Adelie  0.97029703     0.02970297  0.00000000\n#>           7    Adelie    Adelie  0.97029703     0.02970297  0.00000000\n#> ---                                                                   \n#>         338 Chinstrap Chinstrap  0.04651163     0.93023256  0.02325581\n#>         341 Chinstrap    Adelie  0.97029703     0.02970297  0.00000000\n#>         344 Chinstrap Chinstrap  0.04651163     0.93023256  0.02325581\n```\n:::\n\n\nAlso, the interface for classification measures, which are of class `MeasureClassif`, is identical to regression measures. The key difference in usage is that you will need to ensure your selected measure evaluates the prediction type of interest. To evaluate \"response\" predictions, you will need measures with `predict_type = \"response\"`, or to evaluate probability predictions you will need `predict_type = \"prob\"`. The easiest way to find these measures is by filtering the `mlr_measures` dictionary:\n\n> 此外，分类度量标准的接口，其类别为 `MeasureClassif`，与回归度量标准完全相同。在使用上的主要区别在于，您需要确保所选的度量标准评估感兴趣的预测类型。要评估 `“response”` 预测，您需要使用 `predict_type = \"response\"` 的度量标准，或者要评估概率预测，您需要使用 `predict_type = \"prob\"` 的度量标准。查找这些度量标准的最简单方法是通过筛选 `mlr_measures` 字典：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(msr())[\n  task_type == \"classif\" & predict_type == \"prob\" &\n  !sapply(task_properties, \\(x) \"twoclass\" %in% x)\n]\n#>                  key                                      label task_type\n#> 1:   classif.logloss                                   Log Loss   classif\n#> 2: classif.mauc_au1p    Weighted average 1 vs. 1 multiclass AUC   classif\n#> 3: classif.mauc_au1u             Average 1 vs. 1 multiclass AUC   classif\n#> 4: classif.mauc_aunp Weighted average 1 vs. rest multiclass AUC   classif\n#> 5: classif.mauc_aunu          Average 1 vs. rest multiclass AUC   classif\n#> 6:    classif.mbrier                     Multiclass Brier Score   classif\n#>             packages predict_type task_properties\n#> 1: mlr3,mlr3measures         prob                \n#> 2: mlr3,mlr3measures         prob                \n#> 3: mlr3,mlr3measures         prob                \n#> 4: mlr3,mlr3measures         prob                \n#> 5: mlr3,mlr3measures         prob                \n#> 6: mlr3,mlr3measures         prob\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmeasures = msrs(c(\"classif.mbrier\", \"classif.logloss\", \"classif.acc\"))\nprediction$score(measures)\n#>  classif.mbrier classif.logloss     classif.acc \n#>       0.1016821       0.2291407       0.9469027\n```\n:::\n\n\n### PredictionClassif, Confusion Matrix, and Thresholding\n\n`PredictionClassif` objects have two important differences from their regression analog. Firstly, the added field `$confusion`, and secondly the added method `$set_threshold()`.\n\n> `PredictionClassif` 对象与其回归模型的预测对象有两个重要的区别。首先是新增的字段 `$confusion`，其次是新增的方法 `$set_threshold()`。\n\n#### Confusion Matrix\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction$confusion\n#>            truth\n#> response    Adelie Chinstrap Gentoo\n#>   Adelie        49         3      0\n#>   Chinstrap      1        18      1\n#>   Gentoo         0         1     40\n```\n:::\n\n\nThe rows in a confusion matrix are the predicted class and the columns are the true class. All off-diagonal entries are incorrectly classified observations, and all diagonal entries are correctly classified. In this case, the classifier does fairly well classifying all penguins, but we could have found that it only classifies the Adelie species well but often conflates Chinstrap and Gentoo, for example.\n\n> 混淆矩阵中的行表示预测的类别，列表示真实的类别。所有非对角线条目都是被错误分类的观测值，而所有对角线条目都是被正确分类的。在这种情况下，分类器在对所有企鹅进行分类时表现得相当不错，但我们也可能发现它只能很好地对 Adelie 物种进行分类，但经常将 Chinstrap 和 Gentoo 混为一谈。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(prediction)\n```\n\n::: {.cell-output-display}\n![Counts of each class label in the ground truth data (left) and predictions (right).](index_files/figure-html/fig-confusion_matrix-1.png){#fig-confusion_matrix fig-align='center' width=70%}\n:::\n:::\n\n\nIn the binary classification case, the top left entry corresponds to true positives, the top right to false positives, the bottom left to false negatives and the bottom right to true negatives. Taking `tsk_sonar` as an example with `M` as the positive class:\n\n> 在二分类情况下，左上角的条目对应于真正例（true positives），右上角对应于假正例（false positives），左下角对应于假负例（false negatives），右下角对应于真负例（true negatives）。以 `tsk_sonar` 为例，`M` 为正类：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsplits = partition(tsk_sonar)\nlrn_rpart$\n  train(tsk_sonar, splits$train)$\n  predict(tsk_sonar, splits$test)$\n  confusion\n#>         truth\n#> response  M  R\n#>        M 27 10\n#>        R 10 22\n```\n:::\n\n\n#### Thresholding\n\n**阈值化**\n\nThis 50% value is known as the threshold and it can be useful to change this threshold if there is class imbalance (when one class is over- or under-represented in a dataset), or if there are different costs associated with classes, or simply if there is a preference to ‘over’-predict one class. As an example, let us take `tsk(\"german_credit\")` in which 700 customers have good credit and 300 have bad. Now we could easily build a model with around “70%” accuracy simply by always predicting a customer will have good credit:\n\n> 这个 50% 的值被称为阈值，如果数据集中存在类别不平衡（即一个类别在数据集中过多或过少出现），或者不同的类别具有不同的成本，或者只是有一种“过度”预测一种类别的倾向，那么更改这个阈值可能会很有用。举个例子，让我们看看 `tsk(\"german_credit\")`，其中有 700 个客户信用良好，300 个客户信用不良。现在，我们可以很容易地构建一个模型，总是预测客户会有良好的信用，从而获得 “70%” 左右的准确性：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask_credit = tsk(\"german_credit\")\nlrn_featureless = lrn(\"classif.featureless\", predict_type = \"prob\")\nsplits = partition(task_credit)\nlrn_featureless$train(task_credit, splits$train)\nprediction = lrn_featureless$predict(task_credit, splits$test)\nprediction$score(msr(\"classif.acc\"))\n#> classif.acc \n#>         0.7\n```\n:::\n\n\n::: {.callout-caution}\nTODO：等待后续添加交叉引用  13.1\n:::\n\nWhile this model may appear to have good performance on the surface, in fact, it just ignores all ‘bad’ customers – this can create big problems in this finance example, as well as in healthcare tasks and other settings where false positives cost more than false negatives (see Section 13.1 for cost-sensitive classification).\n\nThresholding allows classes to be selected with a different probability threshold, so instead of predicting that a customer has bad credit if P(good) < 50%, we might predict bad credit if P(good) < 70% – notice how we write this in terms of the positive class, which in this task is ‘good’. Let us see this in practice:\n\n> 虽然这个模型表面上看起来性能不错，但实际上它只是忽略了所有“不良”的客户 - 这在金融示例以及在医疗任务和其他一些情况下可能会带来很大问题，特别是在假阳性的成本高于假阴性的情况下（请参见第13.1节的成本敏感分类）。\n>\n> 阈值化允许使用不同的概率阈值选择类别，因此，与其在P(好) < 50%时预测客户信用不良，我们可以在P(好) < 70%时预测客户信用不良。请注意，我们是根据正类别来表示这一点，而在这个任务中正类别是“好”。让我们看看实际应用中的情况：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction$set_threshold(0.7)\nprediction$score(msr(\"classif.acc\"))\n#> classif.acc \n#>   0.5393939\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_rpart = lrn(\"classif.rpart\", predict_type = \"prob\")\nlrn_rpart$train(task_credit, splits$train)\nprediction = lrn_rpart$predict(task_credit, splits$test)\nprediction$score(msr(\"classif.acc\"))\n#> classif.acc \n#>   0.6939394\nprediction$confusion\n#>         truth\n#> response good bad\n#>     good  194  64\n#>     bad    37  35\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction$set_threshold(0.7)\nprediction$score(msr(\"classif.acc\"))\n#> classif.acc \n#>   0.6878788\nprediction$confusion\n#>         truth\n#> response good bad\n#>     good  181  53\n#>     bad    50  46\n```\n:::\n\n\n# Evaluation and Benchmarking {#sec-performance}\n\n**Resampling Does Not Avoid Model Overfitting**: \nA common **misunderstanding** is that holdout and other more advanced resampling strategies can prevent model overfitting. In fact, these methods just make overfitting visible as we can separately evaluate train/test performance. Resampling strategies also allow us to make (nearly) unbiased estimations of the generalization error.\n\n> **重采样不能避免模型过拟合**：一个常见的误解是，留出策略和其他更高级的重采样策略可以防止模型过拟合。实际上，这些方法只是使过拟合问题更加显而易见，因为我们可以单独评估训练/测试性能。重采样策略还允许我们对泛化误差进行（几乎）无偏估计。\n\n## Holdout and Scoring\n\nIn practice, one would usually create an intermediate model, which is trained on a subset of the available data and then tested on the remainder of the data. The performance of this intermediate model, obtained by comparing the model predictions to the ground truth, is an estimate of the generalization performance of the final model, which is the model fitted on all data.\n\n> 在实践中，通常会创建一个中间模型，该模型在可用数据的子集上进行训练，然后在剩余的数据上进行测试。通过将模型的预测与真实情况进行比较，中间模型的性能可以作为最终模型的泛化性能的估计。最终模型是在所有可用数据上训练的模型。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_penguins = tsk(\"penguins\")\nsplits = partition(tsk_penguins)\nlrn_rpart = lrn(\"classif.rpart\")\nlrn_rpart$train(tsk_penguins, splits$train)\nprediction = lrn_rpart$predict(tsk_penguins, splits$test)\nprediction$score(msr(\"classif.acc\"))\n#> classif.acc \n#>   0.9380531\n```\n:::\n\n\n## Resampling\n\n### Constructing a Resampling Strategy\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(rsmp())\n#>            key                         label        params iters\n#> 1:   bootstrap                     Bootstrap ratio,repeats    30\n#> 2:      custom                 Custom Splits                  NA\n#> 3:   custom_cv Custom Split Cross-Validation                  NA\n#> 4:          cv              Cross-Validation         folds    10\n#> 5:     holdout                       Holdout         ratio     1\n#> 6:    insample           Insample Resampling                   1\n#> 7:         loo                 Leave-One-Out                  NA\n#> 8: repeated_cv     Repeated Cross-Validation folds,repeats   100\n#> 9: subsampling                   Subsampling ratio,repeats    30\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrsmp(\"holdout\", ratio = .8)\n#> <ResamplingHoldout>: Holdout\n#> * Iterations: 1\n#> * Instantiated: FALSE\n#> * Parameters: ratio=0.8\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# three-fold CV\ncv3 = rsmp(\"cv\", folds = 3)\n# subsampling with 3 repeats and 9/10 ratio\nss390 = rsmp(\"subsampling\", repeats = 3, ratio = .9)\n# 2-repeats 5-fold cv\nrcv25 = rsmp(\"repeated_cv\", repeats = 2, folds = 5)\n```\n:::\n\n\nWhen a `\"Resampling\"` object is constructed, it is simply a definition for how the data splitting process will be performed on the task when running the resampling strategy. However, it is possible to manually instantiate a resampling strategy, i.e., generate all train-test splits, by calling the `$instantiate()` method on a given task.\n\n> 当构建一个 `\"Resampling\"` 对象时，它只是对在运行重采样策略时如何执行数据拆分过程的定义。然而，可以通过在给定任务上调用 `$instantiate()` 方法来手动实例化一个重采样策略，即生成所有的训练-测试拆分。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv3$instantiate(tsk_penguins)\n# first 5 observations in first traininng set\ncv3$train_set(1)[1:5]\n#> [1] 2 4 5 6 8\n# fitst 5 observations in thirt test set\ncv3$test_set(3)[1:5]\n#> [1]  1  9 12 17 20\n```\n:::\n\n\nWhen the aim is to fairly compare multiple learners, best practice dictates that all learners being compared use the same training data to build a model and that they use the same test data to evaluate the model performance. Resampling strategies are instantiated automatically for you when using the `resample()` method. Therefore, manually instantiating resampling strategies is rarely required but might be useful for debugging or digging deeper into a model’s performance.\n\n> 当目标是公平比较多个学习器时，最佳实践要求所有进行比较的学习器都使用相同的训练数据来构建模型，并且它们使用相同的测试数据来评估模型性能。在使用 `resample()` 方法时，重采样策略会自动为您实例化。因此，手动实例化重采样策略很少是必需的，但在调试或深入研究模型性能时可能会有用。\n\n### Resampling Experiments\n\nThe `resample()` function takes a given `Task`, `Learner`, and `Resampling` object to run the given resampling strategy. `resample()` repeatedly fits a model on training sets, makes predictions on the corresponding test sets and stores them in a `ResampleResult` object, which contains all the information needed to estimate the generalization performance.\n\n`resample()` 函数接受给定的任务（`Task`）、学习器（`Learner`）和重采样（`Resampling`）对象，以运行给定的重采样策略。`resample()` 函数会在训练集上反复拟合模型，在相应的测试集上进行预测，并将预测结果存储在 `ResampleResult` 对象中，该对象包含了估算泛化性能所需的所有信息。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr = resample(tsk_penguins, lrn_rpart, cv3)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr\n#> <ResampleResult> with 3 resampling iterations\n#>   task_id    learner_id resampling_id iteration warnings errors\n#>  penguins classif.rpart            cv         1        0      0\n#>  penguins classif.rpart            cv         2        0      0\n#>  penguins classif.rpart            cv         3        0      0\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# calculate the score for each iteration\nacc = rr$score(msr(\"classif.ce\"))\nacc[, .(iteration, classif.ce)]\n#>    iteration classif.ce\n#> 1:         1 0.04347826\n#> 2:         2 0.09565217\n#> 3:         3 0.06140351\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# aggregated score across all resampling iterations\nrr$aggregate(msr(\"classif.ce\"))\n#> classif.ce \n#> 0.06684465\n```\n:::\n\n\nBy default, the majority of measures will aggregate scores using a macro average, which first calculates the measure in each resampling iteration separately, and then averages these scores across all iterations. However, it is also possible to aggregate scores using a micro average, which pools predictions across resampling iterations into one `Prediction` object and then computes the measure on this directly:\n\n> 默认情况下，大多数性能度量会使用宏平均（macro average）来汇总分数，它首先在每个重采样迭代中分别计算度量，然后在所有迭代中对这些分数进行平均。但也可以使用微平均（micro average）来汇总分数，它将重采样迭代中的预测汇总到一个 `Prediction` 对象中，然后直接在该对象上计算度量：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr$aggregate(msr(\"classif.ce\", average = \"micro\"))\n#> classif.ce \n#> 0.06686047\n```\n:::\n\n\nTo visualize the resampling results, you can use the `autoplot.ResampleResult()` function to plot scores across folds as boxplots or histograms (@fig-resamp-viz). Histograms can be useful to visually gauge the variance of the performance results across resampling iterations, whereas boxplots are often used when multiple learners are compared side-by-side (see @sec-benchmarking).\n\n> 要可视化重采样结果，您可以使用 `autoplot.ResampleResult()` 函数绘制跨折叠的分数箱线图或直方图（@fig-resamp-viz）。直方图可以用于直观评估跨重采样迭代的性能结果方差，而箱线图通常用于比较多个学习器并排放置在一起时（请参阅 @sec-benchmarking）。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr = resample(tsk_penguins, lrn_rpart, rsmp(\"cv\", folds = 10))\n```\n:::\n\n::: {#fig-resamp-viz .cell layout-ncol=\"2\" layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(rr, measure = msr(\"classif.acc\"), type = \"boxplot\")\nautoplot(rr, measure = msr(\"classif.acc\"), type = \"histogram\")\n```\n\n::: {.cell-output-display}\n![Boxplot of accuracy scores.](index_files/figure-html/fig-resamp-viz-1.png){#fig-resamp-viz-1 fig-align='center' fig-alt='Left: a boxplot ranging from 0.875 to 1.0 and the interquartile range between 0.925 and 0.7. Right: a histogram with five bars in a roughly normal distribution with mean 0.95, minimum 0.875 and maximum 1.0.' width=70%}\n:::\n\n::: {.cell-output-display}\n![Histogram of accuracy scores.](index_files/figure-html/fig-resamp-viz-2.png){#fig-resamp-viz-2 fig-align='center' fig-alt='Left: a boxplot ranging from 0.875 to 1.0 and the interquartile range between 0.925 and 0.7. Right: a histogram with five bars in a roughly normal distribution with mean 0.95, minimum 0.875 and maximum 1.0.' width=70%}\n:::\n\nBoxplot and Histogram of accuracy scores.\n:::\n\n\n### ResampleResult Objects\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# list of prediction objects\nrrp = rr$predictions()\n# print first two\nrrp[1:2]\n#> [[1]]\n#> <PredictionClassif> for 35 observations:\n#>     row_ids     truth  response\n#>           7    Adelie    Adelie\n#>          20    Adelie Chinstrap\n#>          32    Adelie    Adelie\n#> ---                            \n#>         326 Chinstrap Chinstrap\n#>         330 Chinstrap Chinstrap\n#>         337 Chinstrap Chinstrap\n#> \n#> [[2]]\n#> <PredictionClassif> for 35 observations:\n#>     row_ids     truth  response\n#>           1    Adelie    Adelie\n#>           5    Adelie    Adelie\n#>           9    Adelie    Adelie\n#> ---                            \n#>         334 Chinstrap Chinstrap\n#>         339 Chinstrap Chinstrap\n#>         340 Chinstrap Chinstrap\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# macro averaged performance\nmean(sapply(rrp, \\(x) x$score()))\n#> [1] 0.05823529\n```\n:::\n\n\nBy default, the intermediate models produced at each resampling iteration are discarded after the prediction step to reduce memory consumption of the `ResampleResult` object (only the predictions are required to calculate most performance measures). However, it can sometimes be useful to inspect, compare, or extract information from these intermediate models. We can configure the `resample()` function to keep the fitted intermediate models by setting `store_models = TRUE`. Each model trained in a specific resampling iteration can then be accessed via `$learners[[i]]$model`, where `i` refers to the `i`-th resampling iteration:\n\n> 默认情况下，在进行预测步骤后，每个重新采样迭代产生的中间模型都会被丢弃，以降低 `ResampleResult` 对象的内存消耗（大多数性能指标仅需要预测）。然而，有时候检查、比较或从这些中间模型中提取信息可能是有用的。我们可以通过设置 `store_models = TRUE` 来配置 `resample()` 函数以保留拟合的中间模型。然后，可以通过 `$learners[[i]]$model` 来访问在特定重新采样迭代中训练的每个模型，其中 `i` 指的是第 `i` 个重新采样迭代：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr = resample(tsk_penguins, lrn_rpart, cv3, store_models = TRUE)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# get the model from the first iteration\nrr$learners[[1]]$model\n#> n= 229 \n#> \n#> node), split, n, loss, yval, (yprob)\n#>       * denotes terminal node\n#> \n#> 1) root 229 130 Adelie (0.432314410 0.205240175 0.362445415)  \n#>   2) flipper_length< 206.5 142  45 Adelie (0.683098592 0.309859155 0.007042254)  \n#>     4) bill_length< 44.65 97   3 Adelie (0.969072165 0.030927835 0.000000000) *\n#>     5) bill_length>=44.65 45   4 Chinstrap (0.066666667 0.911111111 0.022222222) *\n#>   3) flipper_length>=206.5 87   5 Gentoo (0.022988506 0.034482759 0.942528736) *\n```\n:::\n\n\nIn this example, we could then inspect the most important variables in each iteration to help us learn more about the respective fitted models:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# print 2nd and 3rd iteration\nlapply(rr$learners[2:3], \\(x) x$model$variable.importance)\n#> [[1]]\n#> flipper_length    bill_length     bill_depth      body_mass         island \n#>       88.52870       88.07438       71.51814       67.04826       55.13690 \n#> \n#> [[2]]\n#>    bill_length flipper_length     bill_depth      body_mass         island \n#>       82.18794       75.92820       66.94285       57.14539       50.29049\n```\n:::\n\n\n## Benchmarking {#sec-benchmarking}\n\n### benchmark()\n\nBenchmark experiments in `mlr3` are conducted with `benchmark()`, which simply runs `resample()` on each task and learner separately, then collects the results. The provided resampling strategy is automatically instantiated on each task to ensure that all learners are compared against the same training and test data.\n\nTo use the `benchmark()` function we first call `benchmark_grid()`, which constructs an exhaustive *design* to describe all combinations of the learners, tasks and resamplings to be used in a benchmark experiment, and instantiates the resampling strategies.\n\n> `mlr3` 中的基准实验是使用 `benchmark()` 函数进行的，该函数简单地在每个任务和学习器上分别运行 `resample()`，然后收集结果。提供的重新采样策略会自动在每个任务上进行实例化，以确保所有学习器都与相同的训练和测试数据进行比较。\n>\n> 要使用 `benchmark()` 函数，我们首先调用 `benchmark_grid()` 函数，该函数构建一个详尽的设计来描述在基准实验中要使用的所有学习器、任务和重新采样的组合，并实例化重新采样策略。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntasks = tsks(c(\"german_credit\", \"sonar\"))\nlearners = lrns(c(\"classif.rpart\", \"classif.ranger\", \"classif.featureless\"),\n                predict_type = \"prob\")\nrsmp_cv5 = rsmp(\"cv\", folds = 5)\n\ndesign = benchmark_grid(tasks, learners, rsmp_cv5)\ndesign\n#>             task             learner resampling\n#> 1: german_credit       classif.rpart         cv\n#> 2: german_credit      classif.ranger         cv\n#> 3: german_credit classif.featureless         cv\n#> 4:         sonar       classif.rpart         cv\n#> 5:         sonar      classif.ranger         cv\n#> 6:         sonar classif.featureless         cv\n```\n:::\n\n\nBy default, `benchmark_grid()` instantiates the resamplings on the tasks, which means that concrete train-test splits are generated. Since this process is stochastic, it is necessary to set a seed **before** calling `benchmark_grid()` to ensure reproducibility of the data splits.\n\n> 在默认情况下，`benchmark_grid()` 会在任务上实例化重新采样，这意味着会生成具体的训练-测试拆分。由于这个过程是随机的，所以在调用 `benchmark_grid()` 之前需要设置一个种子，以确保数据拆分的可重现性。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# pass design to benchmark()\nbmr = benchmark(design)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmr\n#> <BenchmarkResult> of 30 rows with 6 resampling runs\n#>  nr       task_id          learner_id resampling_id iters warnings errors\n#>   1 german_credit       classif.rpart            cv     5        0      0\n#>   2 german_credit      classif.ranger            cv     5        0      0\n#>   3 german_credit classif.featureless            cv     5        0      0\n#>   4         sonar       classif.rpart            cv     5        0      0\n#>   5         sonar      classif.ranger            cv     5        0      0\n#>   6         sonar classif.featureless            cv     5        0      0\n```\n:::\n\n\nAs `benchmark()` is just an extension of `resample()`, we can once again use `$score()`, or `$aggregate()` depending on your use-case, though note that in this case `$score()` will return results over each fold of each learner/task/resampling combination.\n\n> 由于 `benchmark()` 只是 `resample()` 的扩展，因此我们可以再次使用 `$score()` 或 `$aggregate()`，具体取决于您的用例，但请注意，在这种情况下，`$score()` 将返回每个学习器/任务/重新采样组合的每个折叠的结果。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmr$score()[c(1, 7, 13), .(iteration, task_id, learner_id, classif.ce)]\n#>    iteration       task_id          learner_id classif.ce\n#> 1:         1 german_credit       classif.rpart      0.335\n#> 2:         2 german_credit      classif.ranger      0.240\n#> 3:         3 german_credit classif.featureless      0.300\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmr$aggregate()[, .(task_id, learner_id, classif.ce)]\n#>          task_id          learner_id classif.ce\n#> 1: german_credit       classif.rpart  0.2870000\n#> 2: german_credit      classif.ranger  0.2230000\n#> 3: german_credit classif.featureless  0.3000000\n#> 4:         sonar       classif.rpart  0.3026713\n#> 5:         sonar      classif.ranger  0.1921022\n#> 6:         sonar classif.featureless  0.4659698\n```\n:::\n\n\n::: {.callout-caution}\nTODO：等待后续添加交叉引用  11.3\n:::\n\nThis would conclude a basic benchmark experiment where you can draw tentative conclusions about model performance, in this case we would possibly conclude that the random forest is the best of all three models on each task. We draw conclusions cautiously here as we have not run any statistical tests or included standard errors of measures, so we cannot definitively say if one model outperforms the other.\n\nAs the results of `$score()` and `$aggregate()` are returned in a `data.table`, you can post-process and analyze the results in any way you want. A common mistake is to average the learner performance across all tasks when the tasks vary significantly. This is a mistake as averaging the performance will miss out important insights into how learners compare on ‘easier’ or more ‘difficult’ predictive problems. A more robust alternative to compare the overall algorithm performance across multiple tasks is to compute the ranks of each learner on each task separately and then calculate the average ranks. This can provide a better comparison as task-specific ‘quirks’ are taken into account by comparing learners within tasks before comparing them across tasks. However, using ranks will lose information about the numerical differences between the calculated performance scores. Analysis of benchmark experiments, including statistical tests, is covered in more detail in Section 11.3.\n\n> 这将总结了一个基本的基准实验，您可以初步得出关于模型性能的结论，在这种情况下，我们可能会得出结论，随机森林在每个任务上都是三个模型中最好的。我们在这里谨慎地得出结论，因为我们没有进行任何统计测试，也没有包括性能度量的标准错误，因此我们不能明确地说一个模型是否优于另一个。\n>\n> 由于 `$score()` 和 `$aggregate()` 的结果以 `data.table` 返回，您可以以任何您想要的方式进行后处理和分析结果。一个常见的错误是在任务差异明显的情况下，对所有任务的学习器性能进行平均。这是一个错误，因为对性能进行平均将错过对学习器在“更容易”或“更困难”的预测问题上的比较重要的洞察。比较多个任务上的整体算法性能的更强大的替代方法是分别计算每个任务上每个学习器的排名，然后计算平均排名。这可以提供更好的比较，因为通过在比较任务之前在任务内部比较学习器，可以考虑到特定于任务的“怪癖”。然而，使用排名会丢失关于计算的性能分数之间的数值差异的信息。关于基准实验的分析，包括统计测试，在第11.3节中将更详细地介绍。\n\n### BenchmarkResult Objects\n\nA `BenchmarkResult` object is a collection of multiple `ResampleResult` objects.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmrdt = as.data.table(bmr)\nbmrdt[1:2, .(task, learner, resampling, iteration)]\n#>                 task                   learner         resampling iteration\n#> 1: <TaskClassif[51]> <LearnerClassifRpart[38]> <ResamplingCV[20]>         1\n#> 2: <TaskClassif[51]> <LearnerClassifRpart[38]> <ResamplingCV[20]>         2\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr1 = bmr$resample_result(1)\nrr2 = bmr$resample_result(2)\nrr1\n#> <ResampleResult> with 5 resampling iterations\n#>        task_id    learner_id resampling_id iteration warnings errors\n#>  german_credit classif.rpart            cv         1        0      0\n#>  german_credit classif.rpart            cv         2        0      0\n#>  german_credit classif.rpart            cv         3        0      0\n#>  german_credit classif.rpart            cv         4        0      0\n#>  german_credit classif.rpart            cv         5        0      0\n```\n:::\n\n\nIn addition, `as_benchmark_result()` can be used to convert objects from `ResampleResult` to `BenchmarkResult.` The `c()`-method can be used to combine multiple `BenchmarkResult` objects, which can be useful when conducting experiments across multiple machines:\n\n> 此外，可以使用 `as_benchmark_result()` 将 `ResampleResult` 对象转换为 `BenchmarkResult`。`c()` 方法可用于组合多个 `BenchmarkResult` 对象，这在跨多台计算机进行实验时非常有用：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmr1 = as_benchmark_result(rr1)\nbmr2 = as_benchmark_result(rr2)\n\nc(bmr1, bmr2)\n#> <BenchmarkResult> of 10 rows with 2 resampling runs\n#>  nr       task_id     learner_id resampling_id iters warnings errors\n#>   1 german_credit  classif.rpart            cv     5        0      0\n#>   2 german_credit classif.ranger            cv     5        0      0\n```\n:::\n\n\nBoxplots are most commonly used to visualize benchmark experiments as they can intuitively summarize results across tasks and learners simultaneously.\n\n> 箱线图最常用于可视化基准实验，因为它们可以直观地同时总结任务和学习器之间的结果。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(bmr, measure = msr(\"classif.acc\"))\n```\n\n::: {.cell-output-display}\n![Boxplots of accuracy scores for each learner across resampling iterations and the three tasks. Random forests (`lrn(\"classif.ranger\")`) consistently outperforms the other learners.](index_files/figure-html/fig-benchmark-box-1.png){#fig-benchmark-box fig-align='center' width=70%}\n:::\n:::\n\n\n## Evaluation of Binary Classifiers\n\n### Confusion Matrix\n\nIt is possible for a classifier to have a good classification accuracy but to overlook the nuances provided by a full confusion matrix, as in the following `tsk(\"german_credit\")` example:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_german = tsk(\"german_credit\")\nlrn_ranger = lrn(\"classif.ranger\", predict_type = \"prob\")\nsplits = partition(tsk_german, ratio = .8)\n\nlrn_ranger$train(tsk_german, splits$train)\nprediction = lrn_ranger$predict(tsk_german, splits$test)\nprediction$score(msr(\"classif.acc\"))\n#> classif.acc \n#>        0.74\nprediction$confusion\n#>         truth\n#> response good bad\n#>     good  124  36\n#>     bad    16  24\n```\n:::\n\n\nOn their own, the absolute numbers in a confusion matrix can be less useful when there is class imbalance. Instead, several normalized measures can be derived (@fig-confusion):\n\n- **True Positive Rate (TPR)**, **Sensitivity** or **Recall**: How many of the true positives did we predict as positive?\n\n- **True Negative Rate (TNR)** or **Specificity**: How many of the true negatives did we predict as negative?\n\n- **False Positive Rate (FPR)**, or $1 -$ **Specificity**: How many of the true negatives did we predict as positive?\n\n- **Positive Predictive Value (PPV)** or **Precision**: If we predict positive how likely is it a true positive?\n\n- **Negative Predictive Value (NPV)**: If we predict negative how likely is it a true negative?\n\n- **Accuracy (ACC)**: The proportion of correctly classified instances out of the total number of instances.\n\n- **F1-score**: The harmonic mean of precision and recall, which balances the trade-off between precision and recall. It is calculated as $2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Binary confusion matrix of ground truth class vs. predicted class.](imgs/confusion_matrix.svg){#fig-confusion fig-align='center' width=70%}\n:::\n:::\n\n\nThe `mlr3measures` package allows you to compute several common confusion matrix-based measures using the `confusion_matrix()` function:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlr3measures::confusion_matrix(\n  truth = prediction$truth,\n  response = prediction$response,\n  positive = tsk_german$positive\n)\n#>         truth\n#> response good bad\n#>     good  124  36\n#>     bad    16  24\n#> acc :  0.7400; ce  :  0.2600; dor :  5.1667; f1  :  0.8267 \n#> fdr :  0.2250; fnr :  0.1143; fomr:  0.4000; fpr :  0.6000 \n#> mcc :  0.3273; npv :  0.6000; ppv :  0.7750; tnr :  0.4000 \n#> tpr :  0.8857\n```\n:::\n\n\n### ROC Analysis\n\nThe ROC curve is a line graph with TPR on the y-axis and the FPR on the x-axis. \n\nConsider classifiers that predict probabilities instead of discrete classes. Using different thresholds to cut off predicted probabilities and assign them to the positive and negative class will lead to different TPRs and FPRs and by plotting these values across different thresholds we can characterize the behavior of a binary classifier – this is the ROC curve.\n\n> 考虑预测概率而不是离散类别的分类器。使用不同的阈值来截断预测的概率并将其分配到正类别和负类别将导致不同的 TPR 和 FPR，并通过在不同的阈值上绘制这些值，我们可以表征二元分类器的行为 - 这就是 ROC 曲线。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(prediction, type = \"roc\")\n```\n\n::: {.cell-output-display}\n![ROC-curve based on the `german_credit` dataset and the `classif.ranger` random forest learner. Recall FPR = $1 -$ Specificity and TPR = Sensitivity.](index_files/figure-html/fig-basics-roc-ranger-1.png){#fig-basics-roc-ranger fig-align='center' width=70%}\n:::\n:::\n\n\nA natural performance measure that can be derived from the ROC curve is the area under the curve (AUC), implemented in `msr(\"classif.auc\")`. The AUC can be interpreted as the probability that a randomly chosen positive instance has a higher predicted probability of belonging to the positive class than a randomly chosen negative instance. Therefore, higher values (closer to \n) indicate better performance. Random classifiers (such as the featureless baseline) will always have an AUC of (approximately, when evaluated empirically) 0.5.\n\n> 从 ROC 曲线中可以导出的一个自然性能度量是曲线下面积（AUC），在 `msr(\"classif.auc\")` 中实现。AUC 可以解释为随机选择的正实例具有较高的预测概率，属于正类别，而不是随机选择的负实例的概率。因此，较高的值（越接近 1）表示更好的性能。随机分类器（例如没有特征的基线）的AUC总是为（在经验上评估时约为 0.5）。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction$score(msr(\"classif.auc\"))\n#> classif.auc \n#>   0.7407143\n```\n:::\n\n\nWe can also plot the precision-recall curve (PRC) which visualizes the PPV/precision vs. TPR/recall. The main difference between ROC curves and PR curves is that the number of true-negatives are ignored in the latter. This can be useful in imbalanced populations where the positive class is rare, and where a classifier with high TPR may still not be very informative and have low PPV. See Davis and Goadrich (2006) for a detailed discussion about the relationship between the PRC and ROC curves.\n\n> 我们还可以绘制精确度-召回曲线（PRC），该曲线可视化了 PPV/精确度 与 TPR/召回 之间的关系。ROC曲线和PR曲线之间的主要区别在于后者忽略了真负例的数量。在不平衡的人群中，正类别很少见的情况下，具有高TPR的分类器可能仍然不太具有信息性，并且具有较低的PPV。有关PRC和ROC曲线之间关系的详细讨论，请参阅 Davis 和 Goadrich（2006）。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(prediction, type = \"prc\")\n```\n\n::: {.cell-output-display}\n![Precision-Recall curve based on `tsk(\"german_credit\")` and `lrn(\"classif.ranger\")`.](index_files/figure-html/fig-basics-prc-ranger-1.png){#fig-basics-prc-ranger fig-align='center' width=70%}\n:::\n:::\n\n\nFinally, we can visualize ROC/PR curves for a `BenchmarkResult` to compare multiple learners on the same `Task`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndesign = benchmark_grid(\n  tasks = tsk(\"german_credit\"),\n  learners = lrns(c(\"classif.rpart\", \"classif.ranger\"),\n                  predict_type = \"prob\"),\n  resamplings = rsmp(\"cv\", folds = 5)\n)\nbmr = benchmark(design)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(bmr, type = \"roc\") +\n  autoplot(bmr, type = \"prc\") +\n  plot_layout(guides = \"collect\")\n```\n\n::: {.cell-output-display}\n![Comparing random forest (green) and decision tree (purple) using ROC and PR Curves.](index_files/figure-html/fig-basics-rocpr-bmr-1.png){#fig-basics-rocpr-bmr fig-align='center' width=70%}\n:::\n:::\n\n\n# Tuning and Feature Selection {.unnumbered}\n\n# Hyperparameter Optimization\n\nHyperparameter optimization (HPO) closely relates to model evaluation (@sec-performance) as the objective is to find a hyperparameter configuration that optimizes the generalization performance. Broadly speaking, we could think of finding the optimal model configuration in the same way as selecting a model from a benchmark experiment, where in this case each model in the experiment is the same algorithm but with different hyperparameter configurations. For example, we could benchmark three support vector machines (SVMs) with three different `cost` values.\n\n> HPO与模型评估（@sec-performance）密切相关，因为目标是找到一个优化泛化性能的超参数配置。从广义上讲，我们可以将找到最佳模型配置视为从基准实验中选择模型的方式，其中在这种情况下，实验中的每个模型都是相同的算法，但具有不同的超参数配置。例如，我们可以使用三个不同 `cost` 值来进行支持向量机（SVM）的基准测试。\n\n## Model Tuning\n\n`mlr3tuning` is the hyperparameter optimization package of the `mlr3` ecosystem. At the heart of the package are the R6 classes\n\n- `TuningInstanceSingleCrit`, a tuning ‘instance’ that describes the optimization problem and store the results; and\n\n- `Tuner` which is used to configure and run optimization algorithms.\n\n### Learner and Search Space\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(lrn(\"classif.svm\")$param_set)[,\n                                      .(id, class, lower, upper, nlevels)]\n#>                  id    class lower upper nlevels\n#>  1:       cachesize ParamDbl  -Inf   Inf     Inf\n#>  2:   class.weights ParamUty    NA    NA     Inf\n#>  3:           coef0 ParamDbl  -Inf   Inf     Inf\n#>  4:            cost ParamDbl     0   Inf     Inf\n#>  5:           cross ParamInt     0   Inf     Inf\n#>  6: decision.values ParamLgl    NA    NA       2\n#>  7:          degree ParamInt     1   Inf     Inf\n#>  8:         epsilon ParamDbl     0   Inf     Inf\n#>  9:          fitted ParamLgl    NA    NA       2\n#> 10:           gamma ParamDbl     0   Inf     Inf\n#> 11:          kernel ParamFct    NA    NA       4\n#> 12:              nu ParamDbl  -Inf   Inf     Inf\n#> 13:           scale ParamUty    NA    NA     Inf\n#> 14:       shrinking ParamLgl    NA    NA       2\n#> 15:       tolerance ParamDbl     0   Inf     Inf\n#> 16:            type ParamFct    NA    NA       2\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.svm\",\n    type = \"C-classification\",\n    kernel = \"radial\",\n    cost = to_tune(1e-1, 1e5),\n    gamma = to_tune(1e-1, 1))\n\nlearner\n#> <LearnerClassifSVM:classif.svm>\n#> * Model: -\n#> * Parameters: type=C-classification, kernel=radial,\n#>   cost=<RangeTuneToken>, gamma=<RangeTuneToken>\n#> * Packages: mlr3, mlr3learners, e1071\n#> * Predict Types:  [response], prob\n#> * Feature Types: logical, integer, numeric\n#> * Properties: multiclass, twoclass\n```\n:::\n\n\n### Terminator {#sec-terminator}\n\n`mlr3tuning` includes many methods to specify when to terminate an algorithm (@tbl-terms), which are implemented in `Terminator` classes. Terminators are stored in the `mlr_terminators` dictionary and are constructed with the sugar function `trm()`.\n\n| Terminator            | Function call and default parameters                                    |\n|-----------------------|-------------------------------------------------------------------------|\n| Clock Time            | `trm(\"clock_time\")`              |\n| Combo                 | `trm(\"combo\", any = TRUE)` |\n| None                  | `trm(\"none\")`                                                           |\n| Number of Evaluations | `trm(\"evals\", n_evals = 100, k = 0)`                                           |\n| Performance Level     | `trm(\"perf_reached\", level = 0.1)`                                      |\n| Run Time              | `trm(\"run_time\", secs = 30)`                                           |\n| Stagnation            | `trm(\"stagnation\", iters = 10, threshold = 0)`                        |\n\n: Terminators available in `mlr3tuning` at the time of publication, their function call and default parameters. A complete and up-to-date list can be found at <https://mlr-org.com/terminators.html>. {#tbl-terms}\n\nThe most commonly used terminators are those that stop the tuning after a certain time (`trm(\"run_time\")`) or a given number of evaluations (`trm(\"evals\")`). Choosing a runtime is often based on practical considerations and intuition. Using a time limit can be important on compute clusters where a maximum runtime for a compute job may need to be specified. `trm(\"perf_reached\")` stops the tuning when a specified performance level is reached, which can be helpful if a certain performance is seen as sufficient for the practical use of the model, however, if this is set too optimistically the tuning may never terminate. `trm(\"stagnation\")` stops when no progress greater than the threshold has been made for a set number of iterations. The threshold can be difficult to select as the optimization could stop too soon for complex search spaces despite room for (possibly significant) improvement. `trm(\"none\")` is used for tuners that control termination themselves and so this terminator does nothing. Finally, any of these terminators can be freely combined by using `trm(\"combo\")`, which can be used to specify if HPO finishes when any (`any = TRUE`) terminator is triggered or when all (`any = FALSE`) are triggered.\n\n> 最常用的终止条件通常是那些在一定时间（`trm(\"run_time\")`）或给定的评估次数（`trm(\"evals\")`）之后停止调优的条件。选择运行时间通常基于实际考虑和直觉。在计算集群上使用时间限制可能很重要，因为可能需要为计算作业指定最大运行时间。`trm(\"perf_reached\")`在达到指定性能水平时停止调优，这可以在某种性能被视为足够实际使用的情况下很有帮助，但如果设置得过于乐观，调优可能永远不会结束。`trm(\"stagnation\")`在一定迭代次数内没有超过阈值的进展时停止，阈值的选择可能很困难，因为尽管可能有改进的空间（可能很大），但对于复杂的搜索空间，优化可能会过早停止。`trm(\"none\")`用于控制自己终止的调谐器，因此该终止条件什么也不做。最后，任何这些终止条件都可以通过使用`trm(\"combo\")`自由组合，可以用来指定HPO是否在任何（`any = TRUE`）终止条件触发时结束，或者在所有（`any = FALSE`）终止条件触发时结束。\n\n### Tuning Instance with `ti`\n\n::: {.callout-caution}\nTODO：等待后续添加交叉引用  5\n\n已加，待验证\n:::\n\nThe tuning instance collects the tuner-agnostic information required to optimize a model, i.e., all information about the tuning process, except for the tuning algorithm itself. This includes the task to tune over, the learner to tune, the resampling method and measure used to analytically compare hyperparameter optimization configurations, and the terminator to determine when the measure has been optimized ‘enough’. This implicitly defines a “black box” objective function, mapping hyperparameter configurations to (stochastic) performance values, to be optimized. This concept will be revisited in @sec-optimization-advanced.\n\n> 调优实例收集了优化模型所需的与调谐器无关的信息，即所有与调优过程有关的信息，除了调谐算法本身。这包括要调优的任务、要调优的学习器、用于分析比较超参数优化配置的重抽样方法和度量，以及确定度量何时已经被优化到足够程度的终止条件。这隐式地定义了一个“黑盒”目标函数，将超参数配置映射到（随机的）性能值，以便进行优化。这个概念将在 @sec-optimization-advanced 中重新讨论。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_sonar = tsk(\"sonar\")\n\ninstance = ti(\n  task = tsk_sonar,\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"classif.ce\"),\n  terminator = trm(\"none\")\n)\n\ninstance\n#> <TuningInstanceSingleCrit>\n#> * State:  Not optimized\n#> * Objective: <ObjectiveTuning:classif.svm_on_sonar>\n#> * Search Space:\n#>       id    class lower upper nlevels\n#> 1:  cost ParamDbl   0.1 1e+05     Inf\n#> 2: gamma ParamDbl   0.1 1e+00     Inf\n#> * Terminator: <TerminatorNone>\n```\n:::\n\n\n### Tuner\n\nWith all the pieces of our tuning problem assembled, we can now decide how to tune our model. There are multiple `Tuner` classes in `mlr3tuning`, which implement different HPO (or more generally speaking black box optimization) algorithms (@tbl-tuners).\n\n| Tuner                           | Function call          | Package               |\n|---------------------------------|------------------------|-----------------------|\n| Random Search                   | `tnr(\"random_search\")` | `mlr3tuning`        |\n| Grid Search                     | `tnr(\"grid_search\")`   | `mlr3tuning`        |\n| Bayesian Optimization           | `tnr(\"mbo\")`           | `mlr3mbo`           |\n| CMA-ES                          | `tnr(\"cmaes\")`         | `adagio` |\n| Iterated Racing                | `tnr(\"irace\")`         | `irace`  |\n| Hyperband                       | `tnr(\"hyperband\")`     | `mlr3hyperband`     |\n| Generalized Simulated Annealing | `tnr(\"gensa\")`         | `GenSA`  |\n| Nonlinear Optimization          | `tnr(\"nloptr\")`        | `nloptr` |\n\n: Tuning algorithms available in `mlr3tuning`, their function call and the package in which the algorithm is implemented. A complete and up-to-date list can be found at <https://mlr-org.com/tuners.html>. {#tbl-tuners}\n\n#### Search Strategies\n\n::: {.callout-caution}\nTODO：等待后续添加交叉引用 4.4\n\n已加，待检查\n:::\n\nGrid search and random search (Bergstra and Bengio 2012) are the most basic algorithms and are often selected first in initial experiments. The idea of grid search is to exhaustively evaluate every possible combination of given hyperparameter values. Categorical hyperparameters are usually evaluated over all possible values they can take. Numeric and integer hyperparameter values are then spaced equidistantly in their box constraints (upper and lower bounds) according to a given resolution, which is the number of distinct values to try per hyperparameter. Random search involves randomly selecting values for each hyperparameter independently from a pre-specified distribution, usually uniform. Both methods are non-adaptive, which means each proposed configuration ignores the performance of previous configurations. Due to their simplicity, both grid search and random search can handle mixed search spaces (i.e., hyperparameters can be numeric, integer, or categorical) as well as hierarchical search spaces (@sec-defining-search-spaces).\n\n> 网格搜索和随机搜索（Bergstra和Bengio 2012）是最基本的算法，通常在初始实验中首选。网格搜索的思想是详尽地评估给定超参数值的每种可能组合。通常会对分类超参数评估它们可以取的所有可能值。然后，数值和整数超参数值将根据给定的分辨率均匀分布在它们的箱约束（上下界）中，分辨率是每个超参数要尝试的不同值的数量。随机搜索涉及从预先指定的分布（通常是均匀分布）中独立地随机选择每个超参数的值。这两种方法都是非自适应的，这意味着每个提出的配置都忽略了先前配置的性能。由于它们的简单性，网格搜索和随机搜索可以处理混合搜索空间（即，超参数可以是数值、整数或分类的）以及分层搜索空间（@sec-defining-search-spaces）。\n\n#### Adaptive Algorithms\n\n::: {.callout-caution}\nTODO：等待后续添加交叉引用 5.3 5.4\n:::\n\nAdaptive algorithms learn from previously evaluated configurations to find good configurations quickly, examples in `mlr3` include Bayesian optimization (also called model-based optimization), Covariance Matrix Adaptation Evolution Strategy (CMA-ES), Iterated Racing, and Hyperband.\n\nBayesian optimization (e.g., Snoek, Larochelle, and Adams 2012) describes a family of iterative optimization algorithms that use a surrogate model to approximate the unknown function that is to be optimized – in HPO this would be the mapping from a hyperparameter configuration to the estimated generalization performance. If a suitable surrogate model is chosen, e.g. a random forest, Bayesian optimization can be quite flexible and even handle mixed and hierarchical search spaces. Bayesian optimization is discussed in full detail in Section 5.4.\n\nCMA-ES (Hansen and Auger 2011) is an evolutionary strategy that maintains a probability distribution over candidate points, with the distribution represented by a mean vector and covariance matrix. A new set of candidate points is generated by sampling from this distribution, with the probability of each candidate being proportional to its performance. The covariance matrix is adapted over time to reflect the performance landscape. Further evolutionary strategies are available in `mlr3` via the `miesmuschel` package, however, these will not be covered in this book.\n\nRacing algorithms work by iteratively discarding configurations that show poor performance, as determined by statistical tests. Iterated Racing (López-Ibáñez et al. 2016) starts by ‘racing’ down an initial population of randomly sampled configurations from a parameterized density and then uses the surviving configurations of the race to stochastically update the density of the subsequent race to focus on promising regions of the search space, and so on.\n\nMulti-fidelity HPO is an adaptive method that leverages the predictive power of computationally cheap lower fidelity evaluations (i.e., poorer quality predictions such as those arising from neural networks with a small number of epochs) to improve the overall optimization efficiency. This concept is used in Hyperband (Li et al. 2018), a popular multi-fidelity hyperparameter optimization algorithm that dynamically allocates increasingly more resources to promising configurations and terminates low-performing ones. Hyperband is discussed in full detail in @sec-hyperband.\n\nOther implemented algorithms for numeric search spaces are Generalized Simulated Annealing (Xiang et al. 2013; Tsallis and Stariolo 1996) and various nonlinear optimization algorithms.\n\n> 自适应算法通过学习先前评估的配置来快速找到良好的配置，`mlr3`中的示例包括贝叶斯优化（也称为基于模型的优化）、协方差矩阵自适应进化策略（CMA-ES）、迭代比赛和Hyperband。\n>\n> 贝叶斯优化（例如，Snoek、Larochelle和Adams 2012）描述了一族迭代优化算法，这些算法使用替代模型来近似待优化的未知函数——在HPO中，这将是从超参数配置到估计的泛化性能的映射。如果选择了合适的替代模型，例如随机森林，贝叶斯优化可以非常灵活，甚至可以处理混合和分层搜索空间。贝叶斯优化将在第5.4节中详细讨论。\n>\n> CMA-ES（Hansen和Auger 2011）是一种进化策略，它维护了候选点的概率分布，分布由均值向量和协方差矩阵表示。通过从该分布中抽样生成一组新的候选点，每个候选点的选择概率与其性能成正比。协方差矩阵会随着时间的推移而适应反映性能景观。通过`mlr3`中的`miesmuschel`包，还提供了其他进化策略，不过本书不会涵盖这些内容。\n>\n> 比赛算法通过迭代地丢弃显示性能较差的配置，这是通过统计测试确定的。迭代比赛（López-Ibáñez等人2016）首先通过从参数化密度中随机抽样生成的一组初始配置进行“比赛”，然后使用比赛的生存配置来随机更新后续比赛的密度，以便集中在搜索空间的有前途的区域，依此类推。\n>\n> 多保真度HPO是一种自适应方法，利用计算成本低的低保真度评估（即质量较差的预测，例如由具有较少周期的神经网络产生的预测）来提高整体优化效率。这个概念在Hyperband（Li等人2018）中得到了应用，这是一种流行的多保真度超参数优化算法，动态分配更多资源给有前途的配置并终止性能较低的配置。Hyperband将在 @sec-hyperband 中详细讨论。\n>\n> 对于数值搜索空间，其他已实现的算法包括广义模拟退火（Xiang等人2013；Tsallis和Stariolo 1996）和各种非线性优化算法。\n\n#### Choosing Strategies\n\nAs a rule of thumb, if the search space is small or does not have a complex structure, grid search may be able to exhaustively evaluate the entire search space in a reasonable time. However, grid search is generally not recommended due to the curse of dimensionality – the grid size ‘blows up’ very quickly as the number of parameters to tune increases – and insufficient coverage of numeric search spaces. By construction, grid search cannot evaluate a large number of unique values per hyperparameter, which is suboptimal when some hyperparameters have minimal impact on performance while others do. In such scenarios, random search is often a better choice as it considers more unique values per hyperparameter compared to grid search.\n\nFor higher-dimensional search spaces or search spaces with more complex structure, more guided optimization algorithms such as evolutionary strategies or Bayesian optimization tend to perform better and are more likely to result in peak performance. When choosing between evolutionary strategies and Bayesian optimization, the cost of function evaluation is highly relevant. If hyperparameter configurations can be evaluated quickly, evolutionary strategies often work well. On the other hand, if model evaluations are time-consuming and the optimization budget is limited, Bayesian optimization is usually preferred, as it is quite sample efficient compared to other algorithms, i.e., less function evaluations are needed to find good configurations. Hence, Bayesian optimization is usually recommended for HPO. While the optimization overhead of Bayesian optimization is comparably large (e.g., in each iteration, training of the surrogate model and optimizing the acquisition function), this has less of an impact in the context of relatively costly function evaluations such as resampling of ML models.\n\nFinally, in cases where the hyperparameter optimization problem involves a meaningful fidelity parameter (e.g., number of epochs, number of trees, number of boosting rounds) and where the optimization budget needs to be spent efficiently, multi-fidelity hyperparameter optimization algorithms like Hyperband may be worth considering. For further details on different tuners and practical recommendations, we refer to Bischl et al. (2023).\n\n> 作为一个经验法则，如果搜索空间较小或没有复杂的结构，网格搜索可能能够在合理的时间内详尽地评估整个搜索空间。然而，通常不建议使用网格搜索，因为维度的诅咒问题——随着要调整的参数数量的增加，网格大小会迅速增加——以及对数值搜索空间的不足覆盖。从构造上来说，网格搜索不能评估每个超参数的大量唯一值，这在某些超参数对性能影响较小而其他超参数对性能有显著影响的情况下是不够优化的。在这种情况下，随机搜索通常是更好的选择，因为它考虑了每个超参数的更多唯一值，相对于网格搜索而言。\n>\n> 对于维度较高的搜索空间或搜索空间具有更复杂结构的情况，更有导向性的优化算法，如进化策略或贝叶斯优化，往往表现更好，并更有可能产生最佳性能。在选择进化策略和贝叶斯优化之间，函数评估成本非常重要。如果可以快速评估超参数配置，通常进化策略效果良好。另一方面，如果模型评估需要耗费时间，且优化预算有限，通常首选贝叶斯优化，因为与其他算法相比，它相对高效，即需要更少的函数评估来找到好的配置。因此，通常建议在HPO中使用贝叶斯优化。虽然贝叶斯优化的优化开销相对较大（例如，在每个迭代中，训练替代模型和优化获取函数），但在相对昂贵的函数评估环境中，例如ML模型的重新抽样，这影响较小。\n>\n> 最后，在超参数优化问题涉及有意义的保真度参数（例如，周期数、树数、提升轮数）且需要高效利用优化预算的情况下，可能值得考虑使用多保真度超参数优化算法，例如Hyperband。关于不同调谐器和实际建议的更多详细信息，请参阅Bischl等人（2023）。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntuner = tnr(\"grid_search\", resolution = 5, batch_size = 10)\ntuner\n#> <TunerGridSearch>: Grid Search\n#> * Parameters: resolution=5, batch_size=10\n#> * Parameter classes: ParamLgl, ParamInt, ParamDbl, ParamFct\n#> * Properties: dependencies, single-crit, multi-crit\n#> * Packages: mlr3tuning\n```\n:::\n\n\n::: {.callout-caution}\nTODO：等待后续添加交叉引用 10.1.3\n:::\n\nFor our SVM example, we will use a grid search with a resolution of five for runtime reasons here (in practice a larger resolution would be preferred). The resolution is the number of distinct values to try per hyperparameter, which means in our example the tuner will construct a 5x5 grid of 25 configurations of equally spaced points between the specified upper and lower bounds. All configurations will be tried by the tuner (in random order) until either all configurations are evaluated or the terminator (@sec-terminator) signals that the budget is exhausted. For grid and random search tuners, the `batch_size` parameter controls how many configurations are evaluated at the same time when parallelization is enabled (see Section 10.1.3), and also determines how many configurations should be applied before the terminator should check if the termination criterion has been reached.\n\n> 对于我们的SVM示例，出于运行时的原因，我们将使用具有五个分辨率的网格搜索（在实践中，更大的分辨率将更可取）。分辨率是每个超参数要尝试的不同值的数量，这意味着在我们的示例中，调谐器将构建一个5x5的网格，其中包含25个在指定上限和下限之间等间距点的配置。调谐器将尝试所有配置（以随机顺序），直到所有配置都被评估或终止器（@sec-terminator）发出预算已用尽的信号。对于网格搜索和随机搜索调谐器，`batch_size` 参数控制在启用并行化时同时评估多少个配置（请参阅第10.1.3节），并确定在终止器检查是否达到终止标准之前应用多少个配置。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntuner$param_set\n#> <ParamSet>\n#>                   id    class lower upper nlevels        default value\n#> 1:        batch_size ParamInt     1   Inf     Inf <NoDefault[3]>    10\n#> 2:        resolution ParamInt     1   Inf     Inf <NoDefault[3]>     5\n#> 3: param_resolutions ParamUty    NA    NA     Inf <NoDefault[3]>\n```\n:::\n\n\nWhile changing the control parameters of the tuner can improve optimal performance, we have to take care that is likely the default settings will fit most needs. While it is not possible to cover all application cases, `mlr3tuning`’s defaults were chosen to work well in most cases. However, some control parameters like `batch_size` often interact with the parallelization setup (further described in Section 10.1.3) and may need to be adjusted accordingly.\n\n> 尽管更改调谐器的控制参数可以改善最优性能，但我们必须注意，通常情况下默认设置将适用于大多数需求。虽然不可能涵盖所有应用情况，但`mlr3tuning`的默认设置被选择为在大多数情况下表现良好。但是，一些控制参数，`如batch_size`，通常与并行化设置互动（在第10.1.3节中进一步描述），可能需要相应地进行调整。\n\n#### Triggering the tuning process\n\nNow that we have introduced all our components, we can start the tuning process. To do this we simply pass the constructed TuningInstanceSingleCrit to the $optimize() method of the initialized Tuner.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntuner$optimize(instance)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance$result$learner_param_vals\n#> [[1]]\n#> [[1]]$type\n#> [1] \"C-classification\"\n#> \n#> [[1]]$kernel\n#> [1] \"radial\"\n#> \n#> [[1]]$cost\n#> [1] 50000.05\n#> \n#> [[1]]$gamma\n#> [1] 0.1\n```\n:::\n\n\n### Logarithmic Transformations\n\nTo add this transformation to a hyperparameter we simply pass `logscale = TRUE` to `to_tune()`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.svm\", \n    cost = to_tune(1e-5, 1e5, logscale = TRUE),\n    gamma = to_tune(1e-5, 1e5, logscale = TRUE),\n    kernel = \"radial\",\n    type = \"C-classification\")\n\ninstance = ti(\n  task = tsk_sonar,\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"classif.ce\"),\n  terminator = trm(\"none\")\n)\n\ntuner$optimize(instance)\n```\n:::\n\n\nNote that the fields `cost` and `gamma` show the optimal values before transformation, whereas `x_domain` and `learner_param_vals` contain optimal values *after* transformation, it is these latter fields you would take forward for future model use.\n\n> 请注意，`cost`和`gamma`字段显示了变换之前的最佳值，而`x_domain`和`learner_param_vals`包含了变换之后的最佳值，对于未来的模型使用，您应该使用后者的字段。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance$result$x_domain\n#> [[1]]\n#> [[1]]$cost\n#> [1] 1e+05\n#> \n#> [[1]]$gamma\n#> [1] 0.003162278\n```\n:::\n\n\n### Analyzing and Using the Result\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(instance$archive)[1:3, .(cost, gamma, classif.ce)]\n#>         cost      gamma classif.ce\n#> 1: -5.756463   0.000000  0.4663216\n#> 2: -5.756463   5.756463  0.4663216\n#> 3:  0.000000 -11.512925  0.4663216\n```\n:::\n\n\nAnother powerful feature of the instance is that we can score the internal `ResampleResults` on a different performance measure, for example looking at false negative rate and false positive rate as well as classification error:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(\n  instance$archive,\n  measures = msrs(c(\"classif.fpr\", \"classif.fnr\"))\n)[1:5, .(cost, gamma, classif.ce, classif.fpr, classif.fnr)]\n#>         cost      gamma classif.ce classif.fpr classif.fnr\n#> 1: -5.756463   0.000000  0.4663216    1.000000   0.0000000\n#> 2: -5.756463   5.756463  0.4663216    1.000000   0.0000000\n#> 3:  0.000000 -11.512925  0.4663216    1.000000   0.0000000\n#> 4:  0.000000  -5.756463  0.2400966    0.277289   0.2077999\n#> 5:  0.000000  11.512925  0.4663216    1.000000   0.0000000\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(instance, type = \"surface\")\n```\n\n::: {.cell-output-display}\n![Model performance with different configurations for `cost` and `gamma`. Bright yellow regions represent the model performing worse and dark blue performing better. We can see that high `cost` values and low `gamma` values achieve the best performance. Note that we should not directly infer the performance of new unseen values from the heatmap since it is only an interpolation based on a surrogate model (`regr.ranger`). However, we can see the general interaction between the hyperparameters.](index_files/figure-html/fig-surface-1.png){#fig-surface fig-align='center' width=70%}\n:::\n:::\n\n\nOnce we found good hyperparameters for our learner through tuning, we can use them to train a final model on the whole data. To do this we simply construct a new learner with the same underlying algorithm and set the learner hyperparameters to the optimal configuration:\n\n> 在通过调整找到学习器的良好超参数之后，我们可以使用它们在整个数据集上训练最终模型。为此，我们只需构建一个新的学习器，使用相同的底层算法，并将学习器的超参数设置为最佳配置：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_svm_tuned = lrn(\"classif.svm\")\nlrn_svm_tuned$param_set$values = instance$result_learner_param_vals\nlrn_svm_tuned$train(tsk_sonar)$model\n#> \n#> Call:\n#> svm.default(x = data, y = task$truth(), type = \"C-classification\", \n#>     kernel = \"radial\", gamma = 0.00316227766016838, cost = 1e+05, \n#>     probability = (self$predict_type == \"prob\"))\n#> \n#> \n#> Parameters:\n#>    SVM-Type:  C-classification \n#>  SVM-Kernel:  radial \n#>        cost:  1e+05 \n#> \n#> Number of Support Vectors:  93\n```\n:::\n\n\n## Convenient Tuning with `tune` and `auto_tuner`\n\nIn the previous section, we looked at constructing and manually putting together the components of HPO by creating a tuning instance using `ti()`, passing this to the tuner, and then calling `$optimize()` to start the tuning process. `mlr3tuning` includes two helper methods to simplify this process further.\n\nThe first helper function is `tune()`, which creates the tuning instance and calls `$optimize()` for you. You may prefer the manual method with `ti()` if you want to view and make changes to the instance before tuning.\n\n> 在上一节中，我们看到了通过使用`ti()`创建调整实例，将其传递给调整器，然后调用`$optimize()`来启动调整过程，来构建和手动组合HPO的组件。`mlr3tuning`包括两个辅助方法，以进一步简化这个过程。\n>\n> 第一个辅助函数是`tune()`，它创建调整实例并为您调用`$optimize()`。如果您想在调整之前查看并对实例进行更改，可能更喜欢使用`ti()`的手动方法。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntnr_grid_search = tnr(\"grid_search\", resolution = 5, batch_size = 5)\nlrn_svm = lrn(\n  \"classif.svm\",\n  cost = to_tune(1e-5, 1e5, logscale = TRUE),\n  gamma = to_tune(1e-5, 1e5, logscale = TRUE),\n  kernel = \"radial\",\n  type = \"C-classification\"\n)\nrsmp_cv3 = rsmp(\"cv\", folds = 3)\nmsr_ce = msr(\"classif.ce\")\n\ninstance = tune(\n  tuner = tnr_grid_search,\n  task = tsk_sonar,\n  learner = lrn_svm,\n  resampling = rsmp_cv3,\n  measures = msr_ce\n)\ninstance$result\n```\n:::\n\n\nThe other helper function is `auto_tuner`, which creates an object of class `AutoTuner`. The `AutoTuner` inherits from the `Learner` class and wraps all the information needed for tuning, which means you can treat a learner waiting to be optimized just like any other learner. Under the hood, the `AutoTuner` essentially runs `tune()` on the data that is passed to the model when `$train()` is called and then sets the learner parameters to the optimal configuration.\n\n> 另一个辅助函数是`auto_tuner`，它创建一个`AutoTuner`类的对象。`AutoTuner`继承自`Learner`类，并包装了所有需要进行调整的信息，这意味着您可以像处理任何其他学习器一样处理等待优化的学习器。在底层，`AutoTuner`实际上在调用`$train()`时对传递给模型的数据上运行了`tune()`，然后将学习器参数设置为最佳配置。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nat = auto_tuner(\n  tuner = tnr_grid_search,\n  learner = lrn_svm,\n  resampling = rsmp_cv3,\n  measure = msr_ce\n)\n\nat\n#> <AutoTuner:classif.svm.tuned>\n#> * Model: list\n#> * Search Space:\n#> <ParamSet>\n#>       id    class     lower    upper nlevels        default value\n#> 1:  cost ParamDbl -11.51293 11.51293     Inf <NoDefault[3]>      \n#> 2: gamma ParamDbl -11.51293 11.51293     Inf <NoDefault[3]>      \n#> Trafo is set.\n#> * Packages: mlr3, mlr3tuning, mlr3learners, e1071\n#> * Predict Type: response\n#> * Feature Types: logical, integer, numeric\n#> * Properties: multiclass, twoclass\n```\n:::\n\n\nAnd we can now call `$train()`, which will first tune the hyperparameters in the search space listed above before fitting the optimal model.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsplit = partition(tsk_sonar)\nat$train(tsk_sonar, row_ids = split$train)\nat$predict(tsk_sonar, row_ids = split$test)$score()\n```\n:::\n\n\nThe `AutoTuner` contains a tuning instance that can be analyzed like any other instance.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nat$tuning_instance$result\n#>        cost     gamma learner_param_vals  x_domain classif.ce\n#> 1: 5.756463 -11.51293          <list[4]> <list[2]>  0.2377428\n```\n:::\n\n\nWe could also pass the `AutoTuner` to `resample()` and `benchmark()`, which would result in a nested resampling, discussed next.\n\n## Nested Resampling\n\nNested resampling separates model optimization from the process of estimating the performance of the tuned model by adding an additional resampling, i.e., while model performance is estimated using a resampling method in the ‘usual way’, tuning is then performed by resampling the resampled data (@fig-nested-resampling).\n\n> 嵌套重抽样通过添加额外的重抽样来将模型优化与估计调整模型性能的过程分开，即在“通常方式”中使用重抽样方法来估计模型性能，然后通过对重抽样数据进行重抽样来进行调整（@fig-nested-resampling）。\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![An illustration of nested resampling. The large blocks represent three-fold CV for the outer resampling for model evaluation and the small blocks represent four-fold CV for the inner resampling for HPO. The light blue blocks are the training sets and the dark blue blocks are the test sets.](imgs/mlr3book_figures-11.svg){#fig-nested-resampling fig-align='center' fig-alt='The image shows three rows of large blocks representing three-fold CV for the outer resampling. Below the blocks are four further rows of small blocks representing four-fold CV for the inner resampling. Text annotations highlight how tuned parameters from the inner resampling are passed to the outer resampling.' width=70%}\n:::\n:::\n\n\n@fig-nested-resampling represents the following example of nested resampling:\n\n1. Outer resampling start – Instantiate three-fold CV to create different testing and training datasets.\n\n1. Inner resampling – Within the outer training data instantiate four-fold CV to create different inner testing and training datasets.\n\n1. HPO – Tune the hyperparameters on the outer training set (large, light blue blocks) using the inner data splits.\n\n1. Training – Fit the learner on the outer training dataset using the optimal hyperparameter configuration obtained from the inner resampling (small blocks).\n\n1. Evaluation – Evaluate the performance of the learner on the outer testing data (large, dark blue block).\n\n1. Outer resampling repeats – Repeat (2)-(5) for each of the three outer folds.\n\n1. Aggregation – Take the sample mean of the three performance values for an unbiased performance estimate.\n\nThe inner resampling produces generalization performance estimates for each configuration and selects the optimal configuration to be evaluated on the outer resampling. The outer resampling then produces generalization estimates for these optimal configurations. The result from the outer resampling can be used for comparison to other models trained and tested on the same outer folds.\n\n> @fig-nested-resampling 表示嵌套重抽样的以下示例：\n>\n> 1. 外部重抽样开始 - 实例化三折交叉验证以创建不同的测试和训练数据集。\n>\n> 2. 内部重抽样 - 在外部训练数据中实例化四折交叉验证以创建不同的内部测试和训练数据集。\n>\n> 3. HPO - 使用内部数据拆分在外部训练集（大的浅蓝色块）上调整超参数。\n>\n> 4. 训练 - 使用从内部重抽样获得的最佳超参数配置在外部训练数据集上拟合学习器（小块）。\n>\n> 5. 评估 - 在外部测试数据上评估学习器的性能（大的深蓝色块）。\n>\n> 6. 外部重抽样重复 - 对三个外部折叠中的每一个重复步骤（2）-(5)。\n>\n> 7. 聚合 - 取三个性能值的样本均值以获得无偏性能估计。\n>\n> 内部重抽样为每个配置生成泛化性能估计，并选择要在外部重抽样中评估的最佳配置。然后，外部重抽样为这些最佳配置生成泛化估计。外部重抽样的结果可以用于与在相同外部折叠上训练和测试的其他模型进行比较。\n\nA common mistake is to think of nested resampling as a method to select optimal model configurations. Nested resampling is a method to compare models and to estimate the generalization performance of a tuned model, however, this is the performance based on multiple different configurations (one from each outer fold) and not performance based on a single configuration. If you are interested in identifying optimal configurations, then use `tune()`/`ti()` or `auto_tuner()` with `$train()` on the complete dataset.\n\n> 一个常见的错误是将嵌套重抽样视为选择最佳模型配置的方法。嵌套重抽样是一种用于比较模型和估计调整后模型的泛化性能的方法，但这是基于多种不同配置的性能（每个配置来自于外部折叠的一个），而不是基于单个配置的性能。如果您有兴趣确定最佳配置，那么请使用`tune()`/`ti()`或`auto_tuner()`与`$train()`在完整数据集上进行操作。\n\n### Nested Resampling with an `AutoTuner`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nat = auto_tuner(\n  tuner = tnr_grid_search,\n  learner = lrn_svm,\n  resampling = rsmp(\"cv\", folds = 4),\n  measure = msr_ce\n)\n\nrr = resample(\n  task = tsk_sonar,\n  learner = at,\n  resampling = rsmp_cv3,\n  store_models = TRUE\n)\n\nrr\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr$aggregate()\n#> classif.ce \n#>  0.1733609\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nextract_inner_tuning_results(rr)[,\n           .(iteration, cost, gamma, classif.ce)]\n#>    iteration     cost     gamma classif.ce\n#> 1:         1 11.51293 -5.756463  0.2533613\n#> 2:         2 11.51293 -5.756463  0.1573529\n#> 3:         3 11.51293 -5.756463  0.1441176\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nextract_inner_tuning_archives(rr)[1:3,\n              .(iteration, cost, gamma, classif.ce)]\n#>    iteration       cost    gamma classif.ce\n#> 1:         1 -11.512925  0.00000  0.5728992\n#> 2:         1  -5.756463  0.00000  0.5728992\n#> 3:         1  -5.756463 11.51293  0.5728992\n```\n:::\n\n\n### The Right (and Wrong) Way to Estimate Performance\n\nIn this short section we will empirically demonstrate that directly reporting tuning performance without nested resampling results in optimistically biased performance estimates.\n\n> 在这个简短的部分中，我们将通过实验证明，直接报告调优性能而不使用嵌套重抽样会导致性能估计存在乐观偏差。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_xgboost = lrn(\n  \"classif.xgboost\",\n  eta = to_tune(1e-4, 1, logscale = TRUE),\n  max_depth = to_tune(1, 20),\n  colsample_bytree = to_tune(1e-1, 1),\n  colsample_bylevel = to_tune(1e-1, 1),\n  lambda = to_tune(1e-3, 1e3, logscale = TRUE),\n  alpha = to_tune(1e-3, 1e3, logscale = TRUE),\n  subsample = to_tune(1e-1, 1)\n)\n\ntsk_moons = tgen(\"moons\")\ntsk_moons_train = tsk_moons$generate(100)\ntsk_moons_test = tsk_moons$generate(1e6)\n```\n:::\n\n\nNow we will tune the learner with respect to the classification error, using holdout resampling and random search with 700 evaluations. We then report the tuning performance without nested resampling.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntnr_random = tnr(\"random_search\")\nrsmp_holdout = rsmp(\"holdout\")\ntrm_evals700 = trm(\"evals\", n_evals = 700)\n\ninstance = tune(\n  tuner = tnr_random,\n  task = tsk_moons_train,\n  learner = lrn_xgboost,\n  resampling = rsmp_holdout,\n  measures = msr_ce,\n  terminator = trm_evals700\n)\n\ninsample = instance$result_y\n```\n:::\n\n\nNext, we estimate generalization error by nested resampling (below we use an outer five-fold CV), using an `AutoTuner`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# same setup as above\nat = auto_tuner(\n  tuner = tnr_random,\n  learner = lrn_xgboost,\n  resampling = rsmp_holdout,\n  measure = msr_ce,\n  terminator = trm_evals700\n)\n\nrsmp_cv5 = rsmp(\"cv\", folds = 5)\n\noutsample = resample(tsk_moons_train, at, rsmp_cv5)$aggregate()\n```\n:::\n\n\n\n\nAnd finally, we estimate the generalization error by training the tuned learner (i.e., using the values from the `instance` above) on the full training data again and predicting on the test data.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_xgboost_tuned = lrn(\"classif.xgboost\")\nlrn_xgboost_tuned$param_set$set_values(\n  .values = instance$result_learner_param_vals)\ngeneralization = lrn_xgboost_tuned$train(tsk_moons_train)$\n  predict(tsk_moons_test)$\n  score()\n```\n:::\n\n\n\n\nNow we can compare these three values:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nround(c(\n  true_generalization = as.numeric(generalization),\n  without_nested_resampling = as.numeric(insample),\n  with_nest_resampling = as.numeric(outsample)\n), 2)\n#>       true_generalization without_nested_resampling      with_nest_resampling \n#>                      0.29                      0.09                      0.21\n```\n:::\n\n\nWe find that the performance estimate from unnested tuning optimistically overestimates the true performance (which could indicate ‘meta-overfitting’ to the specific inner holdout-splits), while the outer estimate from nested resampling works much better.\n\n> 我们发现，未经嵌套重抽样的调优性能估计会乐观地高估真实性能（这可能表明对特定内部保留集的‘元过拟合’），而来自嵌套重抽样的外部估计效果要好得多。\n\n## More Advanced Search Spaces {#sec-defining-search-spaces}\n\n### Scalar Parameter Tuning\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = lrn(\n  \"classif.svm\",\n  cost = to_tune(1e-1, 1e5),\n  gamma = to_tune(1e-1, 1),\n  kernel = \"radial\",\n  type = \"C-classification\"\n)\n\nlearner$param_set$search_space()\n#> <ParamSet>\n#>       id    class lower upper nlevels        default value\n#> 1:  cost ParamDbl   0.1 1e+05     Inf <NoDefault[3]>      \n#> 2: gamma ParamDbl   0.1 1e+00     Inf <NoDefault[3]>\n```\n:::\n\n\nIn this example, we can see that `gamma` hyperparameter has class `ParamDbl`, with `lower = 0.1` and `upper = 1`, which was automatically created by `to_tune()` as we passed two numeric values to this function. If we wanted to tune over a non-numeric hyperparameter, we can still use `to_tune()`, which will infer the correct class to construct in the resulting parameter set. For example, say we wanted to tune the numeric `cost`, factor `kernel`, and logical `scale` hyperparameter in our SVM:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = lrn(\n  \"classif.svm\",\n  cost = to_tune(1e-1, 1e5),\n  kernel = to_tune(c(\"radial\", \"linear\")),\n  shrinking = to_tune(),\n  type = \"C-classification\"\n)\n\nlearner$param_set$search_space()\n#> <ParamSet>\n#>           id    class lower upper nlevels        default value\n#> 1:      cost ParamDbl   0.1 1e+05     Inf <NoDefault[3]>      \n#> 2:    kernel ParamFct    NA    NA       2 <NoDefault[3]>      \n#> 3: shrinking ParamLgl    NA    NA       2           TRUE\n```\n:::\n\n\nHere the `kernel` hyperparameter is a factor, so we simply pass in a vector corresponding to the levels we want to tune over. The `shrinking` hyperparameter is a logical, there are only two possible values this could take so we do not need to pass anything to `to_tune()`, it will automatically recognize this is a logical from `learner$param_set` and passes this detail to `learner$param_set$search_space()`. Similarly, for factor parameters, we could also use `to_tune()` without any arguments if we want to tune over all possible values. Finally, we can use `to_tune()` to treat numeric parameters as factors if we want to discretize them over a small subset of possible values, for example, if we wanted to find the optimal number of trees in a random forest we might only consider three scenarios: 100, 200, or 400 trees:\n\n> 在这里，`kernel` 超参数是一个因子，因此我们只需传入一个与我们要调整的级别相对应的向量。`shrinking` 超参数是一个逻辑型的，它只有两个可能的取值，所以我们不需要传递任何参数给 `to_tune()`，它会自动识别这是一个逻辑型，然后将这个细节传递给 `learner$param_set$search_space()`。类似地，对于因子参数，如果我们想要调整所有可能的值，我们也可以使用 `to_tune()` 而不带任何参数。最后，如果我们想要将数值参数视为因子，并希望将其离散化为可能值的一小部分，例如，如果我们想要找到随机森林中最佳的树的数量，我们可能只考虑三种情况：100、200 或 400 棵树：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn(\"classif.ranger\", num.trees = to_tune(c(100, 200, 400)))\n```\n:::\n\n\n### Defining Search Spaces with `ps`\n\nAs a simple example, let us look at how to create a search space to tune `cost` and `gamma` again:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsearch_space = ps(\n  cost = p_dbl(lower = 1e-1, upper = 1e5),\n  kernel = p_fct(c(\"radial\", \"linear\")),\n  shrinking = p_lgl()\n)\n```\n:::\n\n\nThis search space would then be passed to the `search_space` argument in `auto_tuner()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nti(\n  task = tsk_sonar,\n  learner = lrn(\"classif.svm\", type = \"C-classification\"),\n  resampling = rsmp_cv3,\n  measures = msr_ce,\n  terminator = trm(\"none\"),\n  search_space = search_space\n)\n#> <TuningInstanceSingleCrit>\n#> * State:  Not optimized\n#> * Objective: <ObjectiveTuning:classif.svm_on_sonar>\n#> * Search Space:\n#>           id    class lower upper nlevels\n#> 1:      cost ParamDbl   0.1 1e+05     Inf\n#> 2:    kernel ParamFct    NA    NA       2\n#> 3: shrinking ParamLgl    NA    NA       2\n#> * Terminator: <TerminatorNone>\n```\n:::\n\n\n### Transformations and Tuning Over Vectors\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn(\"classif.svm\", cost = to_tune(1e-5, 1e5, logscale = TRUE))$param_set$search_space()\n#> <ParamSet>\n#>      id    class     lower    upper nlevels        default value\n#> 1: cost ParamDbl -11.51293 11.51293     Inf <NoDefault[3]>      \n#> Trafo is set.\n```\n:::\n\n\nNotice that now the `lower` and `upper` fields correspond to the transformed bounds, i.e. $[\\log(1e-5), \\log(1e5)]$.\nTo manually create the same transformation, we can pass the transformation to the `trafo` argument in `p_dbl()` and set the bounds:\n\n> 请注意，现在`lower`和`upper`字段对应于经过变换的界限，即$[\\log(1e-5), \\log(1e5)]$。要手动创建相同的变换，我们可以将变换传递给`p_dbl()`中的`trafo`参数，并设置界限：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsearch_space = ps(cost = p_dbl(log(1e-5), log(1e5),\n                               trafo = \\(x) exp(x)))\nsearch_space\n#> <ParamSet>\n#>      id    class     lower    upper nlevels        default value\n#> 1: cost ParamDbl -11.51293 11.51293     Inf <NoDefault[3]>      \n#> Trafo is set.\n```\n:::\n\n\nWe can confirm it is correctly set by making use of the `$trafo()` method, which takes a named list and applies the specified transformations\n\n> 我们可以通过使用`$trafo()`方法来确认它是否设置正确，该方法接受一个命名的列表并应用指定的转换。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsearch_space$trafo(list(cost = 1))\n#> $cost\n#> [1] 2.718282\n```\n:::\n\n\nWhere transformations become the most powerful is in the ability to pass arbitrary functions that can act on single parameters or even the entire parameter set. As an example, consider a simple transformation to add ‘2’ to our range:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsearch_space = ps(cost = p_dbl(0, 3, trafo = \\(x) x + 2))\nsearch_space$trafo(list(cost = 1))\n#> $cost\n#> [1] 3\n```\n:::\n\n\nSimple transformations such as this can even be added directly to a learner by passing a `Param` object to `to_tune()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn(\"classif.svm\",\n    cost = to_tune(p_dbl(0, 3, trafo = \\(x) x + 2)))\n```\n:::\n\n\nMore complex transformations that require multiple arguments should be passed to the `.extra_trafo` parameter in `ps()`. `.extra_trafo` takes a function with parameters `x` and `param_set` where, during tuning, `x` will be a list containing the configuration being tested, and `param_set` is the whole parameter set. Below we first exponentiate the value of `cost` and then add ‘2’ if the `kernel` is `\"polynomial\"`.\n\n> 需要多个参数的更复杂的转换应该通过 `ps()` 中的 `.extra_trafo` 参数传递。`.extra_trafo` 接受一个带有参数 `x` 和 `param_set` 的函数，在调整过程中，`x` 将是一个包含正在测试的配置的列表，而 `param_set` 则是整个参数集。在下面的示例中，我们首先将 `cost` 的值取幂，然后如果 `kernel` 是 \"polynomial\"，就加上 '2'。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsearch_space = ps(\n  cost = p_dbl(-1, 1, trafo = \\(x) exp(x)),\n  kernel = p_fct(c(\"polynomial\", \"radial\")),\n  .extra_trafo = \\(x, param_set) {\n    if (x$kernel == \"polynomial\") {\n      x$cost = x$cost + 2\n    }\n    x\n  }\n)\n\nsearch_space$trafo(list(cost = 1, kernel = \"radial\"))\n#> $cost\n#> [1] 2.718282\n#> \n#> $kernel\n#> [1] \"radial\"\nsearch_space$trafo(list(cost = 1, kernel = \"polynomial\"))\n#> $cost\n#> [1] 4.718282\n#> \n#> $kernel\n#> [1] \"polynomial\"\n```\n:::\n\n\n### Hyperparameter Dependencies\n\nHyperparameter dependencies occur when a hyperparameter should only be set if another hyperparameter has a particular value. For example, the `degree` parameter in SVM is only valid when `kernel` is `\"polynomial\"`. In the `ps()` function, we specify this using the depends argument, which takes a named argument of the form `<param> == value` or `<param> %in% <vector>`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nps(\n  kernel = p_fct(c(\"polynomial\", \"radial\")),\n  degree = p_int(1, 3, depends = (kernel == \"polynomial\")),\n  gamma = p_dbl(1e-5, 1e5,\n                depends = (kernel %in% c(\"polynomial\", \"radial\")))\n)\n#> <ParamSet>\n#>        id    class lower upper nlevels        default parents value\n#> 1: degree ParamInt 1e+00 3e+00       3 <NoDefault[3]>  kernel      \n#> 2:  gamma ParamDbl 1e-05 1e+05     Inf <NoDefault[3]>  kernel      \n#> 3: kernel ParamFct    NA    NA       2 <NoDefault[3]>\n```\n:::\n\n\nAbove we have said that `degree` should only be set if `kernel` is (`==`) `\"polynomial\"`, and `gamma` should only be set if `kernel` is one of (`%in%`) `\"polynomial\"` or  `\"radial\"`.\nIn practice, some underlying implementations ignore unused parameters and others throw errors, either way, this is problematic during tuning if, for example, we were wasting time trying to tune `degree` when the kernel was not polynomial.\nHence setting the dependency tells the tuning process to tune `degree` if `kernel` is `\"polynomial\"` and to ignore it otherwise.\n\nDependencies can also be passed straight into a learner using `to_tune()`:\n\n> 在上面的示例中，我们说过`degree`只有在`kernel`为(`==`) `\"polynomial\"`时才应设置，而`gamma`只有在`kernel`是(`%in%`) `\"polynomial\"`或`\"radial\"`之一时才应设置。  \n> 实际上，一些底层实现会忽略未使用的参数，而其他一些则会引发错误，无论哪种情况，在调优过程中都会造成问题，例如，当内核不是多项式时，浪费时间尝试调整`degree`。  \n> 因此，设置依赖关系告诉调整过程，如果`kernel`是`\"polynomial\"`，则调整`degree`，否则忽略它。\n>\n> 依赖关系也可以直接传递给学习器，使用 `to_tune()`：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn(\n  \"classif.svm\",\n  kernel = to_tune(c(\"polynomial\", \"radial\")),\n  degree = to_tune(p_int(1, 3, depends = (kernel == \"polynomial\")))\n)$param_set$search_space()\n#> <ParamSet>\n#>        id    class lower upper nlevels        default       parents value\n#> 1: degree ParamInt     1     3       3 <NoDefault[3]> kernel,kernel      \n#> 2: kernel ParamFct    NA    NA       2 <NoDefault[3]>\n```\n:::\n\n\n### Recommended Search Spaces with `mlrtuningspaces`\n\nSelected search spaces can require a lot of background knowledge or expertise. The package `mlr3tuningspaces` tries to make HPO more accessible by providing implementations of published search spaces for many popular machine learning algorithms, the hope is that these search spaces are applicable to a wide range of datasets. The search spaces are stored in the dictionary `mlr_tuning_spaces`.\n\n> 所选的搜索空间可能需要大量的背景知识或专业知识。包`mlr3tuningspaces`试图通过提供许多流行的机器学习算法的已发表搜索空间的实现来使HPO更加可访问，希望这些搜索空间适用于各种各样的数据集。这些搜索空间存储在`mlr_tuning_spaces`字典中。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3tuningspaces)\nas.data.table(mlr_tuning_spaces)[1:3, .(key, label)]\n#>                       key                             label\n#> 1: classif.glmnet.default   Classification GLM with Default\n#> 2:    classif.glmnet.rbv1 Classification GLM with RandomBot\n#> 3:    classif.glmnet.rbv2 Classification GLM with RandomBot\n```\n:::\n\n\nThe tuning spaces are named according to the scheme `{learner-id}.{tuning-space-id}`. The `default` tuning spaces are published in Bischl et al. (2023), other tuning spaces are part of the random bot experiments `rbv1` and `rbv2` published in Kuehn et al. (2018) and Binder, Pfisterer, and Bischl (2020). The sugar function `lts()` (learner tuning space) is used to retrieve a `TuningSpace.`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlts_rpart = lts(\"classif.rpart.default\")\nlts_rpart\n#> <TuningSpace:classif.rpart.default>: Classification Rpart with Default\n#>           id lower upper levels logscale\n#> 1:  minsplit 2e+00 128.0            TRUE\n#> 2: minbucket 1e+00  64.0            TRUE\n#> 3:        cp 1e-04   0.1            TRUE\n```\n:::\n\n\nA tuning space can be passed to `ti()` or `auto_tuner()` as the `search_space.`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = ti(\n  task = tsk_sonar,\n  learner = lrn(\"classif.rpart\"),\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"classif.ce\"),\n  terminator = trm(\"evals\", n_evals = 20),\n  search_space = lts_rpart\n)\n```\n:::\n\n\nAlternatively, as loaded search spaces are just a collection of tune tokens, we could also pass these straight to a learner:\n\n> 或者，由于加载的搜索空间只是一组调整令牌，我们还可以将它们直接传递给学习器：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvals = lts_rpart$values\nvals\n#> $minsplit\n#> Tuning over:\n#> range [2, 128] (log scale)\n#> \n#> \n#> $minbucket\n#> Tuning over:\n#> range [1, 64] (log scale)\n#> \n#> \n#> $cp\n#> Tuning over:\n#> range [1e-04, 0.1] (log scale)\nlearner = lrn(\"classif.rpart\")\nlearner$param_set$set_values(.values = vals)\nlearner$param_set\n#> <ParamSet>\n#>                 id    class lower upper nlevels        default\n#>  1:             cp ParamDbl     0     1     Inf           0.01\n#>  2:     keep_model ParamLgl    NA    NA       2          FALSE\n#>  3:     maxcompete ParamInt     0   Inf     Inf              4\n#>  4:       maxdepth ParamInt     1    30      30             30\n#>  5:   maxsurrogate ParamInt     0   Inf     Inf              5\n#>  6:      minbucket ParamInt     1   Inf     Inf <NoDefault[3]>\n#>  7:       minsplit ParamInt     1   Inf     Inf             20\n#>  8: surrogatestyle ParamInt     0     1       2              0\n#>  9:   usesurrogate ParamInt     0     2       3              2\n#> 10:           xval ParamInt     0   Inf     Inf             10\n#>                   value\n#>  1: <RangeTuneToken[2]>\n#>  2:                    \n#>  3:                    \n#>  4:                    \n#>  5:                    \n#>  6: <RangeTuneToken[2]>\n#>  7: <RangeTuneToken[2]>\n#>  8:                    \n#>  9:                    \n#> 10:                   0\n```\n:::\n\n\nWe could also apply the default search spaces from Bischl et al. (2023) by passing the learner to `lts()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlts(lrn(\"classif.rpart\"))\n#> <LearnerClassifRpart:classif.rpart>: Classification Tree\n#> * Model: -\n#> * Parameters: xval=0, minsplit=<RangeTuneToken>,\n#>   minbucket=<RangeTuneToken>, cp=<RangeTuneToken>\n#> * Packages: mlr3, rpart\n#> * Predict Types:  [response], prob\n#> * Feature Types: logical, integer, numeric, factor, ordered\n#> * Properties: importance, missings, multiclass, selected_features,\n#>   twoclass, weights\n```\n:::\n\n\nFinally, it is possible to overwrite a predefined tuning space in construction, for example, changing the range of the `maxdepth` hyperparameter in a decision tree:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlts(\"classif.rpart.rbv2\", maxdepth = to_tune(1, 20))\n#> <TuningSpace:classif.rpart.rbv2>: Classification Rpart with RandomBot\n#>           id lower upper levels logscale\n#> 1:        cp 1e-04     1            TRUE\n#> 2:  maxdepth 1e+00    20           FALSE\n#> 3: minbucket 1e+00   100           FALSE\n#> 4:  minsplit 1e+00   100           FALSE\n```\n:::\n\n\n# Advanced Tuning Methods and Black Box Optimization {#sec-optimization-advanced}\n\n## Error Handling and Memory Management\n\n### Encapsulation and Fallback Learner\n\nEven in simple machine learning problems, there is a lot of potential for things to go wrong. For example, when learners do not converge, run out of memory, or terminate with an error due to issues in the underlying data. As a common issue, learners can fail if there are factor levels present in the test data that were not in the training data, models fail in this case as there have been no weights/coefficients trained for these new factor levels:\n\n> 即使在简单的机器学习问题中，出现问题的可能性也很大。例如，当学习器不收敛、耗尽内存或由于底层数据问题而出现错误终止时。作为一个常见问题，如果测试数据中存在训练数据中没有的因子水平，那么学习器可能会失败，因为针对这些新的因子水平没有进行权重/系数的训练：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_pen = tsk(\"penguins\")\n\n# remove rows with missing values\ntsk_pen$filter(tsk_pen$row_ids[complete.cases(tsk_pen$data())])\n\nrsmp_custom = rsmp(\"custom\")\nrsmp_custom$instantiate(\n  tsk_pen,\n  train_sets = list(tsk_pen$row_ids[tsk_pen$data()$island != \"Torgersen\"]),\n  test_sets = list(tsk_pen$row_ids[tsk_pen$data()$island == \"Torgersen\"])\n)\n\nmsr_ce = msr(\"classif.ce\")\ntnr_random = tnr(\"random_search\")\nlearner = lrn(\"classif.lda\", method = \"t\", nu = to_tune(3, 10))\n\ntune(tnr_random, tsk_pen, learner, rsmp_custom, msr_ce, 10)\n#> INFO  [16:15:37.841] [bbotk] Starting to optimize 1 parameter(s) with '<OptimizerRandomSearch>' and '<TerminatorEvals> [n_evals=10, k=0]'\n#> INFO  [16:15:37.853] [bbotk] Evaluating 1 configuration(s)\n#> INFO  [16:15:37.887] [mlr3] Running benchmark with 1 resampling iterations\n#> INFO  [16:15:37.897] [mlr3] Applying learner 'classif.lda' on task 'penguins' (iter 1/1)\n#> Error in lda.default(x, grouping, ...): variable 6 appears to be constant within groups\n```\n:::\n\n\n::: {.callout-caution}\nTODO：等待后续添加交叉引用  10.2.1\n:::\n\nIn the above example, we can see the tuning process breaks and we lose all information about the hyperparameter optimization process. This is even worse in nested resampling or benchmarking when errors could cause us to lose all progress across multiple configurations or even learners and tasks.\n\nEncapsulation (Section 10.2.1) allows errors to be isolated and handled, without disrupting the tuning process. We can tell a learner to encapsulate an error by setting the `$encapsulate` field as follows:\n\n> 在上述示例中，我们可以看到调优过程中断，我们失去了有关超参数优化过程的所有信息。在嵌套重抽样或基准测试中，当错误可能导致我们失去跨多个配置甚至学习器和任务的所有进展时，情况会变得更糟。\n>\n> 封装（第10.2.1节）允许隔离和处理错误，而不会干扰调优过程。我们可以通过设置`$encapsulate`字段来告诉学习器封装错误，如下所示：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner$encapsulate = c(train = \"evaluate\", predict = \"evaluate\")\n```\n:::\n\n\nNote by passing `\"evaluate\"` to both `train` and `predict`, we are telling the learner to set up encapsulation in both the training and prediction stages (see Section 10.2 for other encapsulation options).\n\nAnother common issue that cannot be easily solved during HPO is learners not converging and the process running indefinitely. We can prevent this from happening by setting the `timeout` field in a learner, which signals the learner to stop if it has been running for that much time (in seconds), again this can be set for training and prediction individually:\n\n> 请注意，通过在`train`和`predict`中都传递`\"evaluate\"`，我们告诉学习器在训练和预测阶段都设置封装（有关其他封装选项，请参见第10.2节）。\n>\n> 另一个在HPO期间难以轻松解决的常见问题是学习器不收敛，进程无限运行。我们可以通过在学习器中设置`timeout`字段来防止这种情况发生，该字段表示如果学习器运行了这么长时间（以秒为单位），则应停止运行。同样，这可以分别为训练和预测设置：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner$timeout = c(train = 30, predict = 30)\n```\n:::\n\n\nNow if either an error occurs, or the model timeout threshold is reached, then instead of breaking, the learner will simply not make predictions when errors are found and the result is `NA` for resampling iterations with errors. When this happens, our hyperparameter optimization experiment will fail as we cannot aggregate results across resampling iterations. Therefore it is essential to select a fallback learner (Section 10.2.2), which is a learner that will be fitted if the learner of interest fails.\n\nA common approach is to use a featureless baseline (`lrn(\"regr.featureless\"`) or `lrn(\"classif.featureless\"))`. Below we set `lrn(\"classif.featureless\")`, which always predicts the majority class, by passing this learner to the `$fallback` field.\n\n> 如果出现错误或达到模型超时阈值，那么学习器将不会中断，而是在发现错误时不进行预测，对于出现错误的重抽样迭代，结果将是`NA`。当发生这种情况时，我们的超参数优化实验将失败，因为我们无法在重抽样迭代之间聚合结果。因此，选择一个回退学习器（第10.2.2节）非常重要，这是一种在感兴趣的学习器失败时将要训练的备用学习器。\n>\n> 一个常见的方法是使用一个没有特征的基线学习器（`lrn(\"regr.featureless\"`或`lrn(\"classif.featureless\")`）。下面我们设置了`lrn(\"classif.featureless\")`，它总是预测多数类别，通过将这个学习器传递给`$fallback`字段来实现。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner$fallback = lrn(\"classif.featureless\")\n```\n:::\n\n\nWe can now run our experiment and see errors that occurred during tuning in the archive.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = tune(tnr_random, tsk_pen, learner, rsmp_custom, msr_ce, 10)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(instance$archive)[1:3, .(df, classif.ce, errors)]\n#>               df classif.ce errors\n#> 1: <function[1]>          1      1\n#> 2: <function[1]>          1      1\n#> 3: <function[1]>          1      1\n\n# reading the error in the first resample result\ninstance$archive$resample_result(1)$errors\n#>    iteration                                             msg\n#> 1:         1 variable 6 appears to be constant within groups\n```\n:::\n\n\nThe learner was tuned without breaking because the errors were encapsulated and logged before the fallback learners were used for fitting and predicting:\n\n> 由于错误被封装并在使用回退学习器进行拟合和预测之前进行了记录，学习器在没有中断的情况下进行了调优：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance$result\n#>    nu learner_param_vals  x_domain classif.ce\n#> 1:  8          <list[2]> <list[1]>          1\n```\n:::\n\n\n### Memory Management\n\nRunning a large tuning experiment can use a lot of memory, especially when using nested resampling. Most of the memory is consumed by the models since each resampling iteration creates one new model. Storing the models is therefore disabled by default and in most cases is not required. The option `store_models` in the functions `ti()` and `auto_tuner()` allows us to enable the storage of the models.\n\nThe archive stores a `ResampleResult` for each evaluated hyperparameter configuration. The contained `Prediction` objects can also take up a lot of memory, especially with large datasets and many resampling iterations. We can disable the storage of the resample results by setting `store_benchmark_result = FALSE` in the functions `ti()` and `auto_tuner()`. Note that without the resample results, it is no longer possible to score the configurations with another measure.\n\nWhen we run nested resampling with many outer resampling iterations, additional memory can be saved if we set `store_tuning_instance = FALSE` in the `auto_tuner()` function. However, the functions `extract_inner_tuning_results()` and `extract_inner_tuning_archives()` will then no longer work.\n\nThe option `store_models = TRUE` sets `store_benchmark_result` and `store_tuning_instance` to `TRUE` because the models are stored in the benchmark results which in turn is part of the instance. This also means that `store_benchmark_result = TRUE` sets `store_tuning_instance` to `TRUE.`\n\nFinally, we can set `store_models = FALSE` in the `resample()` or `benchmark()` functions to disable the storage of the auto tuners when running nested resampling. This way we can still access the aggregated performance (`rr$aggregate()`) but lose information about the inner resampling.\n\n> 运行大型调优实验可能会使用大量内存，特别是在使用嵌套重抽样时。大多数内存被模型消耗，因为每个重抽样迭代都会创建一个新模型。默认情况下禁用存储模型，而在大多数情况下也不需要存储模型。在函数`ti()`和`auto_tuner()`中，选项`store_models`允许我们启用模型的存储。\n>\n> 归档存储了每个评估的超参数配置的`ResampleResult`。包含的`Prediction`对象在大型数据集和许多重抽样迭代时可能占用大量内存。我们可以通过在函数`ti()`和`auto_tuner()`中设置`store_benchmark_result = FALSE`来禁用重抽样结果的存储。请注意，如果没有重抽样结果，就不再可能使用另一个度量来评分配置。\n>\n> 当我们运行具有许多外部重抽样迭代的嵌套重抽样时，如果在`auto_tuner()`函数中设置`store_tuning_instance = FALSE`，还可以节省额外的内存。然而，`extract_inner_tuning_results()`和`extract_inner_tuning_archives()`函数将不再起作用。\n>\n> 选项`store_models = TRUE`会将`store_benchmark_result`和`store_tuning_instance`设置为`TRUE`，因为模型存储在基准结果中，而基准结果又是实例的一部分。这也意味着`store_benchmark_result = TRUE`会将`store_tuning_instance`设置为`TRUE`。\n>\n> 最后，在运行嵌套重抽样时，可以在`resample()`或`benchmark()`函数中设置`store_models = FALSE`以禁用自动调整器的存储。这样我们仍然可以访问聚合性能（`rr$aggregate()`），但会失去有关内部重抽样的信息。\n\n## Multi-Objective Tuning\n\nSo far we have considered optimizing a model with respect to one metric, but multi-criteria, or multi-objective optimization, is also possible. A simple example of multi-objective optimization might be optimizing a classifier to simultaneously maximize true positive predictions and minimize false negative predictions. In another example, consider the single-objective problem of tuning a neural network to minimize classification error. The best-performing model is likely to be quite complex, possibly with many layers that will have drawbacks like being harder to deploy on devices with limited resources. In this case, we might want to simultaneously minimize the classification error and model complexity.\n\nBy definition, optimization of multiple metrics means these will be in competition (otherwise we would only optimize one of them) and therefore in general no single configuration exists that optimizes all metrics. Therefore, we instead focus on the concept of Pareto optimality. One hyperparameter configuration is said to Pareto-dominate another if the resulting model is equal or better in all metrics and strictly better in at least one metric.\n\nThe goal of multi-objective hyperparameter optimization is to find a set of non-dominated solutions so that their corresponding metric values approximate the Pareto front.\n\n> 到目前为止，我们考虑了根据一个度量来优化模型，但多标准或多目标优化也是可能的。多目标优化的一个简单示例可能是优化分类器，同时最大化真正例预测和最小化假负例预测。在另一个示例中，考虑单一目标问题，即调整神经网络以最小化分类错误。性能最佳的模型可能相当复杂，可能具有许多层，具有诸如在资源有限的设备上部署更困难等缺点。在这种情况下，我们可能希望同时最小化分类错误和模型复杂性。\n>\n> 根据定义，多个度量的优化意味着它们将竞争（否则我们只会优化其中一个），因此通常不存在单个配置可以优化所有度量。因此，我们转而关注帕累托最优的概念。如果得到的模型在所有度量上相等或更好，且至少在一个度量上严格更好，则一个超参数配置被认为帕累托优于另一个。\n>\n> 多目标超参数优化的目标是找到一组非支配解，以便它们对应的度量值近似于帕累托前沿。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.rpart\", cp = to_tune(1e-04, 1e-1),\n              minsplit = to_tune(2, 64), maxdepth = to_tune(1, 30))\n\nmeasures = msrs(c(\"classif.ce\", \"selected_features\"))\n```\n:::\n\n\nAs we are tuning with respect to multiple measures, the function `ti()` automatically creates a `TuningInstanceMultiCrit` instead of a `TuningInstanceSingleCrit.` Below we set `store_models = TRUE` as this is required by the selected features measure.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = ti(\n  task = tsk(\"sonar\"),\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = measures,\n  terminator = trm(\"evals\", n_evals = 30),\n  store_models = TRUE\n)\ninstance\n#> <TuningInstanceMultiCrit>\n#> * State:  Not optimized\n#> * Objective: <ObjectiveTuning:classif.rpart_on_sonar>\n#> * Search Space:\n#>          id    class lower upper nlevels\n#> 1:       cp ParamDbl 1e-04   0.1     Inf\n#> 2: minsplit ParamInt 2e+00  64.0      63\n#> 3: maxdepth ParamInt 1e+00  30.0      30\n#> * Terminator: <TerminatorEvals>\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntuner = tnr(\"random_search\")\ntuner$optimize(instance)\n```\n:::\n\n\nFinally, we inspect the best-performing configurations, i.e., the Pareto set. Note that the `selected_features` measure is averaged across the folds, so the values in the archive may not always be integers.\n\n> 最后，我们检查性能最佳的配置，即帕累托集。请注意，所选择的特征度量是在交叉验证折叠上进行平均的，因此归档中的值可能不总是整数。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance$archive$best()[, .(cp, minsplit, maxdepth, classif.ce, selected_features)]\n#>            cp minsplit maxdepth classif.ce selected_features\n#> 1: 0.06225493       30        7  0.2645273                 3\n#> 2: 0.01311655       59        1  0.2792271                 1\n#> 3: 0.06088867       40        1  0.2792271                 1\n```\n:::\n\n\n## Multi-Fidelity Tuning via Hyperband {#sec-hyperband}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.callout-tip title=\"To be continued\"}\n- <https://mlr3book.mlr-org.com/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#sec-hyperband>\n:::\n\n等待交叉引用：\n\n- 4.4（待检查）\n- 5（待检查）\n- 5.3 5.4\n- 10.1.3\n- 10.2.1\n- 11.3\n- 13.1\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}