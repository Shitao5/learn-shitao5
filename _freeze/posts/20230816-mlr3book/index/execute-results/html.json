{
  "hash": "e67a39d7e654b80d66041ab8377d6e71",
  "result": {
    "markdown": "---\ntitle: \"Applied Machine Learning Using mlr3 in R\"\ndate: \"2023-08-16\"\ndate-modified: \"2023-09-26\"\nimage: \"logo.png\"\ncategories: \n  - Machine Learning\n  - R\n  - mlr3\n---\n\n\n\n\n::: {.callout-note title='Progress'}\nLearning Progress: 24%.\n:::\n\n::: {.callout-tip title=\"Learning Source\"}\n- <https://mlr3book.mlr-org.com/>\n- 中文翻译由 ChatGPT 3.5 提供\n:::\n\n# Getting Started {.unnumbered}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\nlibrary(mlr3pipelines)\nlibrary(mlr3benchmark)\nlibrary(ggplot2)\n```\n:::\n\n\n\n\n# Introduction and Overview\n\n`mlr3` by Example:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\n\ntask = tsk(\"penguins\")\nsplit = partition(task)\nlearner = lrn(\"classif.rpart\")\n\nlearner$train(task, row_ids = split$train)\nlearner$model\n#> n= 231 \n#> \n#> node), split, n, loss, yval, (yprob)\n#>       * denotes terminal node\n#> \n#> 1) root 231 129 Adelie (0.441558442 0.199134199 0.359307359)  \n#>   2) flipper_length< 206.5 144  44 Adelie (0.694444444 0.298611111 0.006944444)  \n#>     4) bill_length< 43.05 98   3 Adelie (0.969387755 0.030612245 0.000000000) *\n#>     5) bill_length>=43.05 46   6 Chinstrap (0.108695652 0.869565217 0.021739130) *\n#>   3) flipper_length>=206.5 87   5 Gentoo (0.022988506 0.034482759 0.942528736) *\n\nprediction = learner$predict(task, row_ids = split$test)\nprediction\n#> <PredictionClassif> for 113 observations:\n#>     row_ids     truth  response\n#>           1    Adelie    Adelie\n#>           2    Adelie    Adelie\n#>           3    Adelie    Adelie\n#> ---                            \n#>         328 Chinstrap Chinstrap\n#>         331 Chinstrap    Adelie\n#>         339 Chinstrap Chinstrap\n\nprediction$score(msr(\"classif.acc\"))\n#> classif.acc \n#>   0.9557522\n```\n:::\n\n\nThe `mlr3` interface also lets you run more complicated experiments in just a few lines of code:\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\nWe use dictionaries to group large collections of relevant objects so they can be listed and retrieved easily.\nFor example, you can see an overview of available learners (that are in loaded packages) and their properties with `as.data.table(mlr_learners)` or by calling the sugar function without any arguments, e.g. `lrn()`.\n\n> 我们使用字典来分组大量相关对象，以便可以轻松地列出和检索它们。例如，您可以通过 `as.data.table(mlr_learners)` 查看可用学习器（位于加载的包中）及其属性的概述，或者通过调用糖函数而不带任何参数，例如 `lrn()`。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(mlr_learners)[1:3]\n#>                    key                              label task_type\n#> 1:   classif.cv_glmnet                               <NA>   classif\n#> 2:       classif.debug   Debug Learner for Classification   classif\n#> 3: classif.featureless Featureless Classification Learner   classif\n#>                                           feature_types\n#> 1:                              logical,integer,numeric\n#> 2:     logical,integer,numeric,character,factor,ordered\n#> 3: logical,integer,numeric,character,factor,ordered,...\n#>                    packages\n#> 1: mlr3,mlr3learners,glmnet\n#> 2:                     mlr3\n#> 3:                     mlr3\n#>                                                               properties\n#> 1:                         multiclass,selected_features,twoclass,weights\n#> 2:                         hotstart_forward,missings,multiclass,twoclass\n#> 3: featureless,importance,missings,multiclass,selected_features,twoclass\n#>    predict_types\n#> 1: response,prob\n#> 2: response,prob\n#> 3: response,prob\n```\n:::\n\n\n# Fundamentals {.unnumbered}\n\n# Data and Basic Modeling\n\n## Tasks\n\n### Constructing Tasks\n\n`mlr3` includes a few predefined machine learning tasks in the `mlr_tasks` Dictionary.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlr_tasks\n#> <DictionaryTask> with 21 stored values\n#> Keys: ames_housing, bike_sharing, boston_housing, breast_cancer,\n#>   german_credit, ilpd, iris, kc_housing, moneyball, mtcars, optdigits,\n#>   penguins, penguins_simple, pima, ruspini, sonar, spam, titanic,\n#>   usarrests, wine, zoo\n# the same as \n# tsk()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_mtcars = tsk(\"mtcars\")\ntsk_mtcars\n#> <TaskRegr:mtcars> (32 x 11): Motor Trends\n#> * Target: mpg\n#> * Properties: -\n#> * Features (10):\n#>   - dbl (10): am, carb, cyl, disp, drat, gear, hp, qsec, vs, wt\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# create my own regression task\ndata(\"mtcars\", package = \"datasets\")\nmtcars_subset = subset(mtcars, select = c(\"mpg\", \"cyl\", \"disp\"))\ntsk_mtcars = as_task_regr(mtcars_subset, target = \"mpg\", id = \"cars\")\ntsk_mtcars\n#> <TaskRegr:cars> (32 x 3)\n#> * Target: mpg\n#> * Properties: -\n#> * Features (2):\n#>   - dbl (2): cyl, disp\n```\n:::\n\n\nThe `id` argument is optional and specifies an identifier for the task that is used in plots and summaries; if omitted the variable name of the data will be used as the `id`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3viz)\nautoplot(tsk_mtcars, type = \"pairs\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n### Retrieving Data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nc(tsk_mtcars$nrow, tsk_mtcars$ncol)\n#> [1] 32  3\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nc(Features = tsk_mtcars$feature_names,\n  Target = tsk_mtcars$target_names)\n#> Features1 Features2    Target \n#>     \"cyl\"    \"disp\"     \"mpg\"\n```\n:::\n\n\nRow IDs are not used as features when training or predicting but are metadata that allow access to individual observations. Note that row IDs are not the same as row numbers.\n\nThis design decision allows tasks and learners to transparently operate on real database management systems, where primary keys are required to be unique, but not necessarily consecutive.\n\n> 行ID在训练或预测时不作为特征使用，而是元数据，用于访问个别观测数据。需要注意的是，行ID与行号不同。\n>\n> 这种设计决策使得任务和学习器能够透明地在真实的数据库管理系统上运行，其中要求主键是唯一的，但不一定连续。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = as_task_regr(data.frame(x = runif(5), y = runif(5)),\n                    target = \"y\")\ntask$row_ids\n#> [1] 1 2 3 4 5\n\ntask$filter(c(4, 1, 3))\ntask$row_ids\n#> [1] 1 3 4\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_mtcars$data()[1:3]\n#>     mpg cyl disp\n#> 1: 21.0   6  160\n#> 2: 21.0   6  160\n#> 3: 22.8   4  108\ntsk_mtcars$data(rows = c(1, 5, 10), cols = tsk_mtcars$feature_names)\n#>    cyl  disp\n#> 1:   6 160.0\n#> 2:   8 360.0\n#> 3:   6 167.6\n```\n:::\n\n\n### Task Mutators\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_mtcars_small = tsk(\"mtcars\")\ntsk_mtcars_small$select(\"cyl\")\ntsk_mtcars_small$filter(2:3)\ntsk_mtcars_small$data()\n#>     mpg cyl\n#> 1: 21.0   6\n#> 2: 22.8   4\n```\n:::\n\n\nAs `R6` uses reference semantics, you need to use `$clone()` if you want to modify a task while keeping the original object intact.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_mtcars = tsk(\"mtcars\")\ntsk_mtcars_clone = tsk_mtcars$clone()\ntsk_mtcars_clone$filter(1:2)\ntsk_mtcars_clone$head()\n#>    mpg am carb cyl disp drat gear  hp  qsec vs    wt\n#> 1:  21  1    4   6  160  3.9    4 110 16.46  0 2.620\n#> 2:  21  1    4   6  160  3.9    4 110 17.02  0 2.875\n```\n:::\n\n\nTo add extra rows and columns to a task, you can use `$rbind()` and `$cbind()` respectively:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_mtcars_small\n#> <TaskRegr:mtcars> (2 x 2): Motor Trends\n#> * Target: mpg\n#> * Properties: -\n#> * Features (1):\n#>   - dbl (1): cyl\ntsk_mtcars_small$cbind(data.frame(disp = c(150, 160)))\ntsk_mtcars_small$rbind(data.frame(mpg = 23, cyl = 5, disp = 170))\ntsk_mtcars_small$data()\n#>     mpg cyl disp\n#> 1: 21.0   6  150\n#> 2: 22.8   4  160\n#> 3: 23.0   5  170\n```\n:::\n\n\n## Learners\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# all the learners available in mlr3\nmlr_learners\n#> <DictionaryLearner> with 46 stored values\n#> Keys: classif.cv_glmnet, classif.debug, classif.featureless,\n#>   classif.glmnet, classif.kknn, classif.lda, classif.log_reg,\n#>   classif.multinom, classif.naive_bayes, classif.nnet, classif.qda,\n#>   classif.ranger, classif.rpart, classif.svm, classif.xgboost,\n#>   clust.agnes, clust.ap, clust.cmeans, clust.cobweb, clust.dbscan,\n#>   clust.diana, clust.em, clust.fanny, clust.featureless, clust.ff,\n#>   clust.hclust, clust.kkmeans, clust.kmeans, clust.MBatchKMeans,\n#>   clust.mclust, clust.meanshift, clust.pam, clust.SimpleKMeans,\n#>   clust.xmeans, regr.cv_glmnet, regr.debug, regr.featureless,\n#>   regr.glmnet, regr.kknn, regr.km, regr.lm, regr.nnet, regr.ranger,\n#>   regr.rpart, regr.svm, regr.xgboost\n# lrns()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn(\"regr.rpart\")\n#> <LearnerRegrRpart:regr.rpart>: Regression Tree\n#> * Model: -\n#> * Parameters: xval=0\n#> * Packages: mlr3, rpart\n#> * Predict Types:  [response]\n#> * Feature Types: logical, integer, numeric, factor, ordered\n#> * Properties: importance, missings, selected_features, weights\n```\n:::\n\n\nAll `Learner` objects include the following metadata, which can be seen in the output above:\n\n- `$feature_types`: the type of features the learner can handle.\n\n- `$packages`: the packages required to be installed to use the learner.\n\n- `$properties`: the properties of the learner. For example, the “missings” properties means a model can handle missing data, and “importance” means it can compute the relative importance of each feature.\n\n- `$predict_types`: the types of prediction that the model can make.\n\n- `$param_set`: the set of available hyperparameters.\n\n### Training\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load mtcars task\ntsk_mtcars = tsk(\"mtcars\")\n\n# load a regression tree\nlrn_rpart = lrn(\"regr.rpart\")\n\n# pass the task to the learner via $train()\nlrn_rpart$train(tsk_mtcars)\n```\n:::\n\n\nAfter training, the fitted model is stored in the `$model` field for future inspection and prediction:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_rpart$model\n#> n= 32 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#> 1) root 32 1126.04700 20.09062  \n#>   2) cyl>=5 21  198.47240 16.64762  \n#>     4) hp>=192.5 7   28.82857 13.41429 *\n#>     5) hp< 192.5 14   59.87214 18.26429 *\n#>   3) cyl< 5 11  203.38550 26.66364 *\n\nsplits = partition(tsk_mtcars)\nsplits\n#> $train\n#>  [1]  1  2  3  4  5 21 25 27 32  7 13 15 16 17 22 23 29 31 18 26 28\n#> \n#> $test\n#>  [1]  8  9 10 30  6 11 12 14 24 19 20\n\nlrn_rpart$train(tsk_mtcars, row_ids = splits$train)\n```\n:::\n\n\n### Predicting\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction = lrn_rpart$predict(tsk_mtcars, row_ids = splits$test)\nprediction\n#> <PredictionRegr> for 11 observations:\n#>     row_ids truth response\n#>           8  24.4 24.52000\n#>           9  22.8 24.52000\n#>          10  19.2 24.52000\n#> ---                       \n#>          24  13.3 15.13636\n#>          19  30.4 24.52000\n#>          20  33.9 24.52000\n\nautoplot(prediction)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmtcars_new = data.table(cyl = c(5, 6), disp = c(100, 120),\n  hp = c(100, 150), drat = c(4, 3.9), wt = c(3.8, 4.1),\n  qsec = c(18, 19.5), vs = c(1, 0), am = c(1, 1),\n  gear = c(6, 4), carb = c(3, 5))\nprediction = lrn_rpart$predict_newdata(mtcars_new)\nprediction\n#> <PredictionRegr> for 2 observations:\n#>  row_ids truth response\n#>        1    NA    24.52\n#>        2    NA    24.52\n```\n:::\n\n\n### Hyperparameters\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_rpart$param_set\n#> <ParamSet>\n#>                 id    class lower upper nlevels        default value\n#>  1:             cp ParamDbl     0     1     Inf           0.01      \n#>  2:     keep_model ParamLgl    NA    NA       2          FALSE      \n#>  3:     maxcompete ParamInt     0   Inf     Inf              4      \n#>  4:       maxdepth ParamInt     1    30      30             30      \n#>  5:   maxsurrogate ParamInt     0   Inf     Inf              5      \n#>  6:      minbucket ParamInt     1   Inf     Inf <NoDefault[3]>      \n#>  7:       minsplit ParamInt     1   Inf     Inf             20      \n#>  8: surrogatestyle ParamInt     0     1       2              0      \n#>  9:   usesurrogate ParamInt     0     2       3              2      \n#> 10:           xval ParamInt     0   Inf     Inf             10     0\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# change hyperparameter\nlrn_rpart = lrn(\"regr.rpart\", maxdepth = 1)\n\nlrn_rpart$param_set$values\n#> $xval\n#> [1] 0\n#> \n#> $maxdepth\n#> [1] 1\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# learned regression tree\nlrn_rpart$train(tsk(\"mtcars\"))$model\n#> n= 32 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#> 1) root 32 1126.0470 20.09062  \n#>   2) cyl>=5 21  198.4724 16.64762 *\n#>   3) cyl< 5 11  203.3855 26.66364 *\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# another way to update hyperparameters\nlrn_rpart$param_set$values$maxdepth = 2\nlrn_rpart$param_set$values\n#> $xval\n#> [1] 0\n#> \n#> $maxdepth\n#> [1] 2\n\n# now with depth 2\nlrn_rpart$train(tsk(\"mtcars\"))$model\n#> n= 32 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#> 1) root 32 1126.04700 20.09062  \n#>   2) cyl>=5 21  198.47240 16.64762  \n#>     4) hp>=192.5 7   28.82857 13.41429 *\n#>     5) hp< 192.5 14   59.87214 18.26429 *\n#>   3) cyl< 5 11  203.38550 26.66364 *\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# or with set_values()\nlrn_rpart$param_set$set_values(xval = 2, cp = .5)\nlrn_rpart$param_set$values\n#> $xval\n#> [1] 2\n#> \n#> $maxdepth\n#> [1] 2\n#> \n#> $cp\n#> [1] 0.5\n```\n:::\n\n\n### Baseline Learners\n\nBaselines are useful in model comparison and as fallback learners. For regression, we have implemented the baseline `lrn(\"regr.featureless\")`, which always predicts new values to be the mean (or median, if the `robust` hyperparameter is set to `TRUE`) of the target in the training data:\n\n基线在模型比较和作为备用学习器中非常有用。对于回归问题，我们已经实现了名为 `lrn(\"regr.featureless\")` 的基线，它总是预测新值为训练数据中目标的均值（如果鲁棒性参数设置为 `TRUE`，则为中位数）：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = as_task_regr(data.frame(x = runif(1000), y = rnorm(1000, 2, 1)),\n                    target = \"y\")\nlrn(\"regr.featureless\")$train(task, 1:995)$predict(task, 996:1000)\n#> <PredictionRegr> for 5 observations:\n#>  row_ids    truth response\n#>      996 1.484589 2.034983\n#>      997 3.012537 2.034983\n#>      998 1.964060 2.034983\n#>      999 1.332658 2.034983\n#>     1000 2.923380 2.034983\n```\n:::\n\n\nIt is good practice to test all new models against a baseline, and also to include baselines in experiments with multiple other models. In general, a model that does not outperform a baseline is a ‘bad’ model, on the other hand, a model is not necessarily ‘good’ if it outperforms the baseline.\n\n> 在实践中，对所有新模型进行与基线的测试是一个良好的做法，同时在与多个其他模型进行实验时也要包括基线。通常情况下，如果一个模型无法超越基线，那么它可以被视为是一个不好的模型；另一方面，如果一个模型超越了基线，也不一定就是一个好模型。\n\n## Evaluation\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_rpart = lrn(\"regr.rpart\")\ntsk_mtcars = tsk(\"mtcars\")\nsplits = partition(tsk_mtcars)\nlrn_rpart$train(tsk_mtcars, splits$train)\nprediction = lrn_rpart$predict(tsk_mtcars, splits$test)\n```\n:::\n\n\n### Measures\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(msr())[1:3]\n#>            key                          label task_type          packages\n#> 1:         aic   Akaike Information Criterion      <NA>              mlr3\n#> 2:         bic Bayesian Information Criterion      <NA>              mlr3\n#> 3: classif.acc        Classification Accuracy   classif mlr3,mlr3measures\n#>    predict_type task_properties\n#> 1:         <NA>                \n#> 2:         <NA>                \n#> 3:     response\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmeasure = msr(\"regr.mae\")\nmeasure\n#> <MeasureRegrSimple:regr.mae>: Mean Absolute Error\n#> * Packages: mlr3, mlr3measures\n#> * Range: [0, Inf]\n#> * Minimize: TRUE\n#> * Average: macro\n#> * Parameters: list()\n#> * Properties: -\n#> * Predict type: response\n```\n:::\n\n\n### Scoring Predictions\n\nNote that all task types have default measures that are used if the argument to `$score()` is omitted, for regression this is the mean squared error (`msr(\"regr.mse\")`).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction$score()\n#> regr.mse \n#> 18.44327\nprediction$score(measure)\n#> regr.mae \n#> 3.832168\nprediction$score(msrs(c(\"regr.mse\", \"regr.mae\")))\n#>  regr.mse  regr.mae \n#> 18.443271  3.832168\n```\n:::\n\n\n### Technical Measures\n\n`mlr3` also provides measures that do not quantify the quality of the predictions of a model, but instead provide ‘meta’-information about the model. These include:\n\n- `msr(\"time_train\")`: The time taken to train a model.\n\n- `msr(\"time_predict\")`: The time taken for the model to make predictions.\n\n- `msr(\"time_both\")`: The total time taken to train the model and then make predictions.\n\n- `msr(\"selected_features\")`: The number of features selected by a model, which can only be used if the model has the “selected_features” property.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmeasures = msrs(c(\"time_train\", \"time_predict\", \"time_both\"))\nprediction$score(measures, learner = lrn_rpart)\n#>   time_train time_predict    time_both \n#>            0            0            0\n```\n:::\n\n\nThese can be used after model training and predicting because we automatically store model run times whenever `$train()` and `$predict()` are called, so the measures above are equivalent to:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nc(lrn_rpart$timings, both = sum(lrn_rpart$timings))\n#>   train predict    both \n#>       0       0       0\n```\n:::\n\n\nThe `selected_features` measure calculates how many features were used in the fitted model.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmsr_sf = msr(\"selected_features\")\nmsr_sf\n#> <MeasureSelectedFeatures:selected_features>: Absolute or Relative Frequency of Selected Features\n#> * Packages: mlr3\n#> * Range: [0, Inf]\n#> * Minimize: TRUE\n#> * Average: macro\n#> * Parameters: normalize=FALSE\n#> * Properties: requires_task, requires_learner, requires_model\n#> * Predict type: NA\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# accessed hyperparameters with `$param_set`\nmsr_sf$param_set\n#> <ParamSet>\n#>           id    class lower upper nlevels default value\n#> 1: normalize ParamLgl    NA    NA       2   FALSE FALSE\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmsr_sf$param_set$values$normalize = TRUE\nprediction$score(msr_sf, task = tsk_mtcars, learner = lrn_rpart)\n#> selected_features \n#>               0.1\n```\n:::\n\n\nNote that we passed the task and learner as the measure has the `requires_task` and `requires_learner` properties.\n\n## Our First Regression Experiment\n\nWe have now seen how to train a model, make predictions and score them. What we have not yet attempted is to ascertain if our predictions are any ‘good’. So before look at how the building blocks of `mlr3` extend to classification, we will take a brief pause to put together everything above in a short experiment to assess the quality of our predictions. We will do this by comparing the performance of a featureless regression learner to a decision tree with changed hyperparameters.\n\n> 我们已经了解了如何训练模型、进行预测并对其进行评分。但是，我们尚未尝试确定我们的预测是否“好”。因此，在深入研究 `mlr3` 的构建模块如何扩展到分类之前，我们将简要停顿一下，通过一个简短的实验来评估我们预测的质量。我们将通过比较无特征的回归学习器与更改超参数的决策树的性能来进行评估。\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(349)\ntsk_mtcars = tsk(\"mtcars\")\nsplits = partition(tsk_mtcars)\nlrn_featureless = lrn(\"regr.featureless\")\nlrn_rpart = lrn(\"regr.rpart\", cp = .2, maxdepth = 5)\nmeasures = msrs(c(\"regr.mse\", \"regr.mae\"))\n\n# train learners\nlrn_featureless$train(tsk_mtcars, splits$train)\nlrn_rpart$train(tsk_mtcars, splits$train)\n# make and score predictions\nlrn_featureless$predict(tsk_mtcars, splits$test)$score(measures)\n#>  regr.mse  regr.mae \n#> 26.726772  4.512987\nlrn_rpart$predict(tsk_mtcars, splits$test)$score(measures)\n#> regr.mse regr.mae \n#> 6.932709 2.206494\n```\n:::\n\n\n## Classification\n\n### Our First Classification Experiment\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(349)\ntsk_penguins = tsk(\"penguins\")\nsplits = partition(tsk_penguins)\nlrn_featureless = lrn(\"classif.featureless\")\nlrn_rpart = lrn(\"classif.rpart\", cp = .2, maxdepth = 5)\nmeasure = msr(\"classif.acc\")\n\n# train learners\nlrn_featureless$train(tsk_penguins, splits$train)\nlrn_rpart$train(tsk_penguins, splits$train)\n\n# make and score predictions\nlrn_featureless$predict(tsk_penguins, splits$test)$score(measure)\n#> classif.acc \n#>   0.4424779\nlrn_rpart$predict(tsk_penguins, splits$test)$score(measure)\n#> classif.acc \n#>   0.9469027\n```\n:::\n\n\n### TaskClassif\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(tsks())[task_type == \"classif\"]\n#>                 key                                     label task_type nrow\n#>  1:   breast_cancer                   Wisconsin Breast Cancer   classif  683\n#>  2:   german_credit                             German Credit   classif 1000\n#>  3:            ilpd                 Indian Liver Patient Data   classif  583\n#>  4:            iris                              Iris Flowers   classif  150\n#>  5:       optdigits Optical Recognition of Handwritten Digits   classif 5620\n#>  6:        penguins                           Palmer Penguins   classif  344\n#>  7: penguins_simple                Simplified Palmer Penguins   classif  333\n#>  8:            pima                      Pima Indian Diabetes   classif  768\n#>  9:           sonar                    Sonar: Mines vs. Rocks   classif  208\n#> 10:            spam                         HP Spam Detection   classif 4601\n#> 11:         titanic                                   Titanic   classif 1309\n#> 12:            wine                              Wine Regions   classif  178\n#> 13:             zoo                               Zoo Animals   classif  101\n#>     ncol properties lgl int dbl chr fct ord pxc\n#>  1:   10   twoclass   0   0   0   0   0   9   0\n#>  2:   21   twoclass   0   3   0   0  14   3   0\n#>  3:   11   twoclass   0   4   5   0   1   0   0\n#>  4:    5 multiclass   0   0   4   0   0   0   0\n#>  5:   65   twoclass   0  64   0   0   0   0   0\n#>  6:    8 multiclass   0   3   2   0   2   0   0\n#>  7:   11 multiclass   0   3   7   0   0   0   0\n#>  8:    9   twoclass   0   0   8   0   0   0   0\n#>  9:   61   twoclass   0   0  60   0   0   0   0\n#> 10:   58   twoclass   0   0  57   0   0   0   0\n#> 11:   11   twoclass   0   2   2   3   2   1   0\n#> 12:   14 multiclass   0   2  11   0   0   0   0\n#> 13:   17 multiclass  15   1   0   0   0   0   0\n```\n:::\n\n\nThe `sonar` task is an example of a binary classification problem, as the target can only take two different values, in `mlr3` terminology it has the “twoclass” property:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_sonar = tsk(\"sonar\")\ntsk_sonar\n#> <TaskClassif:sonar> (208 x 61): Sonar: Mines vs. Rocks\n#> * Target: Class\n#> * Properties: twoclass\n#> * Features (60):\n#>   - dbl (60): V1, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V2,\n#>     V20, V21, V22, V23, V24, V25, V26, V27, V28, V29, V3, V30, V31,\n#>     V32, V33, V34, V35, V36, V37, V38, V39, V4, V40, V41, V42, V43,\n#>     V44, V45, V46, V47, V48, V49, V5, V50, V51, V52, V53, V54, V55,\n#>     V56, V57, V58, V59, V6, V60, V7, V8, V9\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_sonar$class_names\n#> [1] \"M\" \"R\"\n```\n:::\n\n\nIn contrast, `tsk(\"penguins\")` is a multiclass problem as there are more than two species of penguins; it has the “multiclass” property:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_penguins = tsk(\"penguins\")\ntsk_penguins$properties\n#> [1] \"multiclass\"\ntsk_penguins$class_names\n#> [1] \"Adelie\"    \"Chinstrap\" \"Gentoo\"\n```\n:::\n\n\nA further difference between these tasks is that binary classification tasks have an extra field called `$positive`, which defines the ‘positive’ class. In binary classification, as there are only two possible class types, by convention one of these is known as the ‘positive’ class, and the other as the ‘negative’ class. It is arbitrary which is which, though often the more ‘important’ (and often smaller) class is set as the positive class. You can set the positive class during or after construction. If no positive class is specified then `mlr3` assumes the first level in the `target` column is the positive class, which can lead to misleading results.\n\n> 这两种任务之间的另一个区别是，二分类任务有一个额外的字段称为 `$positive`，它定义了“正类”（positive class）。在二分类问题中，由于只有两种可能的类别类型，按照惯例，其中一种被称为“正类”，另一种被称为“负类”。哪个是哪个是任意的，尽管通常更“重要”（通常更小）的类别被设置为正类。您可以在构建期间或之后设置正类。如果未指定正类，则 `mlr3` 假定目标列中的第一个级别是正类，这可能导致误导性的结果。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nSonar = tsk_sonar$data()\ntsk_classif = as_task_classif(Sonar, target = \"Class\", positive = \"R\")\ntsk_classif$positive\n#> [1] \"R\"\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# changing after construction\ntsk_classif$positive = \"M\"\ntsk_classif$positive\n#> [1] \"M\"\n```\n:::\n\n\n### LearnerClassif and MeasureClassif\n\nClassification learners, which inherit from `LearnerClassif`, have nearly the same interface as regression learners. However, a key difference is that the possible predictions in classification are either `\"response\"` – predicting an observation’s class (a penguin’s species in our example, this is sometimes called “hard labeling”) – or `\"prob\"` – predicting a vector of probabilities, also called “posterior probabilities”, of an observation belonging to each class. In classification, the latter can be more useful as it provides information about the confidence of the predictions:\n\n> 分类学习器（继承自 `LearnerClassif`）几乎具有与回归学习器相同的接口。然而，分类中的一个关键区别是，分类问题中可能的预测结果要么是 `\"response\"` （预测观测的类别，例如我们示例中的企鹅物种，有时称为“硬标签”），要么是 `\"prob\"` （预测属于每个类别的概率向量，也称为“后验概率”）。在分类中，后者可能更有用，因为它提供了有关预测的置信度信息：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_rpart = lrn(\"classif.rpart\", predict_type = \"prob\")\nlrn_rpart$train(tsk_penguins, splits$train)\nprediction = lrn_rpart$predict(tsk_penguins, splits$test)\nprediction\n#> <PredictionClassif> for 113 observations:\n#>     row_ids     truth  response prob.Adelie prob.Chinstrap prob.Gentoo\n#>           2    Adelie    Adelie  0.97029703     0.02970297  0.00000000\n#>           4    Adelie    Adelie  0.97029703     0.02970297  0.00000000\n#>           7    Adelie    Adelie  0.97029703     0.02970297  0.00000000\n#> ---                                                                   \n#>         338 Chinstrap Chinstrap  0.04651163     0.93023256  0.02325581\n#>         341 Chinstrap    Adelie  0.97029703     0.02970297  0.00000000\n#>         344 Chinstrap Chinstrap  0.04651163     0.93023256  0.02325581\n```\n:::\n\n\nAlso, the interface for classification measures, which are of class `MeasureClassif`, is identical to regression measures. The key difference in usage is that you will need to ensure your selected measure evaluates the prediction type of interest. To evaluate \"response\" predictions, you will need measures with `predict_type = \"response\"`, or to evaluate probability predictions you will need `predict_type = \"prob\"`. The easiest way to find these measures is by filtering the `mlr_measures` dictionary:\n\n> 此外，分类度量标准的接口，其类别为 `MeasureClassif`，与回归度量标准完全相同。在使用上的主要区别在于，您需要确保所选的度量标准评估感兴趣的预测类型。要评估 `“response”` 预测，您需要使用 `predict_type = \"response\"` 的度量标准，或者要评估概率预测，您需要使用 `predict_type = \"prob\"` 的度量标准。查找这些度量标准的最简单方法是通过筛选 `mlr_measures` 字典：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(msr())[\n  task_type == \"classif\" & predict_type == \"prob\" &\n  !sapply(task_properties, \\(x) \"twoclass\" %in% x)\n]\n#>                  key                                      label task_type\n#> 1:   classif.logloss                                   Log Loss   classif\n#> 2: classif.mauc_au1p    Weighted average 1 vs. 1 multiclass AUC   classif\n#> 3: classif.mauc_au1u             Average 1 vs. 1 multiclass AUC   classif\n#> 4: classif.mauc_aunp Weighted average 1 vs. rest multiclass AUC   classif\n#> 5: classif.mauc_aunu          Average 1 vs. rest multiclass AUC   classif\n#> 6:    classif.mbrier                     Multiclass Brier Score   classif\n#>             packages predict_type task_properties\n#> 1: mlr3,mlr3measures         prob                \n#> 2: mlr3,mlr3measures         prob                \n#> 3: mlr3,mlr3measures         prob                \n#> 4: mlr3,mlr3measures         prob                \n#> 5: mlr3,mlr3measures         prob                \n#> 6: mlr3,mlr3measures         prob\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmeasures = msrs(c(\"classif.mbrier\", \"classif.logloss\", \"classif.acc\"))\nprediction$score(measures)\n#>  classif.mbrier classif.logloss     classif.acc \n#>       0.1016821       0.2291407       0.9469027\n```\n:::\n\n\n### PredictionClassif, Confusion Matrix, and Thresholding\n\n`PredictionClassif` objects have two important differences from their regression analog. Firstly, the added field `$confusion`, and secondly the added method `$set_threshold()`.\n\n> `PredictionClassif` 对象与其回归模型的预测对象有两个重要的区别。首先是新增的字段 `$confusion`，其次是新增的方法 `$set_threshold()`。\n\n#### Confusion Matrix\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction$confusion\n#>            truth\n#> response    Adelie Chinstrap Gentoo\n#>   Adelie        49         3      0\n#>   Chinstrap      1        18      1\n#>   Gentoo         0         1     40\n```\n:::\n\n\nThe rows in a confusion matrix are the predicted class and the columns are the true class. All off-diagonal entries are incorrectly classified observations, and all diagonal entries are correctly classified. In this case, the classifier does fairly well classifying all penguins, but we could have found that it only classifies the Adelie species well but often conflates Chinstrap and Gentoo, for example.\n\n> 混淆矩阵中的行表示预测的类别，列表示真实的类别。所有非对角线条目都是被错误分类的观测值，而所有对角线条目都是被正确分类的。在这种情况下，分类器在对所有企鹅进行分类时表现得相当不错，但我们也可能发现它只能很好地对 Adelie 物种进行分类，但经常将 Chinstrap 和 Gentoo 混为一谈。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(prediction)\n```\n\n::: {.cell-output-display}\n![Counts of each class label in the ground truth data (left) and predictions (right).](index_files/figure-html/fig-confusion_matrix-1.png){#fig-confusion_matrix fig-align='center' width=70%}\n:::\n:::\n\n\nIn the binary classification case, the top left entry corresponds to true positives, the top right to false positives, the bottom left to false negatives and the bottom right to true negatives. Taking `tsk_sonar` as an example with `M` as the positive class:\n\n> 在二分类情况下，左上角的条目对应于真正例（true positives），右上角对应于假正例（false positives），左下角对应于假负例（false negatives），右下角对应于真负例（true negatives）。以 `tsk_sonar` 为例，`M` 为正类：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsplits = partition(tsk_sonar)\nlrn_rpart$\n  train(tsk_sonar, splits$train)$\n  predict(tsk_sonar, splits$test)$\n  confusion\n#>         truth\n#> response  M  R\n#>        M 27 10\n#>        R 10 22\n```\n:::\n\n\n#### Thresholding\n\n**阈值化**\n\nThis 50% value is known as the threshold and it can be useful to change this threshold if there is class imbalance (when one class is over- or under-represented in a dataset), or if there are different costs associated with classes, or simply if there is a preference to ‘over’-predict one class. As an example, let us take `tsk(\"german_credit\")` in which 700 customers have good credit and 300 have bad. Now we could easily build a model with around “70%” accuracy simply by always predicting a customer will have good credit:\n\n> 这个 50% 的值被称为阈值，如果数据集中存在类别不平衡（即一个类别在数据集中过多或过少出现），或者不同的类别具有不同的成本，或者只是有一种“过度”预测一种类别的倾向，那么更改这个阈值可能会很有用。举个例子，让我们看看 `tsk(\"german_credit\")`，其中有 700 个客户信用良好，300 个客户信用不良。现在，我们可以很容易地构建一个模型，总是预测客户会有良好的信用，从而获得 “70%” 左右的准确性：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask_credit = tsk(\"german_credit\")\nlrn_featureless = lrn(\"classif.featureless\", predict_type = \"prob\")\nsplits = partition(task_credit)\nlrn_featureless$train(task_credit, splits$train)\nprediction = lrn_featureless$predict(task_credit, splits$test)\nprediction$score(msr(\"classif.acc\"))\n#> classif.acc \n#>         0.7\n```\n:::\n\n\n::: {.callout-caution}\nTODO：等待后续添加交叉引用\n:::\n\nWhile this model may appear to have good performance on the surface, in fact, it just ignores all ‘bad’ customers – this can create big problems in this finance example, as well as in healthcare tasks and other settings where false positives cost more than false negatives (see Section 13.1 for cost-sensitive classification).\n\nThresholding allows classes to be selected with a different probability threshold, so instead of predicting that a customer has bad credit if P(good) < 50%, we might predict bad credit if P(good) < 70% – notice how we write this in terms of the positive class, which in this task is ‘good’. Let us see this in practice:\n\n> 虽然这个模型表面上看起来性能不错，但实际上它只是忽略了所有“不良”的客户 - 这在金融示例以及在医疗任务和其他一些情况下可能会带来很大问题，特别是在假阳性的成本高于假阴性的情况下（请参见第13.1节的成本敏感分类）。\n>\n> 阈值化允许使用不同的概率阈值选择类别，因此，与其在P(好) < 50%时预测客户信用不良，我们可以在P(好) < 70%时预测客户信用不良。请注意，我们是根据正类别来表示这一点，而在这个任务中正类别是“好”。让我们看看实际应用中的情况：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction$set_threshold(0.7)\nprediction$score(msr(\"classif.acc\"))\n#> classif.acc \n#>   0.5393939\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_rpart = lrn(\"classif.rpart\", predict_type = \"prob\")\nlrn_rpart$train(task_credit, splits$train)\nprediction = lrn_rpart$predict(task_credit, splits$test)\nprediction$score(msr(\"classif.acc\"))\n#> classif.acc \n#>   0.6939394\nprediction$confusion\n#>         truth\n#> response good bad\n#>     good  194  64\n#>     bad    37  35\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction$set_threshold(0.7)\nprediction$score(msr(\"classif.acc\"))\n#> classif.acc \n#>   0.6878788\nprediction$confusion\n#>         truth\n#> response good bad\n#>     good  181  53\n#>     bad    50  46\n```\n:::\n\n\n# Evaluation and Benchmarking\n\n**Resampling Does Not Avoid Model Overfitting**: \nA common **misunderstanding** is that holdout and other more advanced resampling strategies can prevent model overfitting. In fact, these methods just make overfitting visible as we can separately evaluate train/test performance. Resampling strategies also allow us to make (nearly) unbiased estimations of the generalization error.\n\n> **重采样不能避免模型过拟合**：一个常见的误解是，留出策略和其他更高级的重采样策略可以防止模型过拟合。实际上，这些方法只是使过拟合问题更加显而易见，因为我们可以单独评估训练/测试性能。重采样策略还允许我们对泛化误差进行（几乎）无偏估计。\n\n## Holdout and Scoring\n\nIn practice, one would usually create an intermediate model, which is trained on a subset of the available data and then tested on the remainder of the data. The performance of this intermediate model, obtained by comparing the model predictions to the ground truth, is an estimate of the generalization performance of the final model, which is the model fitted on all data.\n\n> 在实践中，通常会创建一个中间模型，该模型在可用数据的子集上进行训练，然后在剩余的数据上进行测试。通过将模型的预测与真实情况进行比较，中间模型的性能可以作为最终模型的泛化性能的估计。最终模型是在所有可用数据上训练的模型。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_penguins = tsk(\"penguins\")\nsplits = partition(tsk_penguins)\nlrn_rpart = lrn(\"classif.rpart\")\nlrn_rpart$train(tsk_penguins, splits$train)\nprediction = lrn_rpart$predict(tsk_penguins, splits$test)\nprediction$score(msr(\"classif.acc\"))\n#> classif.acc \n#>   0.9380531\n```\n:::\n\n\n## Resampling\n\n### Constructing a Resampling Strategy\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(rsmp())\n#>            key                         label        params iters\n#> 1:   bootstrap                     Bootstrap ratio,repeats    30\n#> 2:      custom                 Custom Splits                  NA\n#> 3:   custom_cv Custom Split Cross-Validation                  NA\n#> 4:          cv              Cross-Validation         folds    10\n#> 5:     holdout                       Holdout         ratio     1\n#> 6:    insample           Insample Resampling                   1\n#> 7:         loo                 Leave-One-Out                  NA\n#> 8: repeated_cv     Repeated Cross-Validation folds,repeats   100\n#> 9: subsampling                   Subsampling ratio,repeats    30\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrsmp(\"holdout\", ratio = .8)\n#> <ResamplingHoldout>: Holdout\n#> * Iterations: 1\n#> * Instantiated: FALSE\n#> * Parameters: ratio=0.8\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# three-fold CV\ncv3 = rsmp(\"cv\", folds = 3)\n# subsampling with 3 repeats and 9/10 ratio\nss390 = rsmp(\"subsampling\", repeats = 3, ratio = .9)\n# 2-repeats 5-fold cv\nrcv25 = rsmp(\"repeated_cv\", repeats = 2, folds = 5)\n```\n:::\n\n\nWhen a `\"Resampling\"` object is constructed, it is simply a definition for how the data splitting process will be performed on the task when running the resampling strategy. However, it is possible to manually instantiate a resampling strategy, i.e., generate all train-test splits, by calling the `$instantiate()` method on a given task.\n\n> 当构建一个 `\"Resampling\"` 对象时，它只是对在运行重采样策略时如何执行数据拆分过程的定义。然而，可以通过在给定任务上调用 `$instantiate()` 方法来手动实例化一个重采样策略，即生成所有的训练-测试拆分。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv3$instantiate(tsk_penguins)\n# first 5 observations in first traininng set\ncv3$train_set(1)[1:5]\n#> [1] 2 4 5 6 8\n# fitst 5 observations in thirt test set\ncv3$test_set(3)[1:5]\n#> [1]  1  9 12 17 20\n```\n:::\n\n\nWhen the aim is to fairly compare multiple learners, best practice dictates that all learners being compared use the same training data to build a model and that they use the same test data to evaluate the model performance. Resampling strategies are instantiated automatically for you when using the `resample()` method. Therefore, manually instantiating resampling strategies is rarely required but might be useful for debugging or digging deeper into a model’s performance.\n\n> 当目标是公平比较多个学习器时，最佳实践要求所有进行比较的学习器都使用相同的训练数据来构建模型，并且它们使用相同的测试数据来评估模型性能。在使用 `resample()` 方法时，重采样策略会自动为您实例化。因此，手动实例化重采样策略很少是必需的，但在调试或深入研究模型性能时可能会有用。\n\n### Resampling Experiments\n\nThe `resample()` function takes a given `Task`, `Learner`, and `Resampling` object to run the given resampling strategy. `resample()` repeatedly fits a model on training sets, makes predictions on the corresponding test sets and stores them in a `ResampleResult` object, which contains all the information needed to estimate the generalization performance.\n\n`resample()` 函数接受给定的任务（`Task`）、学习器（`Learner`）和重采样（`Resampling`）对象，以运行给定的重采样策略。`resample()` 函数会在训练集上反复拟合模型，在相应的测试集上进行预测，并将预测结果存储在 `ResampleResult` 对象中，该对象包含了估算泛化性能所需的所有信息。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr = resample(tsk_penguins, lrn_rpart, cv3)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr\n#> <ResampleResult> with 3 resampling iterations\n#>   task_id    learner_id resampling_id iteration warnings errors\n#>  penguins classif.rpart            cv         1        0      0\n#>  penguins classif.rpart            cv         2        0      0\n#>  penguins classif.rpart            cv         3        0      0\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# calculate the score for each iteration\nacc = rr$score(msr(\"classif.ce\"))\nacc[, .(iteration, classif.ce)]\n#>    iteration classif.ce\n#> 1:         1 0.04347826\n#> 2:         2 0.09565217\n#> 3:         3 0.06140351\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# aggregated score across all resampling iterations\nrr$aggregate(msr(\"classif.ce\"))\n#> classif.ce \n#> 0.06684465\n```\n:::\n\n\nBy default, the majority of measures will aggregate scores using a macro average, which first calculates the measure in each resampling iteration separately, and then averages these scores across all iterations. However, it is also possible to aggregate scores using a micro average, which pools predictions across resampling iterations into one `Prediction` object and then computes the measure on this directly:\n\n> 默认情况下，大多数性能度量会使用宏平均（macro average）来汇总分数，它首先在每个重采样迭代中分别计算度量，然后在所有迭代中对这些分数进行平均。但也可以使用微平均（micro average）来汇总分数，它将重采样迭代中的预测汇总到一个 `Prediction` 对象中，然后直接在该对象上计算度量：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr$aggregate(msr(\"classif.ce\", average = \"micro\"))\n#> classif.ce \n#> 0.06686047\n```\n:::\n\n\n::: {.callout-caution}\nTODO：等待后续添加交叉引用\n\n已加，待检查\n:::\n\nTo visualize the resampling results, you can use the `autoplot.ResampleResult()` function to plot scores across folds as boxplots or histograms (@fig-resamp-viz). Histograms can be useful to visually gauge the variance of the performance results across resampling iterations, whereas boxplots are often used when multiple learners are compared side-by-side (see @sec-benchmarking).\n\n> 要可视化重采样结果，您可以使用 `autoplot.ResampleResult()` 函数绘制跨折叠的分数箱线图或直方图（@fig-resamp-viz）。直方图可以用于直观评估跨重采样迭代的性能结果方差，而箱线图通常用于比较多个学习器并排放置在一起时（请参阅 @sec-benchmarking）。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr = resample(tsk_penguins, lrn_rpart, rsmp(\"cv\", folds = 10))\n```\n:::\n\n::: {#fig-resamp-viz .cell layout-ncol=\"2\" layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(rr, measure = msr(\"classif.acc\"), type = \"boxplot\")\nautoplot(rr, measure = msr(\"classif.acc\"), type = \"histogram\")\n```\n\n::: {.cell-output-display}\n![Boxplot of accuracy scores.](index_files/figure-html/fig-resamp-viz-1.png){#fig-resamp-viz-1 fig-align='center' fig-alt='Left: a boxplot ranging from 0.875 to 1.0 and the interquartile range between 0.925 and 0.7. Right: a histogram with five bars in a roughly normal distribution with mean 0.95, minimum 0.875 and maximum 1.0.' width=70%}\n:::\n\n::: {.cell-output-display}\n![Histogram of accuracy scores.](index_files/figure-html/fig-resamp-viz-2.png){#fig-resamp-viz-2 fig-align='center' fig-alt='Left: a boxplot ranging from 0.875 to 1.0 and the interquartile range between 0.925 and 0.7. Right: a histogram with five bars in a roughly normal distribution with mean 0.95, minimum 0.875 and maximum 1.0.' width=70%}\n:::\n\nBoxplot and Histogram of accuracy scores.\n:::\n\n\n### ResampleResult Objects\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# list of prediction objects\nrrp = rr$predictions()\n# print first two\nrrp[1:2]\n#> [[1]]\n#> <PredictionClassif> for 35 observations:\n#>     row_ids     truth  response\n#>           7    Adelie    Adelie\n#>          20    Adelie Chinstrap\n#>          32    Adelie    Adelie\n#> ---                            \n#>         326 Chinstrap Chinstrap\n#>         330 Chinstrap Chinstrap\n#>         337 Chinstrap Chinstrap\n#> \n#> [[2]]\n#> <PredictionClassif> for 35 observations:\n#>     row_ids     truth  response\n#>           1    Adelie    Adelie\n#>           5    Adelie    Adelie\n#>           9    Adelie    Adelie\n#> ---                            \n#>         334 Chinstrap Chinstrap\n#>         339 Chinstrap Chinstrap\n#>         340 Chinstrap Chinstrap\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# macro averaged performance\nmean(sapply(rrp, \\(x) x$score()))\n#> [1] 0.05823529\n```\n:::\n\n\nBy default, the intermediate models produced at each resampling iteration are discarded after the prediction step to reduce memory consumption of the `ResampleResult` object (only the predictions are required to calculate most performance measures). However, it can sometimes be useful to inspect, compare, or extract information from these intermediate models. We can configure the `resample()` function to keep the fitted intermediate models by setting `store_models = TRUE`. Each model trained in a specific resampling iteration can then be accessed via `$learners[[i]]$model`, where `i` refers to the `i`-th resampling iteration:\n\n> 默认情况下，在进行预测步骤后，每个重新采样迭代产生的中间模型都会被丢弃，以降低 `ResampleResult` 对象的内存消耗（大多数性能指标仅需要预测）。然而，有时候检查、比较或从这些中间模型中提取信息可能是有用的。我们可以通过设置 `store_models = TRUE` 来配置 `resample()` 函数以保留拟合的中间模型。然后，可以通过 `$learners[[i]]$model` 来访问在特定重新采样迭代中训练的每个模型，其中 `i` 指的是第 `i` 个重新采样迭代：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr = resample(tsk_penguins, lrn_rpart, cv3, store_models = TRUE)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# get the model from the first iteration\nrr$learners[[1]]$model\n#> n= 229 \n#> \n#> node), split, n, loss, yval, (yprob)\n#>       * denotes terminal node\n#> \n#> 1) root 229 130 Adelie (0.432314410 0.205240175 0.362445415)  \n#>   2) flipper_length< 206.5 142  45 Adelie (0.683098592 0.309859155 0.007042254)  \n#>     4) bill_length< 44.65 97   3 Adelie (0.969072165 0.030927835 0.000000000) *\n#>     5) bill_length>=44.65 45   4 Chinstrap (0.066666667 0.911111111 0.022222222) *\n#>   3) flipper_length>=206.5 87   5 Gentoo (0.022988506 0.034482759 0.942528736) *\n```\n:::\n\n\nIn this example, we could then inspect the most important variables in each iteration to help us learn more about the respective fitted models:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# print 2nd and 3rd iteration\nlapply(rr$learners[2:3], \\(x) x$model$variable.importance)\n#> [[1]]\n#> flipper_length    bill_length     bill_depth      body_mass         island \n#>       88.52870       88.07438       71.51814       67.04826       55.13690 \n#> \n#> [[2]]\n#>    bill_length flipper_length     bill_depth      body_mass         island \n#>       82.18794       75.92820       66.94285       57.14539       50.29049\n```\n:::\n\n\n## Benchmarking {#sec-benchmarking}\n\n### benchmark()\n\nBenchmark experiments in `mlr3` are conducted with `benchmark()`, which simply runs `resample()` on each task and learner separately, then collects the results. The provided resampling strategy is automatically instantiated on each task to ensure that all learners are compared against the same training and test data.\n\nTo use the `benchmark()` function we first call `benchmark_grid()`, which constructs an exhaustive *design* to describe all combinations of the learners, tasks and resamplings to be used in a benchmark experiment, and instantiates the resampling strategies.\n\n> `mlr3` 中的基准实验是使用 `benchmark()` 函数进行的，该函数简单地在每个任务和学习器上分别运行 `resample()`，然后收集结果。提供的重新采样策略会自动在每个任务上进行实例化，以确保所有学习器都与相同的训练和测试数据进行比较。\n>\n> 要使用 `benchmark()` 函数，我们首先调用 `benchmark_grid()` 函数，该函数构建一个详尽的设计来描述在基准实验中要使用的所有学习器、任务和重新采样的组合，并实例化重新采样策略。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntasks = tsks(c(\"german_credit\", \"sonar\"))\nlearners = lrns(c(\"classif.rpart\", \"classif.ranger\", \"classif.featureless\"),\n                predict_type = \"prob\")\nrsmp_cv5 = rsmp(\"cv\", folds = 5)\n\ndesign = benchmark_grid(tasks, learners, rsmp_cv5)\ndesign\n#>             task             learner resampling\n#> 1: german_credit       classif.rpart         cv\n#> 2: german_credit      classif.ranger         cv\n#> 3: german_credit classif.featureless         cv\n#> 4:         sonar       classif.rpart         cv\n#> 5:         sonar      classif.ranger         cv\n#> 6:         sonar classif.featureless         cv\n```\n:::\n\n\nBy default, `benchmark_grid()` instantiates the resamplings on the tasks, which means that concrete train-test splits are generated. Since this process is stochastic, it is necessary to set a seed **before** calling `benchmark_grid()` to ensure reproducibility of the data splits.\n\n> 在默认情况下，`benchmark_grid()` 会在任务上实例化重新采样，这意味着会生成具体的训练-测试拆分。由于这个过程是随机的，所以在调用 `benchmark_grid()` 之前需要设置一个种子，以确保数据拆分的可重现性。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# pass design to benchmark()\nbmr = benchmark(design)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmr\n#> <BenchmarkResult> of 30 rows with 6 resampling runs\n#>  nr       task_id          learner_id resampling_id iters warnings errors\n#>   1 german_credit       classif.rpart            cv     5        0      0\n#>   2 german_credit      classif.ranger            cv     5        0      0\n#>   3 german_credit classif.featureless            cv     5        0      0\n#>   4         sonar       classif.rpart            cv     5        0      0\n#>   5         sonar      classif.ranger            cv     5        0      0\n#>   6         sonar classif.featureless            cv     5        0      0\n```\n:::\n\n\nAs `benchmark()` is just an extension of `resample()`, we can once again use `$score()`, or `$aggregate()` depending on your use-case, though note that in this case `$score()` will return results over each fold of each learner/task/resampling combination.\n\n> 由于 `benchmark()` 只是 `resample()` 的扩展，因此我们可以再次使用 `$score()` 或 `$aggregate()`，具体取决于您的用例，但请注意，在这种情况下，`$score()` 将返回每个学习器/任务/重新采样组合的每个折叠的结果。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmr$score()[c(1, 7, 13), .(iteration, task_id, learner_id, classif.ce)]\n#>    iteration       task_id          learner_id classif.ce\n#> 1:         1 german_credit       classif.rpart      0.335\n#> 2:         2 german_credit      classif.ranger      0.240\n#> 3:         3 german_credit classif.featureless      0.300\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmr$aggregate()[, .(task_id, learner_id, classif.ce)]\n#>          task_id          learner_id classif.ce\n#> 1: german_credit       classif.rpart  0.2870000\n#> 2: german_credit      classif.ranger  0.2230000\n#> 3: german_credit classif.featureless  0.3000000\n#> 4:         sonar       classif.rpart  0.3026713\n#> 5:         sonar      classif.ranger  0.1921022\n#> 6:         sonar classif.featureless  0.4659698\n```\n:::\n\n\n::: {.callout-caution}\nTODO：等待后续添加交叉引用\n:::\n\nThis would conclude a basic benchmark experiment where you can draw tentative conclusions about model performance, in this case we would possibly conclude that the random forest is the best of all three models on each task. We draw conclusions cautiously here as we have not run any statistical tests or included standard errors of measures, so we cannot definitively say if one model outperforms the other.\n\nAs the results of `$score()` and `$aggregate()` are returned in a `data.table`, you can post-process and analyze the results in any way you want. A common mistake is to average the learner performance across all tasks when the tasks vary significantly. This is a mistake as averaging the performance will miss out important insights into how learners compare on ‘easier’ or more ‘difficult’ predictive problems. A more robust alternative to compare the overall algorithm performance across multiple tasks is to compute the ranks of each learner on each task separately and then calculate the average ranks. This can provide a better comparison as task-specific ‘quirks’ are taken into account by comparing learners within tasks before comparing them across tasks. However, using ranks will lose information about the numerical differences between the calculated performance scores. Analysis of benchmark experiments, including statistical tests, is covered in more detail in Section 11.3.\n\n> 这将总结了一个基本的基准实验，您可以初步得出关于模型性能的结论，在这种情况下，我们可能会得出结论，随机森林在每个任务上都是三个模型中最好的。我们在这里谨慎地得出结论，因为我们没有进行任何统计测试，也没有包括性能度量的标准错误，因此我们不能明确地说一个模型是否优于另一个。\n>\n> 由于 `$score()` 和 `$aggregate()` 的结果以 `data.table` 返回，您可以以任何您想要的方式进行后处理和分析结果。一个常见的错误是在任务差异明显的情况下，对所有任务的学习器性能进行平均。这是一个错误，因为对性能进行平均将错过对学习器在“更容易”或“更困难”的预测问题上的比较重要的洞察。比较多个任务上的整体算法性能的更强大的替代方法是分别计算每个任务上每个学习器的排名，然后计算平均排名。这可以提供更好的比较，因为通过在比较任务之前在任务内部比较学习器，可以考虑到特定于任务的“怪癖”。然而，使用排名会丢失关于计算的性能分数之间的数值差异的信息。关于基准实验的分析，包括统计测试，在第11.3节中将更详细地介绍。\n\n### BenchmarkResult Objects\n\nA `BenchmarkResult` object is a collection of multiple `ResampleResult` objects.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmrdt = as.data.table(bmr)\nbmrdt[1:2, .(task, learner, resampling, iteration)]\n#>                 task                   learner         resampling iteration\n#> 1: <TaskClassif[51]> <LearnerClassifRpart[38]> <ResamplingCV[20]>         1\n#> 2: <TaskClassif[51]> <LearnerClassifRpart[38]> <ResamplingCV[20]>         2\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr1 = bmr$resample_result(1)\nrr2 = bmr$resample_result(2)\nrr1\n#> <ResampleResult> with 5 resampling iterations\n#>        task_id    learner_id resampling_id iteration warnings errors\n#>  german_credit classif.rpart            cv         1        0      0\n#>  german_credit classif.rpart            cv         2        0      0\n#>  german_credit classif.rpart            cv         3        0      0\n#>  german_credit classif.rpart            cv         4        0      0\n#>  german_credit classif.rpart            cv         5        0      0\n```\n:::\n\n\nIn addition, `as_benchmark_result()` can be used to convert objects from `ResampleResult` to `BenchmarkResult.` The `c()`-method can be used to combine multiple `BenchmarkResult` objects, which can be useful when conducting experiments across multiple machines:\n\n> 此外，可以使用 `as_benchmark_result()` 将 `ResampleResult` 对象转换为 `BenchmarkResult`。`c()` 方法可用于组合多个 `BenchmarkResult` 对象，这在跨多台计算机进行实验时非常有用：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmr1 = as_benchmark_result(rr1)\nbmr2 = as_benchmark_result(rr2)\n\nc(bmr1, bmr2)\n#> <BenchmarkResult> of 10 rows with 2 resampling runs\n#>  nr       task_id     learner_id resampling_id iters warnings errors\n#>   1 german_credit  classif.rpart            cv     5        0      0\n#>   2 german_credit classif.ranger            cv     5        0      0\n```\n:::\n\n\nBoxplots are most commonly used to visualize benchmark experiments as they can intuitively summarize results across tasks and learners simultaneously.\n\n> 箱线图最常用于可视化基准实验，因为它们可以直观地同时总结任务和学习器之间的结果。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(bmr, measure = msr(\"classif.acc\"))\n```\n\n::: {.cell-output-display}\n![Boxplots of accuracy scores for each learner across resampling iterations and the three tasks. Random forests (`lrn(\"classif.ranger\")`) consistently outperforms the other learners.](index_files/figure-html/fig-benchmark-box-1.png){#fig-benchmark-box fig-align='center' width=70%}\n:::\n:::\n\n\n## Evaluation of Binary Classifiers\n\n### Confusion Matrix\n\nIt is possible for a classifier to have a good classification accuracy but to overlook the nuances provided by a full confusion matrix, as in the following `tsk(\"german_credit\")` example:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_german = tsk(\"german_credit\")\nlrn_ranger = lrn(\"classif.ranger\", predict_type = \"prob\")\nsplits = partition(tsk_german, ratio = .8)\n\nlrn_ranger$train(tsk_german, splits$train)\nprediction = lrn_ranger$predict(tsk_german, splits$test)\nprediction$score(msr(\"classif.acc\"))\n#> classif.acc \n#>        0.74\nprediction$confusion\n#>         truth\n#> response good bad\n#>     good  124  36\n#>     bad    16  24\n```\n:::\n\n\nOn their own, the absolute numbers in a confusion matrix can be less useful when there is class imbalance. Instead, several normalized measures can be derived (@fig-confusion):\n\n- **True Positive Rate (TPR)**, **Sensitivity** or **Recall**: How many of the true positives did we predict as positive?\n\n- **True Negative Rate (TNR)** or **Specificity**: How many of the true negatives did we predict as negative?\n\n- **False Positive Rate (FPR)**, or $1 -$ **Specificity**: How many of the true negatives did we predict as positive?\n\n- **Positive Predictive Value (PPV)** or **Precision**: If we predict positive how likely is it a true positive?\n\n- **Negative Predictive Value (NPV)**: If we predict negative how likely is it a true negative?\n\n- **Accuracy (ACC)**: The proportion of correctly classified instances out of the total number of instances.\n\n- **F1-score**: The harmonic mean of precision and recall, which balances the trade-off between precision and recall. It is calculated as $2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Binary confusion matrix of ground truth class vs. predicted class.](imgs/confusion_matrix.svg){#fig-confusion fig-align='center' width=70%}\n:::\n:::\n\n\nThe `mlr3measures` package allows you to compute several common confusion matrix-based measures using the `confusion_matrix()` function:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlr3measures::confusion_matrix(\n  truth = prediction$truth,\n  response = prediction$response,\n  positive = tsk_german$positive\n)\n#>         truth\n#> response good bad\n#>     good  124  36\n#>     bad    16  24\n#> acc :  0.7400; ce  :  0.2600; dor :  5.1667; f1  :  0.8267 \n#> fdr :  0.2250; fnr :  0.1143; fomr:  0.4000; fpr :  0.6000 \n#> mcc :  0.3273; npv :  0.6000; ppv :  0.7750; tnr :  0.4000 \n#> tpr :  0.8857\n```\n:::\n\n\n### ROC Analysis\n\nThe ROC curve is a line graph with TPR on the y-axis and the FPR on the x-axis. \n\nConsider classifiers that predict probabilities instead of discrete classes. Using different thresholds to cut off predicted probabilities and assign them to the positive and negative class will lead to different TPRs and FPRs and by plotting these values across different thresholds we can characterize the behavior of a binary classifier – this is the ROC curve.\n\n> 考虑预测概率而不是离散类别的分类器。使用不同的阈值来截断预测的概率并将其分配到正类别和负类别将导致不同的 TPR 和 FPR，并通过在不同的阈值上绘制这些值，我们可以表征二元分类器的行为 - 这就是 ROC 曲线。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(prediction, type = \"roc\")\n```\n\n::: {.cell-output-display}\n![ROC-curve based on the `german_credit` dataset and the `classif.ranger` random forest learner. Recall FPR = $1 -$ Specificity and TPR = Sensitivity.](index_files/figure-html/fig-basics-roc-ranger-1.png){#fig-basics-roc-ranger fig-align='center' width=70%}\n:::\n:::\n\n\nA natural performance measure that can be derived from the ROC curve is the area under the curve (AUC), implemented in `msr(\"classif.auc\")`. The AUC can be interpreted as the probability that a randomly chosen positive instance has a higher predicted probability of belonging to the positive class than a randomly chosen negative instance. Therefore, higher values (closer to \n) indicate better performance. Random classifiers (such as the featureless baseline) will always have an AUC of (approximately, when evaluated empirically) 0.5.\n\n> 从 ROC 曲线中可以导出的一个自然性能度量是曲线下面积（AUC），在 `msr(\"classif.auc\")` 中实现。AUC 可以解释为随机选择的正实例具有较高的预测概率，属于正类别，而不是随机选择的负实例的概率。因此，较高的值（越接近 1）表示更好的性能。随机分类器（例如没有特征的基线）的AUC总是为（在经验上评估时约为 0.5）。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction$score(msr(\"classif.auc\"))\n#> classif.auc \n#>   0.7407143\n```\n:::\n\n\nWe can also plot the precision-recall curve (PRC) which visualizes the PPV/precision vs. TPR/recall. The main difference between ROC curves and PR curves is that the number of true-negatives are ignored in the latter. This can be useful in imbalanced populations where the positive class is rare, and where a classifier with high TPR may still not be very informative and have low PPV. See Davis and Goadrich (2006) for a detailed discussion about the relationship between the PRC and ROC curves.\n\n> 我们还可以绘制精确度-召回曲线（PRC），该曲线可视化了 PPV/精确度 与 TPR/召回 之间的关系。ROC曲线和PR曲线之间的主要区别在于后者忽略了真负例的数量。在不平衡的人群中，正类别很少见的情况下，具有高TPR的分类器可能仍然不太具有信息性，并且具有较低的PPV。有关PRC和ROC曲线之间关系的详细讨论，请参阅 Davis 和 Goadrich（2006）。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(prediction, type = \"prc\")\n```\n\n::: {.cell-output-display}\n![Precision-Recall curve based on `tsk(\"german_credit\")` and `lrn(\"classif.ranger\")`.](index_files/figure-html/fig-basics-prc-ranger-1.png){#fig-basics-prc-ranger fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.callout-tip title=\"To be continued\"}\n- <https://mlr3book.mlr-org.com/chapters/chapter3/evaluation_and_benchmarking.html#sec-roc-space>\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}