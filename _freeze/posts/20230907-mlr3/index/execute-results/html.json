{
  "hash": "1b2f154e74b7cd6b59033f01947f8473",
  "result": {
    "markdown": "---\ntitle: \"mlr3verse 技术手册\"\ndate: \"2023-09-07\"\ndate-modified: \"2023-09-11\"\nimage: \"cover.jpg\"\ncategories: \n  - Machine Learning\n  - R\n  - mlr3\n---\n\n\n::: {.callout-note title='Progress'}\nLearning Progress: 75.18%.\n:::\n\n::: {.callout-tip title=\"Learning Source\"}\n- 张敬信老师 QQ 群（222427909）文件\n- <https://github.com/zhjx19/RConf15/tree/main>\n:::\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n```\n:::\n\n\n# 基础知识\n\n## 任务：封装数据\n\n任务是表格数据的封装，自变量为特征，因变量为目标或结果变量。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsks() # 查看所有自带任务\n#> <DictionaryTask> with 21 stored values\n#> Keys: ames_housing, bike_sharing, boston_housing, breast_cancer,\n#>   german_credit, ilpd, iris, kc_housing, moneyball, mtcars, optdigits,\n#>   penguins, penguins_simple, pima, ruspini, sonar, spam, titanic,\n#>   usarrests, wine, zoo\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 创建任务\ndat = tsk(\"german_credit\")$data() # 提取数据\ntask = as_task_classif(dat, target = \"credit_risk\")\ntask\n#> <TaskClassif:dat> (1000 x 21)\n#> * Target: credit_risk\n#> * Properties: twoclass\n#> * Features (20):\n#>   - fct (14): credit_history, employment_duration, foreign_worker,\n#>     housing, job, other_debtors, other_installment_plans,\n#>     people_liable, personal_status_sex, property, purpose, savings,\n#>     status, telephone\n#>   - int (3): age, amount, duration\n#>   - ord (3): installment_rate, number_credits, present_residence\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 选择特征\ntask$select(cols = setdiff(task$feature_names, \"telephone\"))\ntask\n#> <TaskClassif:dat> (1000 x 20)\n#> * Target: credit_risk\n#> * Properties: twoclass\n#> * Features (19):\n#>   - fct (13): credit_history, employment_duration, foreign_worker,\n#>     housing, job, other_debtors, other_installment_plans,\n#>     people_liable, personal_status_sex, property, purpose, savings,\n#>     status\n#>   - int (3): age, amount, duration\n#>   - ord (3): installment_rate, number_credits, present_residence\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 划分训练集、测试集\nset.seed(1)\nsplit = partition(task, ratio = .7)\n```\n:::\n\n\n`stratify = TRUE` 默认按目标变量分层，得到训练集和测试集的索引（行号）。\n\n## 学习器：封装算法\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrns()  # 查看所有自带学习器名字\n#> <DictionaryLearner> with 46 stored values\n#> Keys: classif.cv_glmnet, classif.debug, classif.featureless,\n#>   classif.glmnet, classif.kknn, classif.lda, classif.log_reg,\n#>   classif.multinom, classif.naive_bayes, classif.nnet, classif.qda,\n#>   classif.ranger, classif.rpart, classif.svm, classif.xgboost,\n#>   clust.agnes, clust.ap, clust.cmeans, clust.cobweb, clust.dbscan,\n#>   clust.diana, clust.em, clust.fanny, clust.featureless, clust.ff,\n#>   clust.hclust, clust.kkmeans, clust.kmeans, clust.MBatchKMeans,\n#>   clust.mclust, clust.meanshift, clust.pam, clust.SimpleKMeans,\n#>   clust.xmeans, regr.cv_glmnet, regr.debug, regr.featureless,\n#>   regr.glmnet, regr.kknn, regr.km, regr.lm, regr.nnet, regr.ranger,\n#>   regr.rpart, regr.svm, regr.xgboost\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 选择随机森林分类学习器\nlearner = lrn(\"classif.ranger\", num.trees = 100, predict_type = \"prob\")\nlearner\n#> <LearnerClassifRanger:classif.ranger>\n#> * Model: -\n#> * Parameters: num.threads=1, num.trees=100\n#> * Packages: mlr3, mlr3learners, ranger\n#> * Predict Types:  response, [prob]\n#> * Feature Types: logical, integer, numeric, character, factor, ordered\n#> * Properties: hotstart_backward, importance, multiclass, oob_error,\n#>   twoclass, weights\n```\n:::\n\n\n学习器 `$model` 属性为 `NULL`，用 `$train()` 方法在训练集上训练模型，模型结果存入 `$model`，再用 `predict()` 方法在测试集上做预测，得到结果是 `Prediction` 对象。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner$train(task, row_ids = split$train)\nlearner$model\n#> Ranger result\n#> \n#> Call:\n#>  ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      probability = self$predict_type == \"prob\", case.weights = task$weights$weight,      num.threads = 1L, num.trees = 100L) \n#> \n#> Type:                             Probability estimation \n#> Number of trees:                  100 \n#> Sample size:                      700 \n#> Number of independent variables:  19 \n#> Mtry:                             4 \n#> Target node size:                 10 \n#> Variable importance mode:         none \n#> Splitrule:                        gini \n#> OOB prediction error (Brier s.):  0.1615879\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction = learner$predict(task, row_ids = split$test)\nprediction\n```\n:::\n\n\n## 性能评估\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmsrs() # 查看所有支持的性能度量指标\n#> <DictionaryMeasure> with 66 stored values\n#> Keys: aic, bic, classif.acc, classif.auc, classif.bacc, classif.bbrier,\n#>   classif.ce, classif.costs, classif.dor, classif.fbeta, classif.fdr,\n#>   classif.fn, classif.fnr, classif.fomr, classif.fp, classif.fpr,\n#>   classif.logloss, classif.mauc_au1p, classif.mauc_au1u,\n#>   classif.mauc_aunp, classif.mauc_aunu, classif.mbrier, classif.mcc,\n#>   classif.npv, classif.ppv, classif.prauc, classif.precision,\n#>   classif.recall, classif.sensitivity, classif.specificity, classif.tn,\n#>   classif.tnr, classif.tp, classif.tpr, clust.ch, clust.dunn,\n#>   clust.silhouette, clust.wss, debug_classif, oob_error, regr.bias,\n#>   regr.ktau, regr.mae, regr.mape, regr.maxae, regr.medae, regr.medse,\n#>   regr.mse, regr.msle, regr.pbias, regr.rae, regr.rmse, regr.rmsle,\n#>   regr.rrse, regr.rse, regr.rsq, regr.sae, regr.smape, regr.srho,\n#>   regr.sse, selected_features, sim.jaccard, sim.phi, time_both,\n#>   time_predict, time_train\n```\n:::\n\n\n用预测对象的 `$score()` 方法，计算该度量指标的得分：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction$score(msr(\"classif.acc\"))  # 准确率\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 绘制 ROC 曲线\nlibrary(precrec)\nautoplot(prediction, type = \"roc\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction$score(msr(\"classif.auc\"))  # auc 面积\n```\n:::\n\n\n## 重抽样\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrsmps()  # 查看所有支持的重抽样方法\n#> <DictionaryResampling> with 9 stored values\n#> Keys: bootstrap, custom, custom_cv, cv, holdout, insample, loo,\n#>   repeated_cv, subsampling\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv10 = rsmp(\"cv\", folds = 10)  # 10 折交叉验证\n```\n:::\n\n\n### 实例化重抽样对象\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv10$instantiate(task)  # 实例化\ncv10$iters  # 数据副本数\n#> [1] 10\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv10$train_set(1)  # 第 1 个数据副本的训练集索引\ncv10$test_set(1)   # 第 1 个数据副本的测试集索引\n```\n:::\n\n\n### 使用重抽样\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr =  resample(task, learner, cv10, store_models = TRUE)\n#> INFO  [19:39:42.364] [mlr3] Applying learner 'classif.ranger' on task 'dat' (iter 1/10)\n#> INFO  [19:39:42.611] [mlr3] Applying learner 'classif.ranger' on task 'dat' (iter 2/10)\n#> INFO  [19:39:42.679] [mlr3] Applying learner 'classif.ranger' on task 'dat' (iter 3/10)\n#> INFO  [19:39:42.734] [mlr3] Applying learner 'classif.ranger' on task 'dat' (iter 4/10)\n#> INFO  [19:39:42.790] [mlr3] Applying learner 'classif.ranger' on task 'dat' (iter 5/10)\n#> INFO  [19:39:42.846] [mlr3] Applying learner 'classif.ranger' on task 'dat' (iter 6/10)\n#> INFO  [19:39:42.903] [mlr3] Applying learner 'classif.ranger' on task 'dat' (iter 7/10)\n#> INFO  [19:39:42.960] [mlr3] Applying learner 'classif.ranger' on task 'dat' (iter 8/10)\n#> INFO  [19:39:43.023] [mlr3] Applying learner 'classif.ranger' on task 'dat' (iter 9/10)\n#> INFO  [19:39:43.081] [mlr3] Applying learner 'classif.ranger' on task 'dat' (iter 10/10)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr$aggregate(msr(\"classif.acc\"))  # 所有重抽样的平均准确率\n#> classif.acc \n#>       0.753\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr$score(msr(\"classif.acc\"))  # 各个重抽样的平均准确率\n#>     task_id     learner_id resampling_id iteration classif.acc\n#>  1:     dat classif.ranger            cv         1        0.77\n#>  2:     dat classif.ranger            cv         2        0.84\n#>  3:     dat classif.ranger            cv         3        0.70\n#>  4:     dat classif.ranger            cv         4        0.77\n#>  5:     dat classif.ranger            cv         5        0.76\n#>  6:     dat classif.ranger            cv         6        0.75\n#>  7:     dat classif.ranger            cv         7        0.71\n#>  8:     dat classif.ranger            cv         8        0.83\n#>  9:     dat classif.ranger            cv         9        0.66\n#> 10:     dat classif.ranger            cv        10        0.74\n#> Hidden columns: task, learner, resampling, prediction\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 查看第 1 个数据副本（第 1 折）上的结果\nrr$resampling$train_set(1)  # 第 1 折的训练集索引\nrr$learners[[1]]$model      # 第 1 折学习器的拟合模型\nrr$predictions()[[1]]       # 第 1 折的预测结果\n```\n:::\n\n\n## 基准测试\n\n基准测试（benchmark）用来比较不同学习器（算法）、在多个任务（数据）和/或不同重抽样策略（多个数据副本）上的平均性能表现。\n\n基准测试时有个关键问题：测试的公平性。即每个算法的每次测试必须在相同的重抽样训练集拟合模型，在相同的重抽样测试集评估性能。这些事情 `beachmark()` 会自动做好。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntasks = tsk(\"sonar\")  # 可以多任务\nlearners = lrns(c(\"classif.rpart\", \"classif.kknn\", \"classif.ranger\", \"classif.svm\"),\n                predict_type = \"prob\")\ndesign = benchmark_grid(tasks, learners, rsmps(\"cv\", folds = 5))\ndesign\n#>     task        learner resampling\n#> 1: sonar  classif.rpart         cv\n#> 2: sonar   classif.kknn         cv\n#> 3: sonar classif.ranger         cv\n#> 4: sonar    classif.svm         cv\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmr = benchmark(design)  # 执行基准测试\nbmr$aggregate(list(msr(\"classif.acc\"), msr(\"classif.auc\")))  # 汇总基准测试结果\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 可视化：对比性能\nautoplot(bmr, type = \"roc\")  # ROC 曲线\nautoplot(bmr, measure = msr(\"classif.auc\"))  # AUC 箱线图\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-25-1.png){fig-align='center' width=70%}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-25-2.png){fig-align='center' width=70%}\n:::\n:::\n\n\n## 可视化\n\nmlr3viz 包定义了 `autoplot()` 函数来用 ggplot2 绘图。通常一个对象有不止一种类型的图，可以通过 `type` 参数来改变绘图。图形使用 viridis 的调色板，外观由 theme 参数控制，默认是 minimal 主题。\n\n### 可视化任务\n\n#### 分类任务\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"penguins\")\ntask$select(c(\"body_mass\", \"bill_length\"))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(task, type = \"target\")  # \"target\" 图：条形图展示目标变量的各类别频数\nautoplot(task, type = \"duo\")     # \"duo\" 图：箱线图展示多个特征的分布\nautoplot(task, type = \"pairs\")   # \"pairs\" 图：展示多个特征的成对比较\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-27-1.png){fig-align='center' width=70%}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-27-2.png){fig-align='center' width=70%}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-27-3.png){fig-align='center' width=70%}\n:::\n:::\n\n\n#### 回归任务\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"mtcars\")\ntask$select(c(\"am\", \"carb\"))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(task, type = \"target\")  # \"target\" 图：箱线图展示目标变量的分布\nautoplot(task, type = \"pairs\")   # \"pairs\" 图：展示多个特征与目标变量的成对比较\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-29-1.png){fig-align='center' width=70%}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-29-2.png){fig-align='center' width=70%}\n:::\n:::\n\n\n### 可视化学习器\n\n#### glmnet 回归学习器\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"mtcars\")\nlearner = lrn(\"regr.glmnet\")\nlearner$train(task)\nautoplot(learner)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-30-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n#### 决策树学习器\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 分类树\ntask = tsk(\"penguins\")\nlearner = lrn(\"classif.rpart\", keep_model = TRUE)\nlearner$train(task)\nautoplot(learner)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-31-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 回归树\ntask = tsk(\"mtcars\")\nlearner = lrn(\"regr.rpart\", keep_model = TRUE)\nlearner$train(task)\nautoplot(learner)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-32-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n#### 层次聚类学习器\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 层次聚类树\ntask = tsk(\"usarrests\")\nlearner = lrn(\"clust.hclust\")\nlearner$train(task)\nautoplot(learner, type = \"dend\", task = task)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-33-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 碎石图\nautoplot(learner, type = \"scree\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-34-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n### 可视化预测对象\n\n#### 分类预测对象\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"stacked\" 图：堆叠条形图展示预测类别和真实类别频数对比\ntask = tsk(\"spam\")\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\")\npred = learner$train(task)$predict(task)\nautoplot(pred, type = \"stacked\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-35-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# ROC 曲线：展示不同阈值下的真阳率与假阳率\nautoplot(pred, type = \"roc\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-36-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# PR 曲线：展示不同阈值下的查准率与召回率\nautoplot(pred, type = \"prc\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-37-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# “threshold” 图：展示二元分类在不同阈值下的性能\nautoplot(pred, type = \"threshold\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-38-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n#### 回归预测对象\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"xy\" 图：散点图展示回归预测的真实值与预测值\ntask = tsk(\"boston_housing\")\nlearner = lrn(\"regr.rpart\")\npred = learner$train(task)$predict(task)\nautoplot(pred, type = \"xy\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-39-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"residual\" 图：绘制响应的残差图\nautoplot(pred, type = \"residual\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-40-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"histogram\" 图：残差直方图展示残差的分布\nautoplot(pred, type = \"histogram\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-41-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n#### 聚类预测对象\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"scatter\" 图：绘制按聚类预测结果着色的散点图\ntask = tsk(\"usarrests\")\nlearner = lrn(\"clust.kmeans\", centers = 3)\npred = learner$train(task)$predict(task)\nautoplot(pred, task, type = \"scatter\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-42-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"sil\" 图：展示聚类的silhouette 宽度，虚线是平均silhouette 宽度\nautoplot(pred, task, type = \"sil\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-43-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"pca\" 图：展示数据的前两个主成分，不同聚类用颜色区分\nautoplot(pred, task, type = \"pca\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-44-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n### 可视化重抽样结果\n\n#### 分类重抽样结果\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"boxplot\"/\"histogram\" 图：箱线图展示性能度量的分布\ntask = tsk(\"sonar\")\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\")\nresampling = rsmp(\"cv\")\nrr = resample(task, learner, resampling)\n#> INFO  [19:39:57.052] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 1/10)\n#> INFO  [19:39:57.094] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 2/10)\n#> INFO  [19:39:57.124] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 3/10)\n#> INFO  [19:39:57.155] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 4/10)\n#> INFO  [19:39:57.183] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 5/10)\n#> INFO  [19:39:57.213] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 6/10)\n#> INFO  [19:39:57.246] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 7/10)\n#> INFO  [19:39:57.288] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 8/10)\n#> INFO  [19:39:57.318] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 9/10)\n#> INFO  [19:39:57.349] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 10/10)\nautoplot(rr, type = \"boxplot\")\nautoplot(rr, type = \"histogram\")\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-45-1.png){fig-align='center' width=70%}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-45-2.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# ROC 曲线：展示不同阈值下的真阳率与假阳率\nautoplot(rr, type = \"roc\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-46-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# PR 曲线：展示不同阈值下的查准率与召回率\nautoplot(rr, type = \"prc\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-47-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"prediction\" 图：展示两个特征表示的测试集样本点和以背景色区分的预测类别\ntask = tsk(\"pima\")\ntask$filter(seq(100))\ntask$select(c(\"age\", \"glucose\"))\nlearner = lrn(\"classif.rpart\")\nresampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, learner, resampling, store_models = TRUE)\n#> INFO  [19:39:58.675] [mlr3] Applying learner 'classif.rpart' on task 'pima' (iter 1/3)\n#> INFO  [19:39:58.703] [mlr3] Applying learner 'classif.rpart' on task 'pima' (iter 2/3)\n#> INFO  [19:39:58.737] [mlr3] Applying learner 'classif.rpart' on task 'pima' (iter 3/3)\nautoplot(rr, type = \"prediction\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-48-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 若将学习器的预测类型改为\"prob\"，则用颜色深浅展示概率值\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\")\nresampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, learner, resampling, store_models = TRUE)\n#> INFO  [19:39:59.630] [mlr3] Applying learner 'classif.rpart' on task 'pima' (iter 1/3)\n#> INFO  [19:39:59.656] [mlr3] Applying learner 'classif.rpart' on task 'pima' (iter 2/3)\n#> INFO  [19:39:59.687] [mlr3] Applying learner 'classif.rpart' on task 'pima' (iter 3/3)\nautoplot(rr, type = \"prediction\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-49-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 上面是只绘制测试集，也可以加入训练集\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\",\n              predict_sets = c(\"train\", \"test\"))\nresampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, learner, resampling, store_models = TRUE)\n#> INFO  [19:40:00.659] [mlr3] Applying learner 'classif.rpart' on task 'pima' (iter 1/3)\n#> INFO  [19:40:00.692] [mlr3] Applying learner 'classif.rpart' on task 'pima' (iter 2/3)\n#> INFO  [19:40:00.727] [mlr3] Applying learner 'classif.rpart' on task 'pima' (iter 3/3)\nautoplot(rr, type = \"prediction\",\n         predict_sets = c(\"train\", \"test\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-50-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"prediction\" 图也可以绘制分类特征\ntask = tsk(\"german_credit\")\ntask$filter(seq(100))\ntask$select(c(\"housing\", \"employment_duration\"))\nlearner = lrn(\"classif.rpart\")\nresampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, learner, resampling, store_models = TRUE)\n#> INFO  [19:40:01.705] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 1/3)\n#> INFO  [19:40:01.730] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 2/3)\n#> INFO  [19:40:01.755] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 3/3)\nautoplot(rr, type = \"prediction\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-51-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n#### 回归重抽样结果\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"prediction\" 图：绘制一个特征与响应的散点图，散点表示测试集中的观测\ntask = tsk(\"boston_housing\")\ntask$select(\"age\")\ntask$filter(seq(100))\nlearner = lrn(\"regr.rpart\")\nresampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, learner, resampling, store_models = TRUE)\n#> INFO  [19:40:02.426] [mlr3] Applying learner 'regr.rpart' on task 'boston_housing' (iter 1/3)\n#> INFO  [19:40:02.440] [mlr3] Applying learner 'regr.rpart' on task 'boston_housing' (iter 2/3)\n#> INFO  [19:40:02.505] [mlr3] Applying learner 'regr.rpart' on task 'boston_housing' (iter 3/3)\nautoplot(rr, type = \"prediction\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-52-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 若将学习器的预测类型改为\"se\"，还可以加上置信带\nlearner = lrn(\"regr.lm\", predict_type = \"se\")\nresampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, learner, resampling, store_models = TRUE)\n#> INFO  [19:40:03.114] [mlr3] Applying learner 'regr.lm' on task 'boston_housing' (iter 1/3)\n#> INFO  [19:40:03.137] [mlr3] Applying learner 'regr.lm' on task 'boston_housing' (iter 2/3)\n#> INFO  [19:40:03.157] [mlr3] Applying learner 'regr.lm' on task 'boston_housing' (iter 3/3)\nautoplot(rr, type = \"prediction\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-53-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 上面是只绘制测试集，也可以加入训练集\ntask$select(\"age\")\nlearner = lrn(\"regr.lm\", predict_type = \"se\",\n              predict_sets = c(\"train\", \"test\"))\nresampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, learner, resampling, store_models = TRUE)\n#> INFO  [19:40:03.909] [mlr3] Applying learner 'regr.lm' on task 'boston_housing' (iter 1/3)\n#> INFO  [19:40:03.935] [mlr3] Applying learner 'regr.lm' on task 'boston_housing' (iter 2/3)\n#> INFO  [19:40:03.961] [mlr3] Applying learner 'regr.lm' on task 'boston_housing' (iter 3/3)\nautoplot(rr, type = \"prediction\",\n         predict_sets = c(\"train\", \"test\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-54-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 还可以将预测面添加到背景\ntask = tsk(\"boston_housing\")\ntask$select(c(\"age\", \"rm\"))\ntask$filter(seq(100))\nlearner = lrn(\"regr.rpart\")\nresampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, learner, resampling, store_models = TRUE)\n#> INFO  [19:40:04.765] [mlr3] Applying learner 'regr.rpart' on task 'boston_housing' (iter 1/3)\n#> INFO  [19:40:04.786] [mlr3] Applying learner 'regr.rpart' on task 'boston_housing' (iter 2/3)\n#> INFO  [19:40:04.807] [mlr3] Applying learner 'regr.rpart' on task 'boston_housing' (iter 3/3)\nautoplot(rr, type = \"prediction\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-55-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n### 可视化基准测试结果\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"boxplot\" 图：箱线图展示了多个任务的基准测试的性能分布\ntasks = tsks(c(\"pima\", \"sonar\"))\nlearners = lrns(c(\"classif.featureless\", \"classif.rpart\", \"classif.xgboost\"),\n                predict_type = \"prob\")\nresampling = rsmps(\"cv\")\nbmr = benchmark(benchmark_grid(tasks, learners, resampling))\nautoplot(bmr, type = \"boxplot\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-56-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 绘制一个任务与多个学习器的基准测试\ntask = tsk(\"pima\")\nlearners = lrns(c(\"classif.featureless\", \"classif.rpart\", \"classif.xgboost\"),\n                predict_type = \"prob\")\nresampling = rsmp(\"cv\")\nbmr = benchmark(benchmark_grid(task, learners, resampling))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(bmr, type = \"roc\")\nautoplot(bmr, type = \"prc\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-58-1.png){fig-align='center' width=70%}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-58-2.png){fig-align='center' width=70%}\n:::\n:::\n\n\n### 可视化调参实例\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"performance\" 图：散点折线图展示随批次的模型性能变化\ninstance = tune(\n  tuner = tnr(\"gensa\"),\n  task = tsk(\"sonar\"),\n  learner = lts(lrn(\"classif.rpart\")),\n  resampling = rsmp(\"holdout\"),\n  measures = msr(\"classif.ce\"),\n  term_evals = 100\n)\nautoplot(instance, type = \"performance\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-59-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"parameter\" 图：散点图展示每个超参数取值与模型性能变化\nautoplot(instance, type = \"parameter\", cols_x = c(\"cp\", \"minsplit\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-60-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"marginal\" 图：展示不同超参数值的性能，颜色表示批次\nautoplot(instance, type = \"marginal\", cols_x = \"cp\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-61-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"parallel\" 图：可视化超参数之间的关系\nautoplot(instance, type = \"parallel\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-62-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"points\" 图：散点热力图展示两个超参数的性能对比，用颜色深浅表示模型性能\nautoplot(instance, type = \"points\", cols_x = c(\"cp\", \"minsplit\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-63-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"pairs\" 图：展示所有超参数成对对比\nautoplot(instance, type = \"pairs\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-64-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"surface\" 图：绘制两个超参数的模型性能面，该面是用一个学习器插值的\nautoplot(instance, type = \"surface\", cols_x = c(\"cp\", \"minsplit\"),\n         learner = lrn(\"regr.ranger\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-65-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n### 可视化特征过滤器\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 条形图展示基于过滤器的特征得分\ntask = tsk(\"mtcars\")\nft = flt(\"correlation\")\nft$calculate(task)\nautoplot(ft, n = 5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-66-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n更多细节在 mlr3viz 包的 reference 页。\n\n# 图学习器\n\n一个管道运算（PipeOp），表示机器学习管道中的一个计算步骤。一系列的 PipeOps  通过边连接（`%>>%`）构成图（Graph），图可以是简单的线性图，也可以是复杂的非线性图。\n\n搭建图学习器：\n\n- 用 `po()` 获取 PipeOp，通过连接符 `%>>%` 连接 Graphs 与 PipeOps\n\n- 通过 `gunion()` 将 Graphs 与 PipeOps 并起来\n\n- 用 `ppl(\"replicate\", graph, n)` 将 Graph 或 PipeOps 复制 n 份并起来\n\n- `Graph$plot()` 绘制图的结构关系\n\n- `as_learner(Graph)` 将图转化为学习器，即可跟普通学习器一样使用\n\n管道、图学习器的主要用处在于：\n\n- 特征工程：缺失值插补、特征提取、特征选择、处理不均衡数据……\n\n- 集成学习：袋装法、堆叠法\n\n- 分支训练、分块训练\n\n## 线形图\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 搭建图\ngraph = po(\"scale\") %>>%\n  po(\"encode\") %>>%\n  po(\"imputemedian\") %>>%\n  lrn(\"classif.rpart\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 可视化图\ngraph$plot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-68-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 转为图学习器\ngl = as_learner(graph)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 训练任务\ntask = tsk(\"iris\")\ngl$train(task)\n```\n:::\n\n\n调试图：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 取出或设置图学习器超参数\ngraph$pipeops$scale$param_set$values$center = FALSE\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 获取单独PipeOp 的$state（经过$train() 后）\ngraph$keep_results = TRUE\ngraph$train(task)\n#> $classif.rpart.output\n#> NULL\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph$pipeops$scale$state$scale\n#> Petal.Length  Petal.Width Sepal.Length  Sepal.Width \n#>     4.163367     1.424451     5.921098     3.098387\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 查看图的中间结果：$.result（需提前设置$keep_results = TRUE）\ngraph$pipeops$scale$.result[[1]]$head()\n#>    Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n#> 1:  setosa    0.3362663    0.140405    0.8613268   1.1296201\n#> 2:  setosa    0.3362663    0.140405    0.8275493   0.9682458\n#> 3:  setosa    0.3122473    0.140405    0.7937718   1.0327956\n#> 4:  setosa    0.3602853    0.140405    0.7768830   1.0005207\n#> 5:  setosa    0.3362663    0.140405    0.8444380   1.1618950\n#> 6:  setosa    0.4083234    0.280810    0.9119931   1.2587196\n```\n:::\n\n\n## 非线性图\n\n### 分支\n\n集成学习的袋装法、堆叠法都是非线性图，另一种非线性图是分支：即只执行若干条备选路径中的一条。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_branch = ppl(\"branch\", list(\n  pca = po(\"pca\"),\n  ica = po(\"ica\")\n)) %>>%\n  lrn(\"classif.kknn\")\ngraph_branch$plot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-75-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n### 分块训练\n\n在数据太大，无法载入内存的情况下，一个常用的技术是将数据分成几块，然后分别对各块数据进行训练，之后再用 PipeOpClassifAvg 或 PipeOpRegrAvg 将模型按加权平均汇总。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_chunks = po(\"chunk\", 4) %>>%\n  ppl(\"greplicate\", lrn(\"classif.rpart\"), 4) %>>%\n  po(\"classifavg\", 4)\ngraph_chunks$plot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-76-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 转化为图学习器，与学习器一样使用\nchunks_lrn = as_learner(graph_chunks)\nchunks_lrn$train(tsk(\"iris\"))\n```\n:::\n\n\n## 集成学习\n\n集成学习，是通过构建多个基学习器，并按一定策略结合成强学习器来完成学习任务，即所谓“博采众长”，最终效果是优于任何一个原学习器。集成学习可用于分类/回归集成、特征选择集成、异常值检测集成等。\n\n这多个基学习器可以是同质的，比如都用决策树或都用神经网络，以 Bagging 和 Boosting 模式为代表；也可以是异质的，即采用不同的算法，以 Stacking 模式为代表。\n\n### 装袋法（Bagging）\n\nBagging 采用的是并行机制，即基学习器的训练之间没有前后顺序可以同时进行。\n\nBagging 是用“有放回” 抽样（Bootstrap 法）的方式抽取训练集，对于包含 $m$ 个样本的训练集，进\n行 $m$ 次有放回的随机抽样操作，得到样本子集（有重复）中有接近 36.8% 的样本没有被抽到。\n\n按照同样的方式重复进行，就可以采集到 $T$ 个包含 $m$ 个样本的数据副本，从而训练出 $T$ 个基学习器。\n\n最终对这 $T$ 个基学习器的输出进行结合，分类问题就采用“多数决”，回归问题就采用“取平均”。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 单分支：数据子抽样 + 决策树\nsingle_path = po(\"subsample\") %>>% lrn(\"classif.rpart\")\n# 复制 10 次得到 10 个分支，再接类平均\ngraph_bag = ppl(\"greplicate\", single_path, n = 10) %>>%\n  po(\"classifavg\")\n# 可视化结构关系\ngraph_bag$plot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-78-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 转为图学习器，可与普通学习器一样使用\nbaglrn = as_learner(graph_bag)\nbaglrn$train(tsk(\"iris\"))\n```\n:::\n\n\n### 提升法（Boosting）\n\nBoosting 采用的是串行机制，即基学习器的训练存在依赖关系，按次序一一进行训练（实现上可以做\n到并行）。\n\n基本思想：基模型的训练集按照某种策略每次都进行一定的转化，对所有基模型预测的结果进行线性合\n成产生最终的预测结果。\n\n从偏差-方差分解来看，Boosting 算法主要关注于降低偏差，每轮的迭代都关注于训练过程中预测错误的样本，将弱学习器提升为强学习器。\n\n### 堆叠法（Stacking）\n\nStacking 法，采用的是分阶段机制，将若干基模型的输出作为输入，再接一层主学习器，得到最终的预测。\n\n将训练好的所有基模型对训练集进行预测，第 $j$ 个基模型对第 $i$ 个训练样本的预测值将作为新的训练集中第 $i$ 个样本的第 $j$ 个特征值，最后基于新的训练集进行训练。同理，预测的过程也要先经过所有基模型的预测形成新的测试集，最后再对测试集进行预测。\n\n用于堆叠的基模型通常采用不同的模型，作用在相同的训练集上。\n\n为了充分利用数据，Stacking 通常采用 $k$ 折交叉训练法（类似 $k$ 折交叉验证）：每个基学习器分别在各个 $k-1$ 折数据上训练，在其剩下的 1 折数据上预测，就可以得到对任意 1 折数据的预测结果，进而用于训练主模型。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_stack = gunion(list(\n  po(\"learner_cv\", lrn(\"regr.lm\")),\n  po(\"learner_cv\", lrn(\"regr.svm\")),\n  po(\"nop\")\n)) %>>%\n  po(\"featureunion\") %>>%\n  lrn(\"regr.ranger\")\n\ngraph_stack$plot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-80-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 转为图学习器，可与普通学习器一样使用\nstacklrn = as_learner(graph_stack)\nstacklrn$train(tsk(\"mtcars\"))\n```\n:::\n\n\n# 特征工程\n\n机器学习中的数据预处理，通常也统称为特征工程，主要包括：缺失值插补、特征变换，目的是提升模型性能。\n\n## 特征工程概述\n\n用 mlr3pipelines 包实现特征工程。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 查看所有 PipeOp\npo()\n#> <DictionaryPipeOp> with 64 stored values\n#> Keys: boxcox, branch, chunk, classbalancing, classifavg, classweights,\n#>   colapply, collapsefactors, colroles, copy, datefeatures, encode,\n#>   encodeimpact, encodelmer, featureunion, filter, fixfactors, histbin,\n#>   ica, imputeconstant, imputehist, imputelearner, imputemean,\n#>   imputemedian, imputemode, imputeoor, imputesample, kernelpca,\n#>   learner, learner_cv, missind, modelmatrix, multiplicityexply,\n#>   multiplicityimply, mutate, nmf, nop, ovrsplit, ovrunite, pca, proxy,\n#>   quantilebin, randomprojection, randomresponse, regravg,\n#>   removeconstants, renamecolumns, replicate, scale, scalemaxabs,\n#>   scalerange, select, smote, spatialsign, subsample, targetinvert,\n#>   targetmutate, targettrafoscalerange, textvectorizer, threshold,\n#>   tunethreshold, unbranch, vtreat, yeojohnson\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngr = po(\"scale\") %>>% po(\"pca\", rank. = 2)\ngr$plot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-83-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n特征工程管道的三种用法：\n\n1. <p>调试：查看特征工程步对输入数据做了什么</p>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 训练特征工程管道：提供任务，访问特征工程之后的数据\ntask = tsk(\"iris\")\ngr$train(task)[[1]]$data()\n#>        Species        PC1         PC2\n#>   1:    setosa -2.2571412 -0.47842383\n#>   2:    setosa -2.0740130  0.67188269\n#>   3:    setosa -2.3563351  0.34076642\n#>   4:    setosa -2.2917068  0.59539986\n#>   5:    setosa -2.3818627 -0.64467566\n#>  ---                                 \n#> 146: virginica  1.8642579 -0.38567404\n#> 147: virginica  1.5593565  0.89369285\n#> 148: virginica  1.5160915 -0.26817075\n#> 149: virginica  1.3682042 -1.00787793\n#> 150: virginica  0.9574485  0.02425043\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 将训练好的特征工程管道，用于新的数据\ngr$predict(task$filter(1:5))[[1]]$data()\n#>    Species       PC1        PC2\n#> 1:  setosa -2.257141 -0.4784238\n#> 2:  setosa -2.074013  0.6718827\n#> 3:  setosa -2.356335  0.3407664\n#> 4:  setosa -2.291707  0.5953999\n#> 5:  setosa -2.381863 -0.6446757\n```\n:::\n\n\n2. <p>用于机器学习：原始任务经过特征工程管道变成预处理之后的任务，再正常用于机器学习流程</p>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnewtask = gr$train(task)[[1]]\nnewtask\n#> <TaskClassif:iris> (5 x 3): Iris Flowers\n#> * Target: Species\n#> * Properties: multiclass\n#> * Features (2):\n#>   - dbl (2): PC1, PC2\n```\n:::\n\n\n3. <p>用于机器学习：再接一个学习器，转化成图学习器，同普通学习器一样使用，适合对特征工程步、ML 算法一起联动调参</p>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngr = gr %>>% lrn(\"classif.rpart\")\ngr$plot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-87-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngr_learner = as_learner(gr)\ngr_learner\n#> <GraphLearner:scale.pca.classif.rpart>\n#> * Model: -\n#> * Parameters: scale.robust=FALSE, pca.rank.=2, classif.rpart.xval=0\n#> * Packages: mlr3, mlr3pipelines, rpart\n#> * Predict Types:  [response], prob\n#> * Feature Types: logical, integer, numeric, character, factor, ordered,\n#>   POSIXct\n#> * Properties: featureless, hotstart_backward, hotstart_forward,\n#>   importance, loglik, missings, multiclass, oob_error,\n#>   selected_features, twoclass, weights\n```\n:::\n\n\n## 缺失值插补\n\n目前支持的插补方法：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(mlr_pipeops)[tags %in% \"missings\", \"key\"]\n#>               key\n#> 1: imputeconstant\n#> 2:     imputehist\n#> 3:  imputelearner\n#> 4:     imputemean\n#> 5:   imputemedian\n#> 6:     imputemode\n#> 7:      imputeoor\n#> 8:   imputesample\n```\n:::\n\n\n### 简单插补\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"pima\")\ntask$missings()\n#> diabetes      age  glucose  insulin     mass pedigree pregnant pressure \n#>        0        0        5      374       11        0        0       35 \n#>  triceps \n#>      227\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 常数插补\npo = po(\"imputeconstant\", param_vals = list(\n  constant = -999, affect_columns = selector_name(\"glucose\")\n))\nnew_task = po$train(list(task = task))[[1]]\nnew_task$missings()\n#> diabetes      age  insulin     mass pedigree pregnant pressure  triceps \n#>        0        0      374       11        0        0       35      227 \n#>  glucose \n#>        0\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 均值插补\npo = po(\"imputemean\")\nnew_task = po$train(list(task = task))[[1]]\nnew_task$missings()\n#> diabetes      age pedigree pregnant  glucose  insulin     mass pressure \n#>        0        0        0        0        0        0        0        0 \n#>  triceps \n#>        0\n```\n:::\n\n\n### 随机抽样插补\n\n通过从非缺失的训练数据中随机抽样来插补特征。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npo = po(\"imputesample\")\nnew_task = po$train(list(task = task))[[1]]\nnew_task$missings()\n#> diabetes      age pedigree pregnant  glucose  insulin     mass pressure \n#>        0        0        0        0        0        0        0        0 \n#>  triceps \n#>        0\n```\n:::\n\n\n### 直方图法插补\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npo = po(\"imputehist\")\nnew_task = po$train(list(task = task))[[1]]\nnew_task$missings()\n#> diabetes      age pedigree pregnant  glucose  insulin     mass pressure \n#>        0        0        0        0        0        0        0        0 \n#>  triceps \n#>        0\n```\n:::\n\n\n### 学习器插补\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 决策树插补\npo = po(\"imputelearner\", lrn(\"regr.rpart\"))\nnew_task = po$train(list(task = task))[[1]]\nnew_task$missings()\n#> diabetes      age pedigree pregnant  glucose  insulin     mass pressure \n#>        0        0        0        0        0        0        0        0 \n#>  triceps \n#>        0\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# KNN插补，训练KNN学习器时，先用直方图法插补\npo = po(\"imputelearner\", \n        po(\"imputehist\") %>>% lrn(\"regr.kknn\"))\nnew_task = po$train(list(task = task))[[1]]\nnew_task$missings()\n#> diabetes      age pedigree pregnant  glucose  insulin     mass pressure \n#>        0        0        0        0        0        0        0        0 \n#>  triceps \n#>        0\n```\n:::\n\n\n## 特征工程\n\n### 特征缩放\n\n不同数值型特征的数据量纲可能相差多个数量级，这对很多数据模型会有很大影响，所以有必要做归一化处理，就是将列或行对齐并转化为一致。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndf = as_tibble(iris) %>% \n  set_names(str_c(\"x\", 1:4), \"Species\")\ntask = as_task_classif(df, target = \"Species\")\n```\n:::\n\n\n#### 标准化\n\n标准化也称为 Z 标准化，将数据变成均值为 0，标准差为 1。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npos = po(\"scale\")  # 参数 scale = FALSE，只做中心化\npos$train(list(task))[[1]]$data()\n#>        Species          x1          x2         x3         x4\n#>   1:    setosa -0.89767388  1.01560199 -1.3357516 -1.3110521\n#>   2:    setosa -1.13920048 -0.13153881 -1.3357516 -1.3110521\n#>   3:    setosa -1.38072709  0.32731751 -1.3923993 -1.3110521\n#>   4:    setosa -1.50149039  0.09788935 -1.2791040 -1.3110521\n#>   5:    setosa -1.01843718  1.24503015 -1.3357516 -1.3110521\n#>  ---                                                        \n#> 146: virginica  1.03453895 -0.13153881  0.8168591  1.4439941\n#> 147: virginica  0.55148575 -1.27867961  0.7035638  0.9192234\n#> 148: virginica  0.79301235 -0.13153881  0.8168591  1.0504160\n#> 149: virginica  0.43072244  0.78617383  0.9301544  1.4439941\n#> 150: virginica  0.06843254 -0.13153881  0.7602115  0.7880307\n```\n:::\n\n\n注：中心化后，0 就代表均值，更方便模型解释。\n\n#### 归一化\n\n归一化是将数据线性放缩到 $\\left[0, 1\\right]$（根据需要也可以线性放缩到 $\\left[a, b\\right]$）, 一般还同时考虑指标一致化，将正向指标（值越大越好）和负向指标（值越小越好）都变成正向。\n\n正向指标：\n\n$$\nx_i'=\\frac{x_i - \\text{min} x_i}{\\text{max} x_i - \\text{min} x_i}\n$$\n\n负向指标：\n\n$$\nx_i'=\\frac{\\text{max} x_i - x_i}{\\text{max} x_i - \\text{min} x_i}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npop = po(\"scalerange\", param_vals = list(lower = 0, upper = 1))\npop$train(list(task))[[1]]$data()\n#>        Species         x1        x2         x3         x4\n#>   1:    setosa 0.22222222 0.6250000 0.06779661 0.04166667\n#>   2:    setosa 0.16666667 0.4166667 0.06779661 0.04166667\n#>   3:    setosa 0.11111111 0.5000000 0.05084746 0.04166667\n#>   4:    setosa 0.08333333 0.4583333 0.08474576 0.04166667\n#>   5:    setosa 0.19444444 0.6666667 0.06779661 0.04166667\n#>  ---                                                     \n#> 146: virginica 0.66666667 0.4166667 0.71186441 0.91666667\n#> 147: virginica 0.55555556 0.2083333 0.67796610 0.75000000\n#> 148: virginica 0.61111111 0.4166667 0.71186441 0.79166667\n#> 149: virginica 0.52777778 0.5833333 0.74576271 0.91666667\n#> 150: virginica 0.44444444 0.4166667 0.69491525 0.70833333\n```\n:::\n\n\n#### 行规范化\n\n行规范化，常用于文本数据或聚类算法，是保证每行具有单位范数，即每行的向量“长度”相同。想象一下，$m$ 个特征下，每行数据都是 $m$ 维空间中的一个点，做行规范化能让这些点都落在单位球面上（到原点的距离均为 1）。\n\n行规范化一般采用 L2 范数：\n\n$$\nx_{ij}' = \\frac{x_{ij}}{\\|x_i\\|} = \\frac{x_{ij}}{\\sqrt{\\sum_{j=1}^m x_{ij}^2}}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npop = po(\"spatialsign\")\npop$train(list(task))[[1]]$data()\n#>        Species        x1        x2        x3         x4\n#>   1:    setosa 0.8037728 0.5516088 0.2206435 0.03152050\n#>   2:    setosa 0.8281329 0.5070201 0.2366094 0.03380134\n#>   3:    setosa 0.8053331 0.5483119 0.2227517 0.03426949\n#>   4:    setosa 0.8000302 0.5391508 0.2608794 0.03478392\n#>   5:    setosa 0.7909650 0.5694948 0.2214702 0.03163860\n#>  ---                                                   \n#> 146: virginica 0.7215572 0.3230853 0.5600146 0.24769876\n#> 147: virginica 0.7296536 0.2895451 0.5790902 0.22005426\n#> 148: virginica 0.7165390 0.3307103 0.5732312 0.22047353\n#> 149: virginica 0.6746707 0.3699807 0.5876164 0.25028107\n#> 150: virginica 0.6902592 0.3509792 0.5966647 0.21058754\n```\n:::\n\n\n### 特征变换\n\n#### 非线性特征\n\n对于数值特征 $x_1, x_2, \\cdots$，可以穿件更多的多项式特征：$x_1^2, x_1, x_2^2, x2, \\cdots$，这相当于是用自变量的更高阶泰勒公式去逼近因变量。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npop = po(\"modelmatrix\", formula = ~ . ^ 2 + I(x1 ^ 2) + log(x2))\npop$train(list(task))[[1]]$data()\n#>        Species (Intercept)  x1  x2  x3  x4 I(x1^2)   log(x2) x1:x2 x1:x3 x1:x4\n#>   1:    setosa           1 5.1 3.5 1.4 0.2   26.01 1.2527630 17.85  7.14  1.02\n#>   2:    setosa           1 4.9 3.0 1.4 0.2   24.01 1.0986123 14.70  6.86  0.98\n#>   3:    setosa           1 4.7 3.2 1.3 0.2   22.09 1.1631508 15.04  6.11  0.94\n#>   4:    setosa           1 4.6 3.1 1.5 0.2   21.16 1.1314021 14.26  6.90  0.92\n#>   5:    setosa           1 5.0 3.6 1.4 0.2   25.00 1.2809338 18.00  7.00  1.00\n#>  ---                                                                          \n#> 146: virginica           1 6.7 3.0 5.2 2.3   44.89 1.0986123 20.10 34.84 15.41\n#> 147: virginica           1 6.3 2.5 5.0 1.9   39.69 0.9162907 15.75 31.50 11.97\n#> 148: virginica           1 6.5 3.0 5.2 2.0   42.25 1.0986123 19.50 33.80 13.00\n#> 149: virginica           1 6.2 3.4 5.4 2.3   38.44 1.2237754 21.08 33.48 14.26\n#> 150: virginica           1 5.9 3.0 5.1 1.8   34.81 1.0986123 17.70 30.09 10.62\n#>      x2:x3 x2:x4 x3:x4\n#>   1:  4.90  0.70  0.28\n#>   2:  4.20  0.60  0.28\n#>   3:  4.16  0.64  0.26\n#>   4:  4.65  0.62  0.30\n#>   5:  5.04  0.72  0.28\n#>  ---                  \n#> 146: 15.60  6.90 11.96\n#> 147: 12.50  4.75  9.50\n#> 148: 15.60  6.00 10.40\n#> 149: 18.36  7.82 12.42\n#> 150: 15.30  5.40  9.18\n```\n:::\n\n\n另一种常用的非线性特征是基于自然样条的样条特征（暂时没有专门的PipeOp）：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npop = po(\"modelmatrix\", formula = ~ splines::ns(x1, 5))\npop$train(list(task))[[1]]$data()\n#>        Species (Intercept) splines::ns(x1, 5)1 splines::ns(x1, 5)2\n#>   1:    setosa           1        3.094150e-01        0.0009968102\n#>   2:    setosa           1        1.318681e-01        0.0000000000\n#>   3:    setosa           1        3.907204e-02        0.0000000000\n#>   4:    setosa           1        1.648352e-02        0.0000000000\n#>   5:    setosa           1        2.094017e-01        0.0000000000\n#>  ---                                                              \n#> 146: virginica           1        0.000000e+00        0.3024574669\n#> 147: virginica           1        1.812956e-02        0.6788378608\n#> 148: virginica           1        1.362101e-05        0.4802626315\n#> 149: virginica           1        5.579165e-02        0.7426950594\n#> 150: virginica           1        3.630706e-01        0.6033287054\n#>      splines::ns(x1, 5)3 splines::ns(x1, 5)4 splines::ns(x1, 5)5\n#>   1:         -0.20323541          0.46832508        -0.265089667\n#>   2:         -0.21657099          0.49905488        -0.282483895\n#>   3:         -0.17532519          0.40401021        -0.228685025\n#>   4:         -0.13961683          0.32172574        -0.182108907\n#>   5:         -0.21746675          0.50111903        -0.283652281\n#>  ---                                                            \n#> 146:          0.52049703          0.20822647        -0.031180964\n#> 147:          0.26175304          0.08745429        -0.046174752\n#> 148:          0.42721751          0.15182156        -0.059315324\n#> 149:          0.17592803          0.05799882        -0.032413564\n#> 150:          0.01963969          0.01354992        -0.007669769\n```\n:::\n\n\n#### 计算新特征\n\n管道运算“mutate”，根据以公式形式给出的表达式添加特征，这些表达式可能取决于其他特征的值。这可以增加新的特征，也可以改变现有的特征。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npom = po(\"mutate\", mutation = list(\n  x1_p = ~ x1 + 1,\n  Area1 = ~ x1 * x2,\n  Area2 = ~ x3 * x4,\n  Area = ~ Area1 + Area2\n))\npom$train(list(task))[[1]]$data()\n#>        Species  x1  x2  x3  x4 x1_p Area1 Area2  Area\n#>   1:    setosa 5.1 3.5 1.4 0.2  6.1 17.85  0.28 18.13\n#>   2:    setosa 4.9 3.0 1.4 0.2  5.9 14.70  0.28 14.98\n#>   3:    setosa 4.7 3.2 1.3 0.2  5.7 15.04  0.26 15.30\n#>   4:    setosa 4.6 3.1 1.5 0.2  5.6 14.26  0.30 14.56\n#>   5:    setosa 5.0 3.6 1.4 0.2  6.0 18.00  0.28 18.28\n#>  ---                                                 \n#> 146: virginica 6.7 3.0 5.2 2.3  7.7 20.10 11.96 32.06\n#> 147: virginica 6.3 2.5 5.0 1.9  7.3 15.75  9.50 25.25\n#> 148: virginica 6.5 3.0 5.2 2.0  7.5 19.50 10.40 29.90\n#> 149: virginica 6.2 3.4 5.4 2.3  7.2 21.08 12.42 33.50\n#> 150: virginica 5.9 3.0 5.1 1.8  6.9 17.70  9.18 26.88\n```\n:::\n\n\n利用正则表达式从复杂字符串列提取出相关信息，并转化为因子特征：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npo_ftextract = po(\"mutate\", mutation = list(\n  fare_per_person = ~ fare / (parch + sib_sp + 1),\n  deck = ~ factor(str_sub(cabin, 1, 1)),\n  title = ~ factor(str_extract(name, \"(?<=, ).*(?=\\\\.)\")),\n  surname = ~ factor(str_extract(name, \"^.*(?=,)\")),\n  ticket_prefix = ~ factor(str_extract(ticket, \"^.*(?= )\"))\n))\npo_ftextract$train(list(tsk(\"titanic\")))[[1]]$data()\n#>       survived  age cabin embarked     fare\n#>    1:       no 22.0  <NA>        S   7.2500\n#>    2:      yes 38.0   C85        C  71.2833\n#>    3:      yes 26.0  <NA>        S   7.9250\n#>    4:      yes 35.0  C123        S  53.1000\n#>    5:       no 35.0  <NA>        S   8.0500\n#>   ---                                      \n#> 1305:     <NA>   NA  <NA>        S   8.0500\n#> 1306:     <NA> 39.0  C105        C 108.9000\n#> 1307:     <NA> 38.5  <NA>        S   7.2500\n#> 1308:     <NA>   NA  <NA>        S   8.0500\n#> 1309:     <NA>   NA  <NA>        C  22.3583\n#>                                                      name parch pclass    sex\n#>    1:                             Braund, Mr. Owen Harris     0      3   male\n#>    2: Cumings, Mrs. John Bradley (Florence Briggs Thayer)     0      1 female\n#>    3:                              Heikkinen, Miss. Laina     0      3 female\n#>    4:        Futrelle, Mrs. Jacques Heath (Lily May Peel)     0      1 female\n#>    5:                            Allen, Mr. William Henry     0      3   male\n#>   ---                                                                        \n#> 1305:                                  Spector, Mr. Woolf     0      3   male\n#> 1306:                        Oliva y Ocana, Dona. Fermina     0      1 female\n#> 1307:                        Saether, Mr. Simon Sivertsen     0      3   male\n#> 1308:                                 Ware, Mr. Frederick     0      3   male\n#> 1309:                            Peter, Master. Michael J     1      3   male\n#>       sib_sp             ticket fare_per_person deck  title       surname\n#>    1:      1          A/5 21171        3.625000 <NA>     Mr        Braund\n#>    2:      1           PC 17599       35.641650    C    Mrs       Cumings\n#>    3:      0   STON/O2. 3101282        7.925000 <NA>   Miss     Heikkinen\n#>    4:      1             113803       26.550000    C    Mrs      Futrelle\n#>    5:      0             373450        8.050000 <NA>     Mr         Allen\n#>   ---                                                                    \n#> 1305:      0          A.5. 3236        8.050000 <NA>     Mr       Spector\n#> 1306:      0           PC 17758      108.900000    C   Dona Oliva y Ocana\n#> 1307:      0 SOTON/O.Q. 3101262        7.250000 <NA>     Mr       Saether\n#> 1308:      0             359309        8.050000 <NA>     Mr          Ware\n#> 1309:      1               2668        7.452767 <NA> Master         Peter\n#>       ticket_prefix\n#>    1:           A/5\n#>    2:            PC\n#>    3:      STON/O2.\n#>    4:          <NA>\n#>    5:          <NA>\n#>   ---              \n#> 1305:          A.5.\n#> 1306:            PC\n#> 1307:    SOTON/O.Q.\n#> 1308:          <NA>\n#> 1309:          <NA>\n```\n:::\n\n\n若数据噪声太多的问题，通常就需要做数据平滑。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npom = po(\"mutate\", mutation = list(\n  x1_s = ~ slider::slide_dbl(x1, mean, .before = 2, .after = 2) # 五点移动平均\n))\ndat = pom$train(list(task = task))[[1]]$data()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(patchwork)\np1 = ggplot(dat, aes(1:150, x1)) + geom_line()\np2 = ggplot(dat, aes(1:150, x1_s)) + geom_line()\np1 / p2\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-106-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# colapply：应用函数到任务的每一列，常用于类型转换\npoca = po(\"colapply\", applicator = as.character)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# renamecolumns：修改列名\npop = po(\"renamecolumns\", param_vals = list(\n  renaming = c(\"Petal.Length\" = \"PL\")\n))\n```\n:::\n\n\n#### 正态性变换\n\nBox-Cox 变换是更神奇的正态性变换，用最大似然估计选择最优的 $\\lambda$ 值，让非负的非正态数据变成正态数据：\n\n$$\ny' = \n\\begin{cases}\n\\ln y  & \\lambda = 0 \\\\\n(y^{\\lambda} - 1)/ \\lambda  & \\lambda \\neq 0\n\\end{cases}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npop = po(\"boxcox\")\npop$train(list(task))[[1]]$data()\n#>        Species         x1          x2         x3         x4\n#>   1:    setosa -0.8917547  1.01831791 -1.3431567 -1.3850773\n#>   2:    setosa -1.1812229 -0.08167295 -1.3431567 -1.3850773\n#>   3:    setosa -1.4845435  0.37307046 -1.4033413 -1.3850773\n#>   4:    setosa -1.6417967  0.14833599 -1.2832670 -1.3850773\n#>   5:    setosa -1.0348319  1.22454068 -1.3431567 -1.3850773\n#>  ---                                                       \n#> 146: virginica  1.0385560 -0.08167295  0.8174171  1.2930924\n#> 147: virginica  0.6097200 -1.32264877  0.7075555  0.9020852\n#> 148: virginica  0.8279148 -0.08167295  0.8174171  1.0023867\n#> 149: virginica  0.4976284  0.80781419  0.9269887  1.2930924\n#> 150: virginica  0.1485189 -0.08167295  0.7625234  0.7998822\n```\n:::\n\n\n若数据包含 0 或负数，则 Box-Cox 变换不再适用，可以改用同样原理的 Yeo-Johnson 变换：\n\n$$\ny' = \n\\begin{cases}\n\\ln (y+1)                                 & \\lambda = 0, y \\geq 0\\\\\n\\frac{(y + 1)^{\\lambda} - 1}{\\lambda}     & \\lambda \\neq 0, y \\geq 0\\\\\n- \\ln (1-y)                               & \\lambda = 2, y < 0\\\\\n\\frac{(1-y)^{2-\\lambda} - 1}{\\lambda - 2} & \\lambda \\neq 2, y < 0\n\\end{cases}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npop = po(\"yeojohnson\")\npop$train(list(task))[[1]]$data()\n#>        Species         x1          x2         x3         x4\n#>   1:    setosa -0.8926989  1.01949272 -1.3278574 -1.3278174\n#>   2:    setosa -1.1812158 -0.08164367 -1.3278574 -1.3278174\n#>   3:    setosa -1.4829526  0.37387369 -1.3813346 -1.3278174\n#>   4:    setosa -1.6391179  0.14878429 -1.2741721 -1.3278174\n#>   5:    setosa -1.0353690  1.22553197 -1.3278574 -1.3278174\n#>  ---                                                       \n#> 146: virginica  1.0393000 -0.08164367  0.8157632  1.4105911\n#> 147: virginica  0.6095086 -1.32389503  0.6988626  0.9184998\n#> 148: virginica  0.8281903 -0.08164367  0.8157632  1.0424849\n#> 149: virginica  0.4971768  0.80900587  0.9330158  1.4105911\n#> 150: virginica  0.1474204 -0.08164367  0.7572682  0.7938307\n```\n:::\n\n\n#### 连续变量分箱\n\n在统计和机器学习中，有时需要将连续变量转化为离散变量，称为连续变量离散化或分箱，常用于银行风控建模，特别是线性回归或 Logistic 回归模型。\n\n分箱的好处有：\n\n1. 使得结果更便于分析和解释。比如，年龄从中年到老年，患高血压比例增加25%，而年龄每增加一岁，患高血压比例不一定有显著变化；\n\n1. 简化模型，将自变量与因变量间非线性的潜在的关系，转化为简单的线性关系。当然，分箱也可能带来问题：简化的模型关系可能与潜在的模型关系不一致（甚至发现的是错误的模型关系）、删除数据中的细微差别、切分点可能没有实际意义。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 等宽分箱\npop = po(\"histbin\", param_vals = list(breaks = 4))\npop$train(list(task))[[1]]$data()\n#>        Species       x1         x2       x3         x4\n#>   1:    setosa    (5,6]    (3,3.5] [-Inf,2] [-Inf,0.5]\n#>   2:    setosa [-Inf,5]    (2.5,3] [-Inf,2] [-Inf,0.5]\n#>   3:    setosa [-Inf,5]    (3,3.5] [-Inf,2] [-Inf,0.5]\n#>   4:    setosa [-Inf,5]    (3,3.5] [-Inf,2] [-Inf,0.5]\n#>   5:    setosa [-Inf,5]    (3.5,4] [-Inf,2] [-Inf,0.5]\n#>  ---                                                  \n#> 146: virginica    (6,7]    (2.5,3]    (4,6]   (2, Inf]\n#> 147: virginica    (6,7] [-Inf,2.5]    (4,6]    (1.5,2]\n#> 148: virginica    (6,7]    (2.5,3]    (4,6]    (1.5,2]\n#> 149: virginica    (6,7]    (3,3.5]    (4,6]   (2, Inf]\n#> 150: virginica    (5,6]    (2.5,3]    (4,6]    (1.5,2]\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 分位数分箱\npop = po(\"quantilebin\", numsplits = 4)\npop$train(list(task))[[1]]$data()\n#>        Species         x1         x2         x3         x4\n#>   1:    setosa (-Inf,5.1] (3.3, Inf] (-Inf,1.6] (-Inf,0.3]\n#>   2:    setosa (-Inf,5.1]    (2.8,3] (-Inf,1.6] (-Inf,0.3]\n#>   3:    setosa (-Inf,5.1]    (3,3.3] (-Inf,1.6] (-Inf,0.3]\n#>   4:    setosa (-Inf,5.1]    (3,3.3] (-Inf,1.6] (-Inf,0.3]\n#>   5:    setosa (-Inf,5.1] (3.3, Inf] (-Inf,1.6] (-Inf,0.3]\n#>  ---                                                      \n#> 146: virginica (6.4, Inf]    (2.8,3] (5.1, Inf] (1.8, Inf]\n#> 147: virginica  (5.8,6.4] (-Inf,2.8] (4.35,5.1] (1.8, Inf]\n#> 148: virginica (6.4, Inf]    (2.8,3] (5.1, Inf] (1.8, Inf]\n#> 149: virginica  (5.8,6.4] (3.3, Inf] (5.1, Inf] (1.8, Inf]\n#> 150: virginica  (5.8,6.4]    (2.8,3] (4.35,5.1]  (1.3,1.8]\n```\n:::\n\n\n### 特征降维\n\n有时数据集可能包含过多特征，甚至是冗余特征，可以用降维技术进压缩特征，但通常会降低模型性能。\n\n#### PCA\n\n$n$ 个特征，若转化为 $n$ 个主成分，则会保留原始数据的 100% 信息，但这就失去了降维的意义。所以1一般是只选择前若干个主成分，一般原则是选择至保留 85% 以上信息的主成分。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npop = po(\"pca\", rank. = 2)\npop$train(list(task))[[1]]$data()\n#>        Species       PC1         PC2\n#>   1:    setosa -2.684126 -0.31939725\n#>   2:    setosa -2.714142  0.17700123\n#>   3:    setosa -2.888991  0.14494943\n#>   4:    setosa -2.745343  0.31829898\n#>   5:    setosa -2.728717 -0.32675451\n#>  ---                                \n#> 146: virginica  1.944110 -0.18753230\n#> 147: virginica  1.527167  0.37531698\n#> 148: virginica  1.764346 -0.07885885\n#> 149: virginica  1.900942 -0.11662796\n#> 150: virginica  1.390189  0.28266094\n```\n:::\n\n\n#### 核 PCA\n\nPCA 适用于数据的线性降维。而核主成分分析（Kernel PCA, KPCA）可实现数据的非线性降维，用于处理线性不可分的数据集。\n\nKPCA 的大致思路是：对于输入空间中的矩阵 $X$，先用一个非线性映射把 $X$ 中的所有样本映射到一个高维甚至是无穷维的特征空间（使其线性可分），然后在这个高维空间进行 PCA 降维。\n\n#### ICA：独立成分分析\n\n独立成分分析，提取统计意义上的独立成分：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npop = po(\"ica\", n.comp = 3)\npop$train(list(task))[[1]]$data()\n#>        Species         V1         V2         V3\n#>   1:    setosa  0.1721410  0.4149772 -1.3952557\n#>   2:    setosa  0.2214486 -0.7872973 -1.3361069\n#>   3:    setosa -0.4563409 -0.3146524 -1.3308693\n#>   4:    setosa -0.6610660 -0.5861230 -1.2029337\n#>   5:    setosa -0.1833669  0.6446961 -1.3678177\n#>  ---                                           \n#> 146: virginica -0.1793776  0.7197010  0.9500885\n#> 147: virginica  0.1249361 -0.8278466  0.7937129\n#> 148: virginica -0.1614076  0.4371009  0.8769270\n#> 149: virginica -1.9147519  1.6044931  1.1871656\n#> 150: virginica -1.2683559  0.2261212  0.9094067\n```\n:::\n\n\n#### NMF：非负矩阵分解\n\n对于任意给定的非负矩阵 $V$，NMF 算法能够寻找到非负矩阵 $W$ 和非负矩阵 $H$，使它们的积为矩阵。非负矩阵分解的方法在保证矩阵的非负性的同时能够减少数据量，相当于把 $n$ 维的数据降维到 $r$ 维。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# BiocManager::install(\"Biobase\")\npop = po(\"nmf\", rank = 3)\npop$train(list(task))[[1]]$data()\n#> Registered S3 methods overwritten by 'registry':\n#>   method               from \n#>   print.registry_field proxy\n#>   print.registry_entry proxy\n#>        Species       NMF1      NMF2         NMF3\n#>   1:    setosa 0.08946447 0.4806157  0.039331051\n#>   2:    setosa 0.14847785 0.4012262 -0.003756253\n#>   3:    setosa 0.08482244 0.4394040  0.037395481\n#>   4:    setosa 0.10090716 0.4121730  0.052304626\n#>   5:    setosa 0.06054158 0.4968302  0.064062112\n#>  ---                                            \n#> 146: virginica 0.26134716 0.3282638  0.600819824\n#> 147: virginica 0.34676264 0.2323164  0.469663330\n#> 148: virginica 0.28374883 0.3073426  0.552988371\n#> 149: virginica 0.14504617 0.3818651  0.727874559\n#> 150: virginica 0.23580629 0.3002660  0.566502348\n```\n:::\n\n\n#### 剔除常量特征\n\n从任务中剔除常量特征。对于每个特征，计算不同于其众数值的比例。所有比例低于可设置阈值的特征都会从任务中剔除。缺失值可以被忽略，也可以视为与非缺失值不同的常规值。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata = data.table(y = runif(10), a = 1:10, b = rep(1, 10), \n                  c = rep(1:2, each = 5))\ntask_ex = as_task_regr(data, target = \"y\")\npo = po(\"removeconstants\")  # 剔除常量特征 b\npo$train(list(task_ex))[[1]]$data()\n#>               y  a c\n#>  1: 0.891268901  1 1\n#>  2: 0.002797802  2 1\n#>  3: 0.649207745  3 1\n#>  4: 0.561067845  4 1\n#>  5: 0.427392408  5 1\n#>  6: 0.387362043  6 2\n#>  7: 0.544585730  7 2\n#>  8: 0.622006226  8 2\n#>  9: 0.183312478  9 2\n#> 10: 0.342760530 10 2\n```\n:::\n\n\n### 分类特征\n\n#### 因子折叠\n\n管道运算“collapsefactors”，对因子或有序因子进行折叠，折叠训练样本中最少的水平，直到剩下 `target_level_count` 个水平。然而，那些出现率高于 `no_collapse_above_prevalence` 的水平会被保留。对于因子变量，它们被折叠到下一个更大的水平，对于有序变量，稀有变量被折叠到相邻的类别，以样本数较少者为准。\n\n训练集中没有出现的水平在预测集中不会被使用，因此经常与因子修正结合起来使用。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat = tibble(color = factor(starwars$skin_color), y = 1)\ndat %>% count(color)\n#> # A tibble: 31 × 2\n#>    color                   n\n#>    <fct>               <int>\n#>  1 blue                    2\n#>  2 blue, grey              2\n#>  3 brown                   4\n#>  4 brown mottle            1\n#>  5 brown, white            1\n#>  6 dark                    6\n#>  7 fair                   17\n#>  8 fair, green, yellow     1\n#>  9 gold                    1\n#> 10 green                   6\n#> # ℹ 21 more rows\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = as_task_regr(dat, target = \"y\")\npoc = po(\"collapsefactors\", target_level_count = 5)\npoc$train(list(task))[[1]]$data() %>% count(color)\n#>    color  n\n#> 1:  dark  6\n#> 2:  fair 17\n#> 3: green  6\n#> 4:  grey 47\n#> 5: light 11\n```\n:::\n\n\n#### 因子修正\n\n管道运算“fixfactors”（参数：`droplevels = TRUE`）确保预测过程中的因子水平与训练过程中的相同；可能会在之前丢弃空的训练因子水平。\n\n注意，如果发现未见过的因子水平，这可能会在预测期间引入缺失值。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndattrain = data.table(\n  a = factor(c(\"a\", \"b\", \"c\", NA), levels = letters),\n  b = ordered(c(\"a\", \"b\", \"c\", NA)),\n  target = 1:4\n)\ndattrain\n#>       a    b target\n#> 1:    a    a      1\n#> 2:    b    b      2\n#> 3:    c    c      3\n#> 4: <NA> <NA>      4\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndattest = data.table(\n  a = factor(c(\"a\", \"b\", \"c\", \"d\"), levels = letters[10:1]),\n  b = ordered(c(\"a\", \"b\", \"c\", \"d\")),\n  target = 1:4\n)\ndattest\n#>    a b target\n#> 1: a a      1\n#> 2: b b      2\n#> 3: c c      3\n#> 4: d d      4\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask_train = as_task_regr(dattrain, \"target\")\ntask_test = as_task_regr(dattest, \"target\")\npo = po(\"fixfactors\")\npo$train(list(task_train))\n#> $output\n#> <TaskRegr:dattrain> (4 x 3)\n#> * Target: target\n#> * Properties: -\n#> * Features (2):\n#>   - fct (1): a\n#>   - ord (1): b\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npo$predict(list(task_test))[[1]]$data()\n#>    target    a    b\n#> 1:      1    a    a\n#> 2:      2    b    b\n#> 3:      3    c    c\n#> 4:      4 <NA> <NA>\n```\n:::\n\n\n#### 因子编码\n\n对因子、有序因子、字符特征进行编码。参数 `method` 指定编码方法：\n\n- “one-hot”：独热编码；\n\n- “treatment”：虚拟编码，创建 $n-1$ 列，留出每个因子变量的第一个因子水平（见`stats::conc.treatment()`）；\n\n- “helmert”：根据 Helmert 对比度创建列（见`stats::conc.helmert()`）；\n\n- “poly”：根据正交多项式创建对比列（见`stats::conc.poly()`）；\n\n- “sum”：创建对比度相加为零的列，（见`stats::conc.sum()`）\n\n新创建的列按模式 `[column-name].[x]` 命名，其中 `x` 是 ”one-hot” 和 ”treatment” 编码的各自因子水平，否则是一个整数序列。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"penguins\")\npoe = po(\"encode\", method = \"one-hot\")   # 独热编码\npoe$train(list(task))[[1]]$data()[, 7:11]\n#>      island.Biscoe island.Dream island.Torgersen sex.female sex.male\n#>   1:             0            0                1          0        1\n#>   2:             0            0                1          1        0\n#>   3:             0            0                1          1        0\n#>   4:             0            0                1         NA       NA\n#>   5:             0            0                1          1        0\n#>  ---                                                                \n#> 340:             0            1                0          0        1\n#> 341:             0            1                0          1        0\n#> 342:             0            1                0          0        1\n#> 343:             0            1                0          0        1\n#> 344:             0            1                0          1        0\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoe = po(\"encode\", method = \"treatment\")  # 虚拟编码\npoe$train(list(task))[[1]]$data()[, 7:9]\n#>      island.Dream island.Torgersen sex.male\n#>   1:            0                1        1\n#>   2:            0                1        0\n#>   3:            0                1        0\n#>   4:            0                1       NA\n#>   5:            0                1        0\n#>  ---                                       \n#> 340:            1                0        1\n#> 341:            1                0        0\n#> 342:            1                0        1\n#> 343:            1                0        1\n#> 344:            1                0        0\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoe = po(\"encode\", method = \"helmert\")  # Helmert 编码\npoe$train(list(task))[[1]]$data()[, 7:9]\n#>      island.1 island.2 sex.1\n#>   1:        0        2     1\n#>   2:        0        2    -1\n#>   3:        0        2    -1\n#>   4:        0        2    NA\n#>   5:        0        2    -1\n#>  ---                        \n#> 340:        1       -1     1\n#> 341:        1       -1    -1\n#> 342:        1       -1     1\n#> 343:        1       -1     1\n#> 344:        1       -1    -1\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoe = po(\"encode\", method = \"poly\")  # 多项编码\npoe$train(list(task))[[1]]$data()[, 7:9]\n#>           island.1   island.2      sex.1\n#>   1:  7.071068e-01  0.4082483  0.7071068\n#>   2:  7.071068e-01  0.4082483 -0.7071068\n#>   3:  7.071068e-01  0.4082483 -0.7071068\n#>   4:  7.071068e-01  0.4082483         NA\n#>   5:  7.071068e-01  0.4082483 -0.7071068\n#>  ---                                    \n#> 340: -7.850462e-17 -0.8164966  0.7071068\n#> 341: -7.850462e-17 -0.8164966 -0.7071068\n#> 342: -7.850462e-17 -0.8164966  0.7071068\n#> 343: -7.850462e-17 -0.8164966  0.7071068\n#> 344: -7.850462e-17 -0.8164966 -0.7071068\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoe = po(\"encode\", method = \"sum\")  # sum 编码\npoe$train(list(task))[[1]]$data()[, 7:9]\n#>      island.1 island.2 sex.1\n#>   1:       -1       -1    -1\n#>   2:       -1       -1     1\n#>   3:       -1       -1     1\n#>   4:       -1       -1    NA\n#>   5:       -1       -1     1\n#>  ---                        \n#> 340:        0        1    -1\n#> 341:        0        1     1\n#> 342:        0        1    -1\n#> 343:        0        1    -1\n#> 344:        0        1     1\n```\n:::\n\n\n::: {.callout-warning}\n以下内容暂缓：\n\n- 效应编码\n\n- 日期时间特征\n\n- 文本特征\n:::\n\n### 处理不均衡数据\n\n通过采样对任务进行类平衡，可能有利于对不平衡的训练数据进行分类，采样只发生在训练阶段。\n\n#### 欠采样与过采样\n\n欠采样：只保留多数类的一部分行；过采样：对少数类进行超量采样（重复数据点）。\n\nPipeOp 名字为 ”classbalancing”，参数：\n\n- ratio：相对于 `$reference` 值，要保留的类的行数的比率，默认为 1；\n\n- reference：`$ratio` 的值是根据什么来衡量的。可以是 “all”（默认值，所有类的平均实例数），“major”（拥有最多实例的类的实例数），“minor”（拥有最少实例的类的实例数），“nonmajor”（除主要类外所有类的平均实例数），“nonminor”（除次要类外所有类的平均实例数），以及 “one”（`$ratio` 决定每个类的实例数）；\n\n- adjust：哪些类要向上/向下采样。可以是 “all”（默认）、“major”、“minor”、“nonmajor”、“nonminor”、“upsample”（只过采样）和“downsample”（只欠采样）；\n\n- shuffle：是否对结果任务的行进行洗牌，默认为 `TRUE`。如果数据被过采样且 `shuffle = FALSE`，结果任务将以原始顺序显示原始行（在欠采样中不被移除），然后是所有新添加的行，按目标类别排序。\n\n过/欠采样的过程如下：\n\n- 首先，计算” 目标类计数”，方法是取 `reference` 参数所指示的所有类的平均数（例如，如果 reference 参数是 ”nonmajor”：所有不是 “major” 类的类的平均数，即拥有最多样本的类），然后将其与比率参数的值相乘。如果 reference 是 ”one”，那么“目标类计数” 就是比率的值（即 1 * 比率）。\n\n- 然后，对于每个被 `adjust` 参数引用的类（例如，若调整是“nonminor”：每个不是样本最少的类），PipeOpClassBalancing 要么抛出样本（欠采样），要么增加与随机选择的样本相等的额外行（过采样），直到这些类的样本数等于“目标类计数”。\n\n- 使用 `task$filter()` 来删除行。当在过采样过程中添加相同的行时，那么由于[混淆]，不能使用 `task$row_roles$use` 来复制行；而是使用 `task$rbind()` 函数，并附加一个新的 data.table，其中包含所有被复制的行，其次数正好与被添加的行相同。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"german_credit\")\ntable(task$truth())\n#> \n#> good  bad \n#>  700  300\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## 欠采样\nopb_down = po(\"classbalancing\", reference = \"minor\", adjust = \"major\")\n# 默认ratio = 1, 若ratio = 2, 结果是600 good, 300 bad\nresult = opb_down$train(list(task))[[1]]\ntable(result$truth())\n#> \n#> good  bad \n#>  300  300\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## 过采样\nopb_up = po(\"classbalancing\", reference = \"major\", adjust = \"minor\")\n# 默认ratio = 1, 若ratio = 2, 结果是700 good, 1400 bad\nresult = opb_up$train(list(task))[[1]]\ntable(result$truth())\n#> \n#> good  bad \n#>  700  700\n```\n:::\n\n\n#### SMOTE 法\n\n用 SMOTE 算法创建少数类别的合成观测，生成一个更平衡的数据集。该算法为每个少数类观测取样，基于该观测的 $K$ 个最近邻居生成新观测。它只能应用于具有纯数值特征的任务。详见 `smotefamily::SMOTE`。\n\nPipeOp 名字：“smote”，其参数（具体参阅 `SMOTE()`）：\n\n- K：用于抽取新值的近邻的数量；\n\n- dup_size：合成的少数实例在原始多数实例数量上的期望次数。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 只支持double 型特征, 需安装 smotefamily 包\npop = po(\"colapply\", applicator = as.numeric,\n         affect_columns = selector_type(\"integer\")) %>>%\n  po(\"encodeimpact\") %>>%\n  po(\"smote\", K = 5, dup_size = 1)  # 少数类增加 1 倍\nresult = pop$train(task)[[1]]\ntable(result$truth())\n#> \n#> good  bad \n#>  700  600\n```\n:::\n\n\n### 目标变换\n\n为了提高预测能力，对于异方差或右偏的因变量数据，经常需要做取对数变换、或 Box-Cox 变换，以变成正态数据，再进行回归建模，预测值要回到原数据量级，还要做逆变换。\n\nmlr3pipelines 包还提供了目标变换管道，将目标变换及其逆变换，都封装到图学习器，这样就省了手动做两次变换，还能整体进行调参或基准测试。\n\n对于方差逐渐变大的异方差的时间序列数据，或右偏分布的数据，可以尝试做对数变换或开平方变换，以稳定方差和变成正态分布。\n\n对数变换特别有用，因为具有可解释性：**对数值的变化是原始尺度上的相对（百分比）变化**。若使用以 10 为底的对数，则对数刻度上每增加 1 对应原始刻度上的乘以 10。\n\n注意，原始数据若存在零或负，则不能取对数或开根号，解决办法是做平移：$a = \\max \\{ 0, -\\min \\{x_i\\} + \\varepsilon \\}$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"mtcars\")\nlearner = lrn(\"regr.lm\")\ng_ppl = ppl(\"targettrafo\", graph = learner)\ng_ppl$param_set$values$targetmutate.trafo = function(x) log(x)\ng_ppl$param_set$values$targetmutate.inverter = function(x) list(response = exp(x$response))\ng_ppl$plot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-132-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngl = as_learner(g_ppl)\ngl$train(task)\ngl$predict(task)\n#> <PredictionRegr> for 32 observations:\n#>     row_ids truth response\n#>           1  21.0 21.67976\n#>           2  21.0 21.10831\n#>           3  22.8 25.73690\n#> ---                       \n#>          30  19.7 19.58533\n#>          31  15.0 14.11015\n#>          32  21.4 23.11105\n```\n:::\n\n\n# 嵌套重抽样\n\n构建模型，是如何从一组潜在的候选模型（如不同的算法，不同的超参数，不同的特征子集）中选择最佳模型。在构建模型过程中所使用的重抽样划分，不应该原样用来评估最终选择模型的性能。\n\n通过在相同的测试集或相同的 CV 划分上反复评估学习器，测试集的信息会“泄露”到评估中，导致最终的性能估计偏于乐观。\n\n模型构建的所有部分（包括模型选择、预处理）都应该纳入到训练数据的模型寻找过程中。测试集应该只使用一次，测试集只有在模型完全训练好之后才能被使用，例如已确定好了超参数。这样从测试集获得的性能才是真实性能的无偏估计。\n\n对于本身需要重抽样的步骤（如超参数调参），这需要两个嵌套的重抽样循环，即内层调参和外层评估都需要重抽样策略。\n\n下面从实例来看：\n\n1. 使用3 折交叉验证来获得不同的非测试集和测试集（外层重抽样）\n\n2. 在非测试集上使用 4 折交叉验证来获得不同的内层训练集和内层测试集（内层重抽样）\n\n3. 使用内层数据划分来做超参数调参\n\n4. 使用经过内层重抽样调参的最优超参数在外层非测试集上拟合学习器\n\n5. 评估学习器在外层测试集上的性能\n\n6. 对外层重抽样的三折中的每一折重复执行 2 - 5\n\n7. 三个性能值被汇总为一个无偏的性能估计\n\n过程看起来复杂，实现非常简单，只需要将内层重抽样放在自动调参器，外层重抽样放在 `resample()` 中，其中的学习器换成自动调参器。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 内层重抽样（超参数调参）\nat = auto_tuner(\n  tuner = tnr(\"grid_search\", resolution = 10),\n  learner = lrn(\"classif.rpart\", cp = to_tune(lower = .001, upper = .1)),\n  resampling = rsmp(\"cv\", folds = 4),\n  measure = msr(\"classif.acc\"),\n  term_evals = 5\n)\n\n# 外层重抽样\ntask = tsk(\"pima\")\nouter_resampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, at, outer_resampling, store_models = TRUE)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 提取内层重抽样结果，查看最优的超参数\nextract_inner_tuning_results(rr)\n#>    iteration    cp classif.acc learner_param_vals  x_domain task_id\n#> 1:         1 0.067   0.7480469          <list[2]> <list[1]>    pima\n#> 2:         2 0.012   0.7578125          <list[2]> <list[1]>    pima\n#> 3:         3 0.012   0.7343750          <list[2]> <list[1]>    pima\n#>             learner_id resampling_id\n#> 1: classif.rpart.tuned            cv\n#> 2: classif.rpart.tuned            cv\n#> 3: classif.rpart.tuned            cv\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 查看外层重抽样的每次结果\nrr$score()\n#>    task_id          learner_id resampling_id iteration classif.ce\n#> 1:    pima classif.rpart.tuned            cv         1  0.2968750\n#> 2:    pima classif.rpart.tuned            cv         2  0.2695312\n#> 3:    pima classif.rpart.tuned            cv         3  0.2578125\n#> Hidden columns: task, learner, resampling, prediction\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 查看外层重抽样的平均模型性能\nrr$aggregate()\n#> classif.ce \n#>  0.2747396\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 用全部数据自动调参，预测新数据\nat$train(task)\ndat = task$data()[1:5, -1]\nat$predict_newdata(dat)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 若改用tune_nested() 函数，嵌套重抽样过程还可以进一步简化\nrr = tune_nested(\n  tuner = tnr(\"grid_search\", resolution = 10),\n  task = task,\n  learner = lrn(\"classif.rpart\", cp = to_tune(lower = .001, upper = .1)),\n  inner_resampling = rsmp(\"cv\", folds = 4),\n  outer_resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.acc\"),\n  term_evals = 5\n)\n```\n:::\n\n\n# 超参数调参\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.callout-tip title=\"To be continued\"}\n- 超参数调参\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}