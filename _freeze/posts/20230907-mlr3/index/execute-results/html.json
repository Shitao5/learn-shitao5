{
  "hash": "5f57e9ca3bb1dbde1a978955233a95de",
  "result": {
    "markdown": "---\ntitle: \"mlr3verse 技术手册\"\ndate: \"2023-09-07\"\ndate-modified: \"2023-09-12\"\nimage: \"cover.jpg\"\ncategories: \n  - Machine Learning\n  - R\n  - mlr3\n---\n\n\n::: {.callout-note title='Progress'}\nLearning Progress: Completed.🎊\n:::\n\n::: {.callout-tip title=\"Learning Source\"}\n- 张敬信老师 QQ 群（222427909）文件\n- <https://github.com/zhjx19/RConf15/tree/main>\n:::\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n```\n:::\n\n\n# 基础知识\n\n## 任务：封装数据\n\n任务是表格数据的封装，自变量为特征，因变量为目标或结果变量。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsks() # 查看所有自带任务\n#> <DictionaryTask> with 21 stored values\n#> Keys: ames_housing, bike_sharing, boston_housing, breast_cancer,\n#>   german_credit, ilpd, iris, kc_housing, moneyball, mtcars, optdigits,\n#>   penguins, penguins_simple, pima, ruspini, sonar, spam, titanic,\n#>   usarrests, wine, zoo\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 创建任务\ndat = tsk(\"german_credit\")$data() # 提取数据\ntask = as_task_classif(dat, target = \"credit_risk\")\ntask\n#> <TaskClassif:dat> (1000 x 21)\n#> * Target: credit_risk\n#> * Properties: twoclass\n#> * Features (20):\n#>   - fct (14): credit_history, employment_duration, foreign_worker,\n#>     housing, job, other_debtors, other_installment_plans,\n#>     people_liable, personal_status_sex, property, purpose, savings,\n#>     status, telephone\n#>   - int (3): age, amount, duration\n#>   - ord (3): installment_rate, number_credits, present_residence\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 选择特征\ntask$select(cols = setdiff(task$feature_names, \"telephone\"))\ntask\n#> <TaskClassif:dat> (1000 x 20)\n#> * Target: credit_risk\n#> * Properties: twoclass\n#> * Features (19):\n#>   - fct (13): credit_history, employment_duration, foreign_worker,\n#>     housing, job, other_debtors, other_installment_plans,\n#>     people_liable, personal_status_sex, property, purpose, savings,\n#>     status\n#>   - int (3): age, amount, duration\n#>   - ord (3): installment_rate, number_credits, present_residence\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 划分训练集、测试集\nset.seed(1)\nsplit = partition(task, ratio = .7)\n```\n:::\n\n\n`stratify = TRUE` 默认按目标变量分层，得到训练集和测试集的索引（行号）。\n\n## 学习器：封装算法\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrns()  # 查看所有自带学习器名字\n#> <DictionaryLearner> with 46 stored values\n#> Keys: classif.cv_glmnet, classif.debug, classif.featureless,\n#>   classif.glmnet, classif.kknn, classif.lda, classif.log_reg,\n#>   classif.multinom, classif.naive_bayes, classif.nnet, classif.qda,\n#>   classif.ranger, classif.rpart, classif.svm, classif.xgboost,\n#>   clust.agnes, clust.ap, clust.cmeans, clust.cobweb, clust.dbscan,\n#>   clust.diana, clust.em, clust.fanny, clust.featureless, clust.ff,\n#>   clust.hclust, clust.kkmeans, clust.kmeans, clust.MBatchKMeans,\n#>   clust.mclust, clust.meanshift, clust.pam, clust.SimpleKMeans,\n#>   clust.xmeans, regr.cv_glmnet, regr.debug, regr.featureless,\n#>   regr.glmnet, regr.kknn, regr.km, regr.lm, regr.nnet, regr.ranger,\n#>   regr.rpart, regr.svm, regr.xgboost\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 选择随机森林分类学习器\nlearner = lrn(\"classif.ranger\", num.trees = 100, predict_type = \"prob\")\nlearner\n#> <LearnerClassifRanger:classif.ranger>\n#> * Model: -\n#> * Parameters: num.threads=1, num.trees=100\n#> * Packages: mlr3, mlr3learners, ranger\n#> * Predict Types:  response, [prob]\n#> * Feature Types: logical, integer, numeric, character, factor, ordered\n#> * Properties: hotstart_backward, importance, multiclass, oob_error,\n#>   twoclass, weights\n```\n:::\n\n\n学习器 `$model` 属性为 `NULL`，用 `$train()` 方法在训练集上训练模型，模型结果存入 `$model`，再用 `predict()` 方法在测试集上做预测，得到结果是 `Prediction` 对象。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner$train(task, row_ids = split$train)\nlearner$model\n#> Ranger result\n#> \n#> Call:\n#>  ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      probability = self$predict_type == \"prob\", case.weights = task$weights$weight,      num.threads = 1L, num.trees = 100L) \n#> \n#> Type:                             Probability estimation \n#> Number of trees:                  100 \n#> Sample size:                      700 \n#> Number of independent variables:  19 \n#> Mtry:                             4 \n#> Target node size:                 10 \n#> Variable importance mode:         none \n#> Splitrule:                        gini \n#> OOB prediction error (Brier s.):  0.1615879\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction = learner$predict(task, row_ids = split$test)\nprediction\n```\n:::\n\n\n## 性能评估\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmsrs() # 查看所有支持的性能度量指标\n#> <DictionaryMeasure> with 66 stored values\n#> Keys: aic, bic, classif.acc, classif.auc, classif.bacc, classif.bbrier,\n#>   classif.ce, classif.costs, classif.dor, classif.fbeta, classif.fdr,\n#>   classif.fn, classif.fnr, classif.fomr, classif.fp, classif.fpr,\n#>   classif.logloss, classif.mauc_au1p, classif.mauc_au1u,\n#>   classif.mauc_aunp, classif.mauc_aunu, classif.mbrier, classif.mcc,\n#>   classif.npv, classif.ppv, classif.prauc, classif.precision,\n#>   classif.recall, classif.sensitivity, classif.specificity, classif.tn,\n#>   classif.tnr, classif.tp, classif.tpr, clust.ch, clust.dunn,\n#>   clust.silhouette, clust.wss, debug_classif, oob_error, regr.bias,\n#>   regr.ktau, regr.mae, regr.mape, regr.maxae, regr.medae, regr.medse,\n#>   regr.mse, regr.msle, regr.pbias, regr.rae, regr.rmse, regr.rmsle,\n#>   regr.rrse, regr.rse, regr.rsq, regr.sae, regr.smape, regr.srho,\n#>   regr.sse, selected_features, sim.jaccard, sim.phi, time_both,\n#>   time_predict, time_train\n```\n:::\n\n\n用预测对象的 `$score()` 方法，计算该度量指标的得分：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction$score(msr(\"classif.acc\"))  # 准确率\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 绘制 ROC 曲线\nlibrary(precrec)\nautoplot(prediction, type = \"roc\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction$score(msr(\"classif.auc\"))  # auc 面积\n```\n:::\n\n\n## 重抽样\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrsmps()  # 查看所有支持的重抽样方法\n#> <DictionaryResampling> with 9 stored values\n#> Keys: bootstrap, custom, custom_cv, cv, holdout, insample, loo,\n#>   repeated_cv, subsampling\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv10 = rsmp(\"cv\", folds = 10)  # 10 折交叉验证\n```\n:::\n\n\n### 实例化重抽样对象\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv10$instantiate(task)  # 实例化\ncv10$iters  # 数据副本数\n#> [1] 10\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv10$train_set(1)  # 第 1 个数据副本的训练集索引\ncv10$test_set(1)   # 第 1 个数据副本的测试集索引\n```\n:::\n\n\n### 使用重抽样\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr =  resample(task, learner, cv10, store_models = TRUE)\n#> INFO  [15:10:52.117] [mlr3] Applying learner 'classif.ranger' on task 'dat' (iter 1/10)\n#> INFO  [15:10:52.368] [mlr3] Applying learner 'classif.ranger' on task 'dat' (iter 2/10)\n#> INFO  [15:10:52.429] [mlr3] Applying learner 'classif.ranger' on task 'dat' (iter 3/10)\n#> INFO  [15:10:52.485] [mlr3] Applying learner 'classif.ranger' on task 'dat' (iter 4/10)\n#> INFO  [15:10:52.543] [mlr3] Applying learner 'classif.ranger' on task 'dat' (iter 5/10)\n#> INFO  [15:10:52.603] [mlr3] Applying learner 'classif.ranger' on task 'dat' (iter 6/10)\n#> INFO  [15:10:52.659] [mlr3] Applying learner 'classif.ranger' on task 'dat' (iter 7/10)\n#> INFO  [15:10:52.717] [mlr3] Applying learner 'classif.ranger' on task 'dat' (iter 8/10)\n#> INFO  [15:10:52.774] [mlr3] Applying learner 'classif.ranger' on task 'dat' (iter 9/10)\n#> INFO  [15:10:52.836] [mlr3] Applying learner 'classif.ranger' on task 'dat' (iter 10/10)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr$aggregate(msr(\"classif.acc\"))  # 所有重抽样的平均准确率\n#> classif.acc \n#>       0.753\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr$score(msr(\"classif.acc\"))  # 各个重抽样的平均准确率\n#>     task_id     learner_id resampling_id iteration classif.acc\n#>  1:     dat classif.ranger            cv         1        0.77\n#>  2:     dat classif.ranger            cv         2        0.84\n#>  3:     dat classif.ranger            cv         3        0.70\n#>  4:     dat classif.ranger            cv         4        0.77\n#>  5:     dat classif.ranger            cv         5        0.76\n#>  6:     dat classif.ranger            cv         6        0.75\n#>  7:     dat classif.ranger            cv         7        0.71\n#>  8:     dat classif.ranger            cv         8        0.83\n#>  9:     dat classif.ranger            cv         9        0.66\n#> 10:     dat classif.ranger            cv        10        0.74\n#> Hidden columns: task, learner, resampling, prediction\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 查看第 1 个数据副本（第 1 折）上的结果\nrr$resampling$train_set(1)  # 第 1 折的训练集索引\nrr$learners[[1]]$model      # 第 1 折学习器的拟合模型\nrr$predictions()[[1]]       # 第 1 折的预测结果\n```\n:::\n\n\n## 基准测试\n\n基准测试（benchmark）用来比较不同学习器（算法）、在多个任务（数据）和/或不同重抽样策略（多个数据副本）上的平均性能表现。\n\n基准测试时有个关键问题：测试的公平性。即每个算法的每次测试必须在相同的重抽样训练集拟合模型，在相同的重抽样测试集评估性能。这些事情 `beachmark()` 会自动做好。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntasks = tsk(\"sonar\")  # 可以多任务\nlearners = lrns(c(\"classif.rpart\", \"classif.kknn\", \"classif.ranger\", \"classif.svm\"),\n                predict_type = \"prob\")\ndesign = benchmark_grid(tasks, learners, rsmps(\"cv\", folds = 5))\ndesign\n#>     task        learner resampling\n#> 1: sonar  classif.rpart         cv\n#> 2: sonar   classif.kknn         cv\n#> 3: sonar classif.ranger         cv\n#> 4: sonar    classif.svm         cv\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmr = benchmark(design)  # 执行基准测试\nbmr$aggregate(list(msr(\"classif.acc\"), msr(\"classif.auc\")))  # 汇总基准测试结果\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 可视化：对比性能\nautoplot(bmr, type = \"roc\")  # ROC 曲线\nautoplot(bmr, measure = msr(\"classif.auc\"))  # AUC 箱线图\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-25-1.png){fig-align='center' width=70%}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-25-2.png){fig-align='center' width=70%}\n:::\n:::\n\n\n## 可视化\n\nmlr3viz 包定义了 `autoplot()` 函数来用 ggplot2 绘图。通常一个对象有不止一种类型的图，可以通过 `type` 参数来改变绘图。图形使用 viridis 的调色板，外观由 theme 参数控制，默认是 minimal 主题。\n\n### 可视化任务\n\n#### 分类任务\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"penguins\")\ntask$select(c(\"body_mass\", \"bill_length\"))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(task, type = \"target\")  # \"target\" 图：条形图展示目标变量的各类别频数\nautoplot(task, type = \"duo\")     # \"duo\" 图：箱线图展示多个特征的分布\nautoplot(task, type = \"pairs\")   # \"pairs\" 图：展示多个特征的成对比较\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-27-1.png){fig-align='center' width=70%}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-27-2.png){fig-align='center' width=70%}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-27-3.png){fig-align='center' width=70%}\n:::\n:::\n\n\n#### 回归任务\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"mtcars\")\ntask$select(c(\"am\", \"carb\"))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(task, type = \"target\")  # \"target\" 图：箱线图展示目标变量的分布\nautoplot(task, type = \"pairs\")   # \"pairs\" 图：展示多个特征与目标变量的成对比较\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-29-1.png){fig-align='center' width=70%}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-29-2.png){fig-align='center' width=70%}\n:::\n:::\n\n\n### 可视化学习器\n\n#### glmnet 回归学习器\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"mtcars\")\nlearner = lrn(\"regr.glmnet\")\nlearner$train(task)\nautoplot(learner)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-30-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n#### 决策树学习器\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 分类树\ntask = tsk(\"penguins\")\nlearner = lrn(\"classif.rpart\", keep_model = TRUE)\nlearner$train(task)\nautoplot(learner)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-31-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 回归树\ntask = tsk(\"mtcars\")\nlearner = lrn(\"regr.rpart\", keep_model = TRUE)\nlearner$train(task)\nautoplot(learner)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-32-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n#### 层次聚类学习器\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 层次聚类树\ntask = tsk(\"usarrests\")\nlearner = lrn(\"clust.hclust\")\nlearner$train(task)\nautoplot(learner, type = \"dend\", task = task)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-33-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 碎石图\nautoplot(learner, type = \"scree\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-34-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n### 可视化预测对象\n\n#### 分类预测对象\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"stacked\" 图：堆叠条形图展示预测类别和真实类别频数对比\ntask = tsk(\"spam\")\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\")\npred = learner$train(task)$predict(task)\nautoplot(pred, type = \"stacked\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-35-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# ROC 曲线：展示不同阈值下的真阳率与假阳率\nautoplot(pred, type = \"roc\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-36-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# PR 曲线：展示不同阈值下的查准率与召回率\nautoplot(pred, type = \"prc\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-37-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# “threshold” 图：展示二元分类在不同阈值下的性能\nautoplot(pred, type = \"threshold\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-38-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n#### 回归预测对象\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"xy\" 图：散点图展示回归预测的真实值与预测值\ntask = tsk(\"boston_housing\")\nlearner = lrn(\"regr.rpart\")\npred = learner$train(task)$predict(task)\nautoplot(pred, type = \"xy\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-39-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"residual\" 图：绘制响应的残差图\nautoplot(pred, type = \"residual\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-40-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"histogram\" 图：残差直方图展示残差的分布\nautoplot(pred, type = \"histogram\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-41-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n#### 聚类预测对象\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"scatter\" 图：绘制按聚类预测结果着色的散点图\ntask = tsk(\"usarrests\")\nlearner = lrn(\"clust.kmeans\", centers = 3)\npred = learner$train(task)$predict(task)\nautoplot(pred, task, type = \"scatter\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-42-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"sil\" 图：展示聚类的silhouette 宽度，虚线是平均silhouette 宽度\nautoplot(pred, task, type = \"sil\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-43-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"pca\" 图：展示数据的前两个主成分，不同聚类用颜色区分\nautoplot(pred, task, type = \"pca\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-44-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n### 可视化重抽样结果\n\n#### 分类重抽样结果\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"boxplot\"/\"histogram\" 图：箱线图展示性能度量的分布\ntask = tsk(\"sonar\")\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\")\nresampling = rsmp(\"cv\")\nrr = resample(task, learner, resampling)\n#> INFO  [15:11:07.498] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 1/10)\n#> INFO  [15:11:07.535] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 2/10)\n#> INFO  [15:11:07.565] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 3/10)\n#> INFO  [15:11:07.594] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 4/10)\n#> INFO  [15:11:07.623] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 5/10)\n#> INFO  [15:11:07.657] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 6/10)\n#> INFO  [15:11:07.688] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 7/10)\n#> INFO  [15:11:07.721] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 8/10)\n#> INFO  [15:11:07.754] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 9/10)\n#> INFO  [15:11:07.785] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 10/10)\nautoplot(rr, type = \"boxplot\")\nautoplot(rr, type = \"histogram\")\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-45-1.png){fig-align='center' width=70%}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-45-2.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# ROC 曲线：展示不同阈值下的真阳率与假阳率\nautoplot(rr, type = \"roc\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-46-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# PR 曲线：展示不同阈值下的查准率与召回率\nautoplot(rr, type = \"prc\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-47-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"prediction\" 图：展示两个特征表示的测试集样本点和以背景色区分的预测类别\ntask = tsk(\"pima\")\ntask$filter(seq(100))\ntask$select(c(\"age\", \"glucose\"))\nlearner = lrn(\"classif.rpart\")\nresampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, learner, resampling, store_models = TRUE)\n#> INFO  [15:11:09.101] [mlr3] Applying learner 'classif.rpart' on task 'pima' (iter 1/3)\n#> INFO  [15:11:09.124] [mlr3] Applying learner 'classif.rpart' on task 'pima' (iter 2/3)\n#> INFO  [15:11:09.158] [mlr3] Applying learner 'classif.rpart' on task 'pima' (iter 3/3)\nautoplot(rr, type = \"prediction\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-48-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 若将学习器的预测类型改为\"prob\"，则用颜色深浅展示概率值\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\")\nresampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, learner, resampling, store_models = TRUE)\n#> INFO  [15:11:10.010] [mlr3] Applying learner 'classif.rpart' on task 'pima' (iter 1/3)\n#> INFO  [15:11:10.034] [mlr3] Applying learner 'classif.rpart' on task 'pima' (iter 2/3)\n#> INFO  [15:11:10.058] [mlr3] Applying learner 'classif.rpart' on task 'pima' (iter 3/3)\nautoplot(rr, type = \"prediction\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-49-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 上面是只绘制测试集，也可以加入训练集\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\",\n              predict_sets = c(\"train\", \"test\"))\nresampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, learner, resampling, store_models = TRUE)\n#> INFO  [15:11:11.101] [mlr3] Applying learner 'classif.rpart' on task 'pima' (iter 1/3)\n#> INFO  [15:11:11.132] [mlr3] Applying learner 'classif.rpart' on task 'pima' (iter 2/3)\n#> INFO  [15:11:11.174] [mlr3] Applying learner 'classif.rpart' on task 'pima' (iter 3/3)\nautoplot(rr, type = \"prediction\",\n         predict_sets = c(\"train\", \"test\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-50-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"prediction\" 图也可以绘制分类特征\ntask = tsk(\"german_credit\")\ntask$filter(seq(100))\ntask$select(c(\"housing\", \"employment_duration\"))\nlearner = lrn(\"classif.rpart\")\nresampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, learner, resampling, store_models = TRUE)\n#> INFO  [15:11:12.175] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 1/3)\n#> INFO  [15:11:12.198] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 2/3)\n#> INFO  [15:11:12.221] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 3/3)\nautoplot(rr, type = \"prediction\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-51-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n#### 回归重抽样结果\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"prediction\" 图：绘制一个特征与响应的散点图，散点表示测试集中的观测\ntask = tsk(\"boston_housing\")\ntask$select(\"age\")\ntask$filter(seq(100))\nlearner = lrn(\"regr.rpart\")\nresampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, learner, resampling, store_models = TRUE)\n#> INFO  [15:11:13.043] [mlr3] Applying learner 'regr.rpart' on task 'boston_housing' (iter 1/3)\n#> INFO  [15:11:13.064] [mlr3] Applying learner 'regr.rpart' on task 'boston_housing' (iter 2/3)\n#> INFO  [15:11:13.082] [mlr3] Applying learner 'regr.rpart' on task 'boston_housing' (iter 3/3)\nautoplot(rr, type = \"prediction\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-52-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 若将学习器的预测类型改为\"se\"，还可以加上置信带\nlearner = lrn(\"regr.lm\", predict_type = \"se\")\nresampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, learner, resampling, store_models = TRUE)\n#> INFO  [15:11:13.692] [mlr3] Applying learner 'regr.lm' on task 'boston_housing' (iter 1/3)\n#> INFO  [15:11:13.711] [mlr3] Applying learner 'regr.lm' on task 'boston_housing' (iter 2/3)\n#> INFO  [15:11:13.729] [mlr3] Applying learner 'regr.lm' on task 'boston_housing' (iter 3/3)\nautoplot(rr, type = \"prediction\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-53-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 上面是只绘制测试集，也可以加入训练集\ntask$select(\"age\")\nlearner = lrn(\"regr.lm\", predict_type = \"se\",\n              predict_sets = c(\"train\", \"test\"))\nresampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, learner, resampling, store_models = TRUE)\n#> INFO  [15:11:14.399] [mlr3] Applying learner 'regr.lm' on task 'boston_housing' (iter 1/3)\n#> INFO  [15:11:14.419] [mlr3] Applying learner 'regr.lm' on task 'boston_housing' (iter 2/3)\n#> INFO  [15:11:14.441] [mlr3] Applying learner 'regr.lm' on task 'boston_housing' (iter 3/3)\nautoplot(rr, type = \"prediction\",\n         predict_sets = c(\"train\", \"test\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-54-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 还可以将预测面添加到背景\ntask = tsk(\"boston_housing\")\ntask$select(c(\"age\", \"rm\"))\ntask$filter(seq(100))\nlearner = lrn(\"regr.rpart\")\nresampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, learner, resampling, store_models = TRUE)\n#> INFO  [15:11:15.202] [mlr3] Applying learner 'regr.rpart' on task 'boston_housing' (iter 1/3)\n#> INFO  [15:11:15.221] [mlr3] Applying learner 'regr.rpart' on task 'boston_housing' (iter 2/3)\n#> INFO  [15:11:15.239] [mlr3] Applying learner 'regr.rpart' on task 'boston_housing' (iter 3/3)\nautoplot(rr, type = \"prediction\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-55-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n### 可视化基准测试结果\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"boxplot\" 图：箱线图展示了多个任务的基准测试的性能分布\ntasks = tsks(c(\"pima\", \"sonar\"))\nlearners = lrns(c(\"classif.featureless\", \"classif.rpart\", \"classif.xgboost\"),\n                predict_type = \"prob\")\nresampling = rsmps(\"cv\")\nbmr = benchmark(benchmark_grid(tasks, learners, resampling))\nautoplot(bmr, type = \"boxplot\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-56-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 绘制一个任务与多个学习器的基准测试\ntask = tsk(\"pima\")\nlearners = lrns(c(\"classif.featureless\", \"classif.rpart\", \"classif.xgboost\"),\n                predict_type = \"prob\")\nresampling = rsmp(\"cv\")\nbmr = benchmark(benchmark_grid(task, learners, resampling))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(bmr, type = \"roc\")\nautoplot(bmr, type = \"prc\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-58-1.png){fig-align='center' width=70%}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-58-2.png){fig-align='center' width=70%}\n:::\n:::\n\n\n### 可视化调参实例\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"performance\" 图：散点折线图展示随批次的模型性能变化\ninstance = tune(\n  tuner = tnr(\"gensa\"),\n  task = tsk(\"sonar\"),\n  learner = lts(lrn(\"classif.rpart\")),\n  resampling = rsmp(\"holdout\"),\n  measures = msr(\"classif.ce\"),\n  term_evals = 100\n)\nautoplot(instance, type = \"performance\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-59-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"parameter\" 图：散点图展示每个超参数取值与模型性能变化\nautoplot(instance, type = \"parameter\", cols_x = c(\"cp\", \"minsplit\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-60-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"marginal\" 图：展示不同超参数值的性能，颜色表示批次\nautoplot(instance, type = \"marginal\", cols_x = \"cp\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-61-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"parallel\" 图：可视化超参数之间的关系\nautoplot(instance, type = \"parallel\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-62-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"points\" 图：散点热力图展示两个超参数的性能对比，用颜色深浅表示模型性能\nautoplot(instance, type = \"points\", cols_x = c(\"cp\", \"minsplit\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-63-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"pairs\" 图：展示所有超参数成对对比\nautoplot(instance, type = \"pairs\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-64-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# \"surface\" 图：绘制两个超参数的模型性能面，该面是用一个学习器插值的\nautoplot(instance, type = \"surface\", cols_x = c(\"cp\", \"minsplit\"),\n         learner = lrn(\"regr.ranger\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-65-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n### 可视化特征过滤器\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 条形图展示基于过滤器的特征得分\ntask = tsk(\"mtcars\")\nft = flt(\"correlation\")\nft$calculate(task)\nautoplot(ft, n = 5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-66-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n更多细节在 mlr3viz 包的 reference 页。\n\n# 图学习器\n\n一个管道运算（PipeOp），表示机器学习管道中的一个计算步骤。一系列的 PipeOps  通过边连接（`%>>%`）构成图（Graph），图可以是简单的线性图，也可以是复杂的非线性图。\n\n搭建图学习器：\n\n- 用 `po()` 获取 PipeOp，通过连接符 `%>>%` 连接 Graphs 与 PipeOps\n\n- 通过 `gunion()` 将 Graphs 与 PipeOps 并起来\n\n- 用 `ppl(\"replicate\", graph, n)` 将 Graph 或 PipeOps 复制 n 份并起来\n\n- `Graph$plot()` 绘制图的结构关系\n\n- `as_learner(Graph)` 将图转化为学习器，即可跟普通学习器一样使用\n\n管道、图学习器的主要用处在于：\n\n- 特征工程：缺失值插补、特征提取、特征选择、处理不均衡数据……\n\n- 集成学习：袋装法、堆叠法\n\n- 分支训练、分块训练\n\n## 线形图\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 搭建图\ngraph = po(\"scale\") %>>%\n  po(\"encode\") %>>%\n  po(\"imputemedian\") %>>%\n  lrn(\"classif.rpart\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 可视化图\ngraph$plot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-68-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 转为图学习器\ngl = as_learner(graph)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 训练任务\ntask = tsk(\"iris\")\ngl$train(task)\n```\n:::\n\n\n调试图：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 取出或设置图学习器超参数\ngraph$pipeops$scale$param_set$values$center = FALSE\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 获取单独PipeOp 的$state（经过$train() 后）\ngraph$keep_results = TRUE\ngraph$train(task)\n#> $classif.rpart.output\n#> NULL\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph$pipeops$scale$state$scale\n#> Petal.Length  Petal.Width Sepal.Length  Sepal.Width \n#>     4.163367     1.424451     5.921098     3.098387\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 查看图的中间结果：$.result（需提前设置$keep_results = TRUE）\ngraph$pipeops$scale$.result[[1]]$head()\n#>    Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n#> 1:  setosa    0.3362663    0.140405    0.8613268   1.1296201\n#> 2:  setosa    0.3362663    0.140405    0.8275493   0.9682458\n#> 3:  setosa    0.3122473    0.140405    0.7937718   1.0327956\n#> 4:  setosa    0.3602853    0.140405    0.7768830   1.0005207\n#> 5:  setosa    0.3362663    0.140405    0.8444380   1.1618950\n#> 6:  setosa    0.4083234    0.280810    0.9119931   1.2587196\n```\n:::\n\n\n## 非线性图\n\n### 分支\n\n集成学习的袋装法、堆叠法都是非线性图，另一种非线性图是分支：即只执行若干条备选路径中的一条。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_branch = ppl(\"branch\", list(\n  pca = po(\"pca\"),\n  ica = po(\"ica\")\n)) %>>%\n  lrn(\"classif.kknn\")\ngraph_branch$plot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-75-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n### 分块训练\n\n在数据太大，无法载入内存的情况下，一个常用的技术是将数据分成几块，然后分别对各块数据进行训练，之后再用 PipeOpClassifAvg 或 PipeOpRegrAvg 将模型按加权平均汇总。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_chunks = po(\"chunk\", 4) %>>%\n  ppl(\"greplicate\", lrn(\"classif.rpart\"), 4) %>>%\n  po(\"classifavg\", 4)\ngraph_chunks$plot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-76-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 转化为图学习器，与学习器一样使用\nchunks_lrn = as_learner(graph_chunks)\nchunks_lrn$train(tsk(\"iris\"))\n```\n:::\n\n\n## 集成学习\n\n集成学习，是通过构建多个基学习器，并按一定策略结合成强学习器来完成学习任务，即所谓“博采众长”，最终效果是优于任何一个原学习器。集成学习可用于分类/回归集成、特征选择集成、异常值检测集成等。\n\n这多个基学习器可以是同质的，比如都用决策树或都用神经网络，以 Bagging 和 Boosting 模式为代表；也可以是异质的，即采用不同的算法，以 Stacking 模式为代表。\n\n### 装袋法（Bagging）\n\nBagging 采用的是并行机制，即基学习器的训练之间没有前后顺序可以同时进行。\n\nBagging 是用“有放回” 抽样（Bootstrap 法）的方式抽取训练集，对于包含 $m$ 个样本的训练集，进\n行 $m$ 次有放回的随机抽样操作，得到样本子集（有重复）中有接近 36.8% 的样本没有被抽到。\n\n按照同样的方式重复进行，就可以采集到 $T$ 个包含 $m$ 个样本的数据副本，从而训练出 $T$ 个基学习器。\n\n最终对这 $T$ 个基学习器的输出进行结合，分类问题就采用“多数决”，回归问题就采用“取平均”。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 单分支：数据子抽样 + 决策树\nsingle_path = po(\"subsample\") %>>% lrn(\"classif.rpart\")\n# 复制 10 次得到 10 个分支，再接类平均\ngraph_bag = ppl(\"greplicate\", single_path, n = 10) %>>%\n  po(\"classifavg\")\n# 可视化结构关系\ngraph_bag$plot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-78-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 转为图学习器，可与普通学习器一样使用\nbaglrn = as_learner(graph_bag)\nbaglrn$train(tsk(\"iris\"))\n```\n:::\n\n\n### 提升法（Boosting）\n\nBoosting 采用的是串行机制，即基学习器的训练存在依赖关系，按次序一一进行训练（实现上可以做\n到并行）。\n\n基本思想：基模型的训练集按照某种策略每次都进行一定的转化，对所有基模型预测的结果进行线性合\n成产生最终的预测结果。\n\n从偏差-方差分解来看，Boosting 算法主要关注于降低偏差，每轮的迭代都关注于训练过程中预测错误的样本，将弱学习器提升为强学习器。\n\n### 堆叠法（Stacking）\n\nStacking 法，采用的是分阶段机制，将若干基模型的输出作为输入，再接一层主学习器，得到最终的预测。\n\n将训练好的所有基模型对训练集进行预测，第 $j$ 个基模型对第 $i$ 个训练样本的预测值将作为新的训练集中第 $i$ 个样本的第 $j$ 个特征值，最后基于新的训练集进行训练。同理，预测的过程也要先经过所有基模型的预测形成新的测试集，最后再对测试集进行预测。\n\n用于堆叠的基模型通常采用不同的模型，作用在相同的训练集上。\n\n为了充分利用数据，Stacking 通常采用 $k$ 折交叉训练法（类似 $k$ 折交叉验证）：每个基学习器分别在各个 $k-1$ 折数据上训练，在其剩下的 1 折数据上预测，就可以得到对任意 1 折数据的预测结果，进而用于训练主模型。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_stack = gunion(list(\n  po(\"learner_cv\", lrn(\"regr.lm\")),\n  po(\"learner_cv\", lrn(\"regr.svm\")),\n  po(\"nop\")\n)) %>>%\n  po(\"featureunion\") %>>%\n  lrn(\"regr.ranger\")\n\ngraph_stack$plot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-80-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 转为图学习器，可与普通学习器一样使用\nstacklrn = as_learner(graph_stack)\nstacklrn$train(tsk(\"mtcars\"))\n```\n:::\n\n\n# 特征工程\n\n机器学习中的数据预处理，通常也统称为特征工程，主要包括：缺失值插补、特征变换，目的是提升模型性能。\n\n## 特征工程概述\n\n用 mlr3pipelines 包实现特征工程。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 查看所有 PipeOp\npo()\n#> <DictionaryPipeOp> with 64 stored values\n#> Keys: boxcox, branch, chunk, classbalancing, classifavg, classweights,\n#>   colapply, collapsefactors, colroles, copy, datefeatures, encode,\n#>   encodeimpact, encodelmer, featureunion, filter, fixfactors, histbin,\n#>   ica, imputeconstant, imputehist, imputelearner, imputemean,\n#>   imputemedian, imputemode, imputeoor, imputesample, kernelpca,\n#>   learner, learner_cv, missind, modelmatrix, multiplicityexply,\n#>   multiplicityimply, mutate, nmf, nop, ovrsplit, ovrunite, pca, proxy,\n#>   quantilebin, randomprojection, randomresponse, regravg,\n#>   removeconstants, renamecolumns, replicate, scale, scalemaxabs,\n#>   scalerange, select, smote, spatialsign, subsample, targetinvert,\n#>   targetmutate, targettrafoscalerange, textvectorizer, threshold,\n#>   tunethreshold, unbranch, vtreat, yeojohnson\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngr = po(\"scale\") %>>% po(\"pca\", rank. = 2)\ngr$plot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-83-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n特征工程管道的三种用法：\n\n1. <p>调试：查看特征工程步对输入数据做了什么</p>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 训练特征工程管道：提供任务，访问特征工程之后的数据\ntask = tsk(\"iris\")\ngr$train(task)[[1]]$data()\n#>        Species        PC1         PC2\n#>   1:    setosa -2.2571412 -0.47842383\n#>   2:    setosa -2.0740130  0.67188269\n#>   3:    setosa -2.3563351  0.34076642\n#>   4:    setosa -2.2917068  0.59539986\n#>   5:    setosa -2.3818627 -0.64467566\n#>  ---                                 \n#> 146: virginica  1.8642579 -0.38567404\n#> 147: virginica  1.5593565  0.89369285\n#> 148: virginica  1.5160915 -0.26817075\n#> 149: virginica  1.3682042 -1.00787793\n#> 150: virginica  0.9574485  0.02425043\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 将训练好的特征工程管道，用于新的数据\ngr$predict(task$filter(1:5))[[1]]$data()\n#>    Species       PC1        PC2\n#> 1:  setosa -2.257141 -0.4784238\n#> 2:  setosa -2.074013  0.6718827\n#> 3:  setosa -2.356335  0.3407664\n#> 4:  setosa -2.291707  0.5953999\n#> 5:  setosa -2.381863 -0.6446757\n```\n:::\n\n\n2. <p>用于机器学习：原始任务经过特征工程管道变成预处理之后的任务，再正常用于机器学习流程</p>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnewtask = gr$train(task)[[1]]\nnewtask\n#> <TaskClassif:iris> (5 x 3): Iris Flowers\n#> * Target: Species\n#> * Properties: multiclass\n#> * Features (2):\n#>   - dbl (2): PC1, PC2\n```\n:::\n\n\n3. <p>用于机器学习：再接一个学习器，转化成图学习器，同普通学习器一样使用，适合对特征工程步、ML 算法一起联动调参</p>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngr = gr %>>% lrn(\"classif.rpart\")\ngr$plot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-87-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngr_learner = as_learner(gr)\ngr_learner\n#> <GraphLearner:scale.pca.classif.rpart>\n#> * Model: -\n#> * Parameters: scale.robust=FALSE, pca.rank.=2, classif.rpart.xval=0\n#> * Packages: mlr3, mlr3pipelines, rpart\n#> * Predict Types:  [response], prob\n#> * Feature Types: logical, integer, numeric, character, factor, ordered,\n#>   POSIXct\n#> * Properties: featureless, hotstart_backward, hotstart_forward,\n#>   importance, loglik, missings, multiclass, oob_error,\n#>   selected_features, twoclass, weights\n```\n:::\n\n\n## 缺失值插补\n\n目前支持的插补方法：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(mlr_pipeops)[tags %in% \"missings\", \"key\"]\n#>               key\n#> 1: imputeconstant\n#> 2:     imputehist\n#> 3:  imputelearner\n#> 4:     imputemean\n#> 5:   imputemedian\n#> 6:     imputemode\n#> 7:      imputeoor\n#> 8:   imputesample\n```\n:::\n\n\n### 简单插补\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"pima\")\ntask$missings()\n#> diabetes      age  glucose  insulin     mass pedigree pregnant pressure \n#>        0        0        5      374       11        0        0       35 \n#>  triceps \n#>      227\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 常数插补\npo = po(\"imputeconstant\", param_vals = list(\n  constant = -999, affect_columns = selector_name(\"glucose\")\n))\nnew_task = po$train(list(task = task))[[1]]\nnew_task$missings()\n#> diabetes      age  insulin     mass pedigree pregnant pressure  triceps \n#>        0        0      374       11        0        0       35      227 \n#>  glucose \n#>        0\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 均值插补\npo = po(\"imputemean\")\nnew_task = po$train(list(task = task))[[1]]\nnew_task$missings()\n#> diabetes      age pedigree pregnant  glucose  insulin     mass pressure \n#>        0        0        0        0        0        0        0        0 \n#>  triceps \n#>        0\n```\n:::\n\n\n### 随机抽样插补\n\n通过从非缺失的训练数据中随机抽样来插补特征。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npo = po(\"imputesample\")\nnew_task = po$train(list(task = task))[[1]]\nnew_task$missings()\n#> diabetes      age pedigree pregnant  glucose  insulin     mass pressure \n#>        0        0        0        0        0        0        0        0 \n#>  triceps \n#>        0\n```\n:::\n\n\n### 直方图法插补\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npo = po(\"imputehist\")\nnew_task = po$train(list(task = task))[[1]]\nnew_task$missings()\n#> diabetes      age pedigree pregnant  glucose  insulin     mass pressure \n#>        0        0        0        0        0        0        0        0 \n#>  triceps \n#>        0\n```\n:::\n\n\n### 学习器插补\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 决策树插补\npo = po(\"imputelearner\", lrn(\"regr.rpart\"))\nnew_task = po$train(list(task = task))[[1]]\nnew_task$missings()\n#> diabetes      age pedigree pregnant  glucose  insulin     mass pressure \n#>        0        0        0        0        0        0        0        0 \n#>  triceps \n#>        0\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# KNN插补，训练KNN学习器时，先用直方图法插补\npo = po(\"imputelearner\", \n        po(\"imputehist\") %>>% lrn(\"regr.kknn\"))\nnew_task = po$train(list(task = task))[[1]]\nnew_task$missings()\n#> diabetes      age pedigree pregnant  glucose  insulin     mass pressure \n#>        0        0        0        0        0        0        0        0 \n#>  triceps \n#>        0\n```\n:::\n\n\n## 特征工程\n\n### 特征缩放\n\n不同数值型特征的数据量纲可能相差多个数量级，这对很多数据模型会有很大影响，所以有必要做归一化处理，就是将列或行对齐并转化为一致。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndf = as_tibble(iris) %>% \n  set_names(str_c(\"x\", 1:4), \"Species\")\ntask = as_task_classif(df, target = \"Species\")\n```\n:::\n\n\n#### 标准化\n\n标准化也称为 Z 标准化，将数据变成均值为 0，标准差为 1。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npos = po(\"scale\")  # 参数 scale = FALSE，只做中心化\npos$train(list(task))[[1]]$data()\n#>        Species          x1          x2         x3         x4\n#>   1:    setosa -0.89767388  1.01560199 -1.3357516 -1.3110521\n#>   2:    setosa -1.13920048 -0.13153881 -1.3357516 -1.3110521\n#>   3:    setosa -1.38072709  0.32731751 -1.3923993 -1.3110521\n#>   4:    setosa -1.50149039  0.09788935 -1.2791040 -1.3110521\n#>   5:    setosa -1.01843718  1.24503015 -1.3357516 -1.3110521\n#>  ---                                                        \n#> 146: virginica  1.03453895 -0.13153881  0.8168591  1.4439941\n#> 147: virginica  0.55148575 -1.27867961  0.7035638  0.9192234\n#> 148: virginica  0.79301235 -0.13153881  0.8168591  1.0504160\n#> 149: virginica  0.43072244  0.78617383  0.9301544  1.4439941\n#> 150: virginica  0.06843254 -0.13153881  0.7602115  0.7880307\n```\n:::\n\n\n注：中心化后，0 就代表均值，更方便模型解释。\n\n#### 归一化\n\n归一化是将数据线性放缩到 $\\left[0, 1\\right]$（根据需要也可以线性放缩到 $\\left[a, b\\right]$）, 一般还同时考虑指标一致化，将正向指标（值越大越好）和负向指标（值越小越好）都变成正向。\n\n正向指标：\n\n$$\nx_i'=\\frac{x_i - \\text{min} x_i}{\\text{max} x_i - \\text{min} x_i}\n$$\n\n负向指标：\n\n$$\nx_i'=\\frac{\\text{max} x_i - x_i}{\\text{max} x_i - \\text{min} x_i}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npop = po(\"scalerange\", param_vals = list(lower = 0, upper = 1))\npop$train(list(task))[[1]]$data()\n#>        Species         x1        x2         x3         x4\n#>   1:    setosa 0.22222222 0.6250000 0.06779661 0.04166667\n#>   2:    setosa 0.16666667 0.4166667 0.06779661 0.04166667\n#>   3:    setosa 0.11111111 0.5000000 0.05084746 0.04166667\n#>   4:    setosa 0.08333333 0.4583333 0.08474576 0.04166667\n#>   5:    setosa 0.19444444 0.6666667 0.06779661 0.04166667\n#>  ---                                                     \n#> 146: virginica 0.66666667 0.4166667 0.71186441 0.91666667\n#> 147: virginica 0.55555556 0.2083333 0.67796610 0.75000000\n#> 148: virginica 0.61111111 0.4166667 0.71186441 0.79166667\n#> 149: virginica 0.52777778 0.5833333 0.74576271 0.91666667\n#> 150: virginica 0.44444444 0.4166667 0.69491525 0.70833333\n```\n:::\n\n\n#### 行规范化\n\n行规范化，常用于文本数据或聚类算法，是保证每行具有单位范数，即每行的向量“长度”相同。想象一下，$m$ 个特征下，每行数据都是 $m$ 维空间中的一个点，做行规范化能让这些点都落在单位球面上（到原点的距离均为 1）。\n\n行规范化一般采用 L2 范数：\n\n$$\nx_{ij}' = \\frac{x_{ij}}{\\|x_i\\|} = \\frac{x_{ij}}{\\sqrt{\\sum_{j=1}^m x_{ij}^2}}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npop = po(\"spatialsign\")\npop$train(list(task))[[1]]$data()\n#>        Species        x1        x2        x3         x4\n#>   1:    setosa 0.8037728 0.5516088 0.2206435 0.03152050\n#>   2:    setosa 0.8281329 0.5070201 0.2366094 0.03380134\n#>   3:    setosa 0.8053331 0.5483119 0.2227517 0.03426949\n#>   4:    setosa 0.8000302 0.5391508 0.2608794 0.03478392\n#>   5:    setosa 0.7909650 0.5694948 0.2214702 0.03163860\n#>  ---                                                   \n#> 146: virginica 0.7215572 0.3230853 0.5600146 0.24769876\n#> 147: virginica 0.7296536 0.2895451 0.5790902 0.22005426\n#> 148: virginica 0.7165390 0.3307103 0.5732312 0.22047353\n#> 149: virginica 0.6746707 0.3699807 0.5876164 0.25028107\n#> 150: virginica 0.6902592 0.3509792 0.5966647 0.21058754\n```\n:::\n\n\n### 特征变换\n\n#### 非线性特征\n\n对于数值特征 $x_1, x_2, \\cdots$，可以穿件更多的多项式特征：$x_1^2, x_1, x_2^2, x2, \\cdots$，这相当于是用自变量的更高阶泰勒公式去逼近因变量。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npop = po(\"modelmatrix\", formula = ~ . ^ 2 + I(x1 ^ 2) + log(x2))\npop$train(list(task))[[1]]$data()\n#>        Species (Intercept)  x1  x2  x3  x4 I(x1^2)   log(x2) x1:x2 x1:x3 x1:x4\n#>   1:    setosa           1 5.1 3.5 1.4 0.2   26.01 1.2527630 17.85  7.14  1.02\n#>   2:    setosa           1 4.9 3.0 1.4 0.2   24.01 1.0986123 14.70  6.86  0.98\n#>   3:    setosa           1 4.7 3.2 1.3 0.2   22.09 1.1631508 15.04  6.11  0.94\n#>   4:    setosa           1 4.6 3.1 1.5 0.2   21.16 1.1314021 14.26  6.90  0.92\n#>   5:    setosa           1 5.0 3.6 1.4 0.2   25.00 1.2809338 18.00  7.00  1.00\n#>  ---                                                                          \n#> 146: virginica           1 6.7 3.0 5.2 2.3   44.89 1.0986123 20.10 34.84 15.41\n#> 147: virginica           1 6.3 2.5 5.0 1.9   39.69 0.9162907 15.75 31.50 11.97\n#> 148: virginica           1 6.5 3.0 5.2 2.0   42.25 1.0986123 19.50 33.80 13.00\n#> 149: virginica           1 6.2 3.4 5.4 2.3   38.44 1.2237754 21.08 33.48 14.26\n#> 150: virginica           1 5.9 3.0 5.1 1.8   34.81 1.0986123 17.70 30.09 10.62\n#>      x2:x3 x2:x4 x3:x4\n#>   1:  4.90  0.70  0.28\n#>   2:  4.20  0.60  0.28\n#>   3:  4.16  0.64  0.26\n#>   4:  4.65  0.62  0.30\n#>   5:  5.04  0.72  0.28\n#>  ---                  \n#> 146: 15.60  6.90 11.96\n#> 147: 12.50  4.75  9.50\n#> 148: 15.60  6.00 10.40\n#> 149: 18.36  7.82 12.42\n#> 150: 15.30  5.40  9.18\n```\n:::\n\n\n另一种常用的非线性特征是基于自然样条的样条特征（暂时没有专门的PipeOp）：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npop = po(\"modelmatrix\", formula = ~ splines::ns(x1, 5))\npop$train(list(task))[[1]]$data()\n#>        Species (Intercept) splines::ns(x1, 5)1 splines::ns(x1, 5)2\n#>   1:    setosa           1        3.094150e-01        0.0009968102\n#>   2:    setosa           1        1.318681e-01        0.0000000000\n#>   3:    setosa           1        3.907204e-02        0.0000000000\n#>   4:    setosa           1        1.648352e-02        0.0000000000\n#>   5:    setosa           1        2.094017e-01        0.0000000000\n#>  ---                                                              \n#> 146: virginica           1        0.000000e+00        0.3024574669\n#> 147: virginica           1        1.812956e-02        0.6788378608\n#> 148: virginica           1        1.362101e-05        0.4802626315\n#> 149: virginica           1        5.579165e-02        0.7426950594\n#> 150: virginica           1        3.630706e-01        0.6033287054\n#>      splines::ns(x1, 5)3 splines::ns(x1, 5)4 splines::ns(x1, 5)5\n#>   1:         -0.20323541          0.46832508        -0.265089667\n#>   2:         -0.21657099          0.49905488        -0.282483895\n#>   3:         -0.17532519          0.40401021        -0.228685025\n#>   4:         -0.13961683          0.32172574        -0.182108907\n#>   5:         -0.21746675          0.50111903        -0.283652281\n#>  ---                                                            \n#> 146:          0.52049703          0.20822647        -0.031180964\n#> 147:          0.26175304          0.08745429        -0.046174752\n#> 148:          0.42721751          0.15182156        -0.059315324\n#> 149:          0.17592803          0.05799882        -0.032413564\n#> 150:          0.01963969          0.01354992        -0.007669769\n```\n:::\n\n\n#### 计算新特征\n\n管道运算“mutate”，根据以公式形式给出的表达式添加特征，这些表达式可能取决于其他特征的值。这可以增加新的特征，也可以改变现有的特征。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npom = po(\"mutate\", mutation = list(\n  x1_p = ~ x1 + 1,\n  Area1 = ~ x1 * x2,\n  Area2 = ~ x3 * x4,\n  Area = ~ Area1 + Area2\n))\npom$train(list(task))[[1]]$data()\n#>        Species  x1  x2  x3  x4 x1_p Area1 Area2  Area\n#>   1:    setosa 5.1 3.5 1.4 0.2  6.1 17.85  0.28 18.13\n#>   2:    setosa 4.9 3.0 1.4 0.2  5.9 14.70  0.28 14.98\n#>   3:    setosa 4.7 3.2 1.3 0.2  5.7 15.04  0.26 15.30\n#>   4:    setosa 4.6 3.1 1.5 0.2  5.6 14.26  0.30 14.56\n#>   5:    setosa 5.0 3.6 1.4 0.2  6.0 18.00  0.28 18.28\n#>  ---                                                 \n#> 146: virginica 6.7 3.0 5.2 2.3  7.7 20.10 11.96 32.06\n#> 147: virginica 6.3 2.5 5.0 1.9  7.3 15.75  9.50 25.25\n#> 148: virginica 6.5 3.0 5.2 2.0  7.5 19.50 10.40 29.90\n#> 149: virginica 6.2 3.4 5.4 2.3  7.2 21.08 12.42 33.50\n#> 150: virginica 5.9 3.0 5.1 1.8  6.9 17.70  9.18 26.88\n```\n:::\n\n\n利用正则表达式从复杂字符串列提取出相关信息，并转化为因子特征：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npo_ftextract = po(\"mutate\", mutation = list(\n  fare_per_person = ~ fare / (parch + sib_sp + 1),\n  deck = ~ factor(str_sub(cabin, 1, 1)),\n  title = ~ factor(str_extract(name, \"(?<=, ).*(?=\\\\.)\")),\n  surname = ~ factor(str_extract(name, \"^.*(?=,)\")),\n  ticket_prefix = ~ factor(str_extract(ticket, \"^.*(?= )\"))\n))\npo_ftextract$train(list(tsk(\"titanic\")))[[1]]$data()\n#>       survived  age cabin embarked     fare\n#>    1:       no 22.0  <NA>        S   7.2500\n#>    2:      yes 38.0   C85        C  71.2833\n#>    3:      yes 26.0  <NA>        S   7.9250\n#>    4:      yes 35.0  C123        S  53.1000\n#>    5:       no 35.0  <NA>        S   8.0500\n#>   ---                                      \n#> 1305:     <NA>   NA  <NA>        S   8.0500\n#> 1306:     <NA> 39.0  C105        C 108.9000\n#> 1307:     <NA> 38.5  <NA>        S   7.2500\n#> 1308:     <NA>   NA  <NA>        S   8.0500\n#> 1309:     <NA>   NA  <NA>        C  22.3583\n#>                                                      name parch pclass    sex\n#>    1:                             Braund, Mr. Owen Harris     0      3   male\n#>    2: Cumings, Mrs. John Bradley (Florence Briggs Thayer)     0      1 female\n#>    3:                              Heikkinen, Miss. Laina     0      3 female\n#>    4:        Futrelle, Mrs. Jacques Heath (Lily May Peel)     0      1 female\n#>    5:                            Allen, Mr. William Henry     0      3   male\n#>   ---                                                                        \n#> 1305:                                  Spector, Mr. Woolf     0      3   male\n#> 1306:                        Oliva y Ocana, Dona. Fermina     0      1 female\n#> 1307:                        Saether, Mr. Simon Sivertsen     0      3   male\n#> 1308:                                 Ware, Mr. Frederick     0      3   male\n#> 1309:                            Peter, Master. Michael J     1      3   male\n#>       sib_sp             ticket fare_per_person deck  title       surname\n#>    1:      1          A/5 21171        3.625000 <NA>     Mr        Braund\n#>    2:      1           PC 17599       35.641650    C    Mrs       Cumings\n#>    3:      0   STON/O2. 3101282        7.925000 <NA>   Miss     Heikkinen\n#>    4:      1             113803       26.550000    C    Mrs      Futrelle\n#>    5:      0             373450        8.050000 <NA>     Mr         Allen\n#>   ---                                                                    \n#> 1305:      0          A.5. 3236        8.050000 <NA>     Mr       Spector\n#> 1306:      0           PC 17758      108.900000    C   Dona Oliva y Ocana\n#> 1307:      0 SOTON/O.Q. 3101262        7.250000 <NA>     Mr       Saether\n#> 1308:      0             359309        8.050000 <NA>     Mr          Ware\n#> 1309:      1               2668        7.452767 <NA> Master         Peter\n#>       ticket_prefix\n#>    1:           A/5\n#>    2:            PC\n#>    3:      STON/O2.\n#>    4:          <NA>\n#>    5:          <NA>\n#>   ---              \n#> 1305:          A.5.\n#> 1306:            PC\n#> 1307:    SOTON/O.Q.\n#> 1308:          <NA>\n#> 1309:          <NA>\n```\n:::\n\n\n若数据噪声太多的问题，通常就需要做数据平滑。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npom = po(\"mutate\", mutation = list(\n  x1_s = ~ slider::slide_dbl(x1, mean, .before = 2, .after = 2) # 五点移动平均\n))\ndat = pom$train(list(task = task))[[1]]$data()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(patchwork)\np1 = ggplot(dat, aes(1:150, x1)) + geom_line()\np2 = ggplot(dat, aes(1:150, x1_s)) + geom_line()\np1 / p2\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-106-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# colapply：应用函数到任务的每一列，常用于类型转换\npoca = po(\"colapply\", applicator = as.character)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# renamecolumns：修改列名\npop = po(\"renamecolumns\", param_vals = list(\n  renaming = c(\"Petal.Length\" = \"PL\")\n))\n```\n:::\n\n\n#### 正态性变换\n\nBox-Cox 变换是更神奇的正态性变换，用最大似然估计选择最优的 $\\lambda$ 值，让非负的非正态数据变成正态数据：\n\n$$\ny' = \n\\begin{cases}\n\\ln y  & \\lambda = 0 \\\\\n(y^{\\lambda} - 1)/ \\lambda  & \\lambda \\neq 0\n\\end{cases}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npop = po(\"boxcox\")\npop$train(list(task))[[1]]$data()\n#>        Species         x1          x2         x3         x4\n#>   1:    setosa -0.8917547  1.01831791 -1.3431567 -1.3850773\n#>   2:    setosa -1.1812229 -0.08167295 -1.3431567 -1.3850773\n#>   3:    setosa -1.4845435  0.37307046 -1.4033413 -1.3850773\n#>   4:    setosa -1.6417967  0.14833599 -1.2832670 -1.3850773\n#>   5:    setosa -1.0348319  1.22454068 -1.3431567 -1.3850773\n#>  ---                                                       \n#> 146: virginica  1.0385560 -0.08167295  0.8174171  1.2930924\n#> 147: virginica  0.6097200 -1.32264877  0.7075555  0.9020852\n#> 148: virginica  0.8279148 -0.08167295  0.8174171  1.0023867\n#> 149: virginica  0.4976284  0.80781419  0.9269887  1.2930924\n#> 150: virginica  0.1485189 -0.08167295  0.7625234  0.7998822\n```\n:::\n\n\n若数据包含 0 或负数，则 Box-Cox 变换不再适用，可以改用同样原理的 Yeo-Johnson 变换：\n\n$$\ny' = \n\\begin{cases}\n\\ln (y+1)                                 & \\lambda = 0, y \\geq 0\\\\\n\\frac{(y + 1)^{\\lambda} - 1}{\\lambda}     & \\lambda \\neq 0, y \\geq 0\\\\\n- \\ln (1-y)                               & \\lambda = 2, y < 0\\\\\n\\frac{(1-y)^{2-\\lambda} - 1}{\\lambda - 2} & \\lambda \\neq 2, y < 0\n\\end{cases}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npop = po(\"yeojohnson\")\npop$train(list(task))[[1]]$data()\n#>        Species         x1          x2         x3         x4\n#>   1:    setosa -0.8926989  1.01949272 -1.3278574 -1.3278174\n#>   2:    setosa -1.1812158 -0.08164367 -1.3278574 -1.3278174\n#>   3:    setosa -1.4829526  0.37387369 -1.3813346 -1.3278174\n#>   4:    setosa -1.6391179  0.14878429 -1.2741721 -1.3278174\n#>   5:    setosa -1.0353690  1.22553197 -1.3278574 -1.3278174\n#>  ---                                                       \n#> 146: virginica  1.0393000 -0.08164367  0.8157632  1.4105911\n#> 147: virginica  0.6095086 -1.32389503  0.6988626  0.9184998\n#> 148: virginica  0.8281903 -0.08164367  0.8157632  1.0424849\n#> 149: virginica  0.4971768  0.80900587  0.9330158  1.4105911\n#> 150: virginica  0.1474204 -0.08164367  0.7572682  0.7938307\n```\n:::\n\n\n#### 连续变量分箱\n\n在统计和机器学习中，有时需要将连续变量转化为离散变量，称为连续变量离散化或分箱，常用于银行风控建模，特别是线性回归或 Logistic 回归模型。\n\n分箱的好处有：\n\n1. 使得结果更便于分析和解释。比如，年龄从中年到老年，患高血压比例增加25%，而年龄每增加一岁，患高血压比例不一定有显著变化；\n\n1. 简化模型，将自变量与因变量间非线性的潜在的关系，转化为简单的线性关系。当然，分箱也可能带来问题：简化的模型关系可能与潜在的模型关系不一致（甚至发现的是错误的模型关系）、删除数据中的细微差别、切分点可能没有实际意义。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 等宽分箱\npop = po(\"histbin\", param_vals = list(breaks = 4))\npop$train(list(task))[[1]]$data()\n#>        Species       x1         x2       x3         x4\n#>   1:    setosa    (5,6]    (3,3.5] [-Inf,2] [-Inf,0.5]\n#>   2:    setosa [-Inf,5]    (2.5,3] [-Inf,2] [-Inf,0.5]\n#>   3:    setosa [-Inf,5]    (3,3.5] [-Inf,2] [-Inf,0.5]\n#>   4:    setosa [-Inf,5]    (3,3.5] [-Inf,2] [-Inf,0.5]\n#>   5:    setosa [-Inf,5]    (3.5,4] [-Inf,2] [-Inf,0.5]\n#>  ---                                                  \n#> 146: virginica    (6,7]    (2.5,3]    (4,6]   (2, Inf]\n#> 147: virginica    (6,7] [-Inf,2.5]    (4,6]    (1.5,2]\n#> 148: virginica    (6,7]    (2.5,3]    (4,6]    (1.5,2]\n#> 149: virginica    (6,7]    (3,3.5]    (4,6]   (2, Inf]\n#> 150: virginica    (5,6]    (2.5,3]    (4,6]    (1.5,2]\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 分位数分箱\npop = po(\"quantilebin\", numsplits = 4)\npop$train(list(task))[[1]]$data()\n#>        Species         x1         x2         x3         x4\n#>   1:    setosa (-Inf,5.1] (3.3, Inf] (-Inf,1.6] (-Inf,0.3]\n#>   2:    setosa (-Inf,5.1]    (2.8,3] (-Inf,1.6] (-Inf,0.3]\n#>   3:    setosa (-Inf,5.1]    (3,3.3] (-Inf,1.6] (-Inf,0.3]\n#>   4:    setosa (-Inf,5.1]    (3,3.3] (-Inf,1.6] (-Inf,0.3]\n#>   5:    setosa (-Inf,5.1] (3.3, Inf] (-Inf,1.6] (-Inf,0.3]\n#>  ---                                                      \n#> 146: virginica (6.4, Inf]    (2.8,3] (5.1, Inf] (1.8, Inf]\n#> 147: virginica  (5.8,6.4] (-Inf,2.8] (4.35,5.1] (1.8, Inf]\n#> 148: virginica (6.4, Inf]    (2.8,3] (5.1, Inf] (1.8, Inf]\n#> 149: virginica  (5.8,6.4] (3.3, Inf] (5.1, Inf] (1.8, Inf]\n#> 150: virginica  (5.8,6.4]    (2.8,3] (4.35,5.1]  (1.3,1.8]\n```\n:::\n\n\n### 特征降维\n\n有时数据集可能包含过多特征，甚至是冗余特征，可以用降维技术进压缩特征，但通常会降低模型性能。\n\n#### PCA\n\n$n$ 个特征，若转化为 $n$ 个主成分，则会保留原始数据的 100% 信息，但这就失去了降维的意义。所以1一般是只选择前若干个主成分，一般原则是选择至保留 85% 以上信息的主成分。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npop = po(\"pca\", rank. = 2)\npop$train(list(task))[[1]]$data()\n#>        Species       PC1         PC2\n#>   1:    setosa -2.684126 -0.31939725\n#>   2:    setosa -2.714142  0.17700123\n#>   3:    setosa -2.888991  0.14494943\n#>   4:    setosa -2.745343  0.31829898\n#>   5:    setosa -2.728717 -0.32675451\n#>  ---                                \n#> 146: virginica  1.944110 -0.18753230\n#> 147: virginica  1.527167  0.37531698\n#> 148: virginica  1.764346 -0.07885885\n#> 149: virginica  1.900942 -0.11662796\n#> 150: virginica  1.390189  0.28266094\n```\n:::\n\n\n#### 核 PCA\n\nPCA 适用于数据的线性降维。而核主成分分析（Kernel PCA, KPCA）可实现数据的非线性降维，用于处理线性不可分的数据集。\n\nKPCA 的大致思路是：对于输入空间中的矩阵 $X$，先用一个非线性映射把 $X$ 中的所有样本映射到一个高维甚至是无穷维的特征空间（使其线性可分），然后在这个高维空间进行 PCA 降维。\n\n#### ICA：独立成分分析\n\n独立成分分析，提取统计意义上的独立成分：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npop = po(\"ica\", n.comp = 3)\npop$train(list(task))[[1]]$data()\n#>        Species         V1         V2         V3\n#>   1:    setosa  0.1721410  0.4149772 -1.3952557\n#>   2:    setosa  0.2214486 -0.7872973 -1.3361069\n#>   3:    setosa -0.4563409 -0.3146524 -1.3308693\n#>   4:    setosa -0.6610660 -0.5861230 -1.2029337\n#>   5:    setosa -0.1833669  0.6446961 -1.3678177\n#>  ---                                           \n#> 146: virginica -0.1793776  0.7197010  0.9500885\n#> 147: virginica  0.1249361 -0.8278466  0.7937129\n#> 148: virginica -0.1614076  0.4371009  0.8769270\n#> 149: virginica -1.9147519  1.6044931  1.1871656\n#> 150: virginica -1.2683559  0.2261212  0.9094067\n```\n:::\n\n\n#### NMF：非负矩阵分解\n\n对于任意给定的非负矩阵 $V$，NMF 算法能够寻找到非负矩阵 $W$ 和非负矩阵 $H$，使它们的积为矩阵。非负矩阵分解的方法在保证矩阵的非负性的同时能够减少数据量，相当于把 $n$ 维的数据降维到 $r$ 维。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# BiocManager::install(\"Biobase\")\npop = po(\"nmf\", rank = 3)\npop$train(list(task))[[1]]$data()\n#> Registered S3 methods overwritten by 'registry':\n#>   method               from \n#>   print.registry_field proxy\n#>   print.registry_entry proxy\n#>        Species       NMF1      NMF2         NMF3\n#>   1:    setosa 0.08946447 0.4806157  0.039331051\n#>   2:    setosa 0.14847785 0.4012262 -0.003756253\n#>   3:    setosa 0.08482244 0.4394040  0.037395481\n#>   4:    setosa 0.10090716 0.4121730  0.052304626\n#>   5:    setosa 0.06054158 0.4968302  0.064062112\n#>  ---                                            \n#> 146: virginica 0.26134716 0.3282638  0.600819824\n#> 147: virginica 0.34676264 0.2323164  0.469663330\n#> 148: virginica 0.28374883 0.3073426  0.552988371\n#> 149: virginica 0.14504617 0.3818651  0.727874559\n#> 150: virginica 0.23580629 0.3002660  0.566502348\n```\n:::\n\n\n#### 剔除常量特征\n\n从任务中剔除常量特征。对于每个特征，计算不同于其众数值的比例。所有比例低于可设置阈值的特征都会从任务中剔除。缺失值可以被忽略，也可以视为与非缺失值不同的常规值。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata = data.table(y = runif(10), a = 1:10, b = rep(1, 10), \n                  c = rep(1:2, each = 5))\ntask_ex = as_task_regr(data, target = \"y\")\npo = po(\"removeconstants\")  # 剔除常量特征 b\npo$train(list(task_ex))[[1]]$data()\n#>               y  a c\n#>  1: 0.891268901  1 1\n#>  2: 0.002797802  2 1\n#>  3: 0.649207745  3 1\n#>  4: 0.561067845  4 1\n#>  5: 0.427392408  5 1\n#>  6: 0.387362043  6 2\n#>  7: 0.544585730  7 2\n#>  8: 0.622006226  8 2\n#>  9: 0.183312478  9 2\n#> 10: 0.342760530 10 2\n```\n:::\n\n\n### 分类特征\n\n#### 因子折叠\n\n管道运算“collapsefactors”，对因子或有序因子进行折叠，折叠训练样本中最少的水平，直到剩下 `target_level_count` 个水平。然而，那些出现率高于 `no_collapse_above_prevalence` 的水平会被保留。对于因子变量，它们被折叠到下一个更大的水平，对于有序变量，稀有变量被折叠到相邻的类别，以样本数较少者为准。\n\n训练集中没有出现的水平在预测集中不会被使用，因此经常与因子修正结合起来使用。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat = tibble(color = factor(starwars$skin_color), y = 1)\ndat %>% count(color)\n#> # A tibble: 31 × 2\n#>    color                   n\n#>    <fct>               <int>\n#>  1 blue                    2\n#>  2 blue, grey              2\n#>  3 brown                   4\n#>  4 brown mottle            1\n#>  5 brown, white            1\n#>  6 dark                    6\n#>  7 fair                   17\n#>  8 fair, green, yellow     1\n#>  9 gold                    1\n#> 10 green                   6\n#> # ℹ 21 more rows\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = as_task_regr(dat, target = \"y\")\npoc = po(\"collapsefactors\", target_level_count = 5)\npoc$train(list(task))[[1]]$data() %>% count(color)\n#>    color  n\n#> 1:  dark  6\n#> 2:  fair 17\n#> 3: green  6\n#> 4:  grey 47\n#> 5: light 11\n```\n:::\n\n\n#### 因子修正\n\n管道运算“fixfactors”（参数：`droplevels = TRUE`）确保预测过程中的因子水平与训练过程中的相同；可能会在之前丢弃空的训练因子水平。\n\n注意，如果发现未见过的因子水平，这可能会在预测期间引入缺失值。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndattrain = data.table(\n  a = factor(c(\"a\", \"b\", \"c\", NA), levels = letters),\n  b = ordered(c(\"a\", \"b\", \"c\", NA)),\n  target = 1:4\n)\ndattrain\n#>       a    b target\n#> 1:    a    a      1\n#> 2:    b    b      2\n#> 3:    c    c      3\n#> 4: <NA> <NA>      4\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndattest = data.table(\n  a = factor(c(\"a\", \"b\", \"c\", \"d\"), levels = letters[10:1]),\n  b = ordered(c(\"a\", \"b\", \"c\", \"d\")),\n  target = 1:4\n)\ndattest\n#>    a b target\n#> 1: a a      1\n#> 2: b b      2\n#> 3: c c      3\n#> 4: d d      4\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask_train = as_task_regr(dattrain, \"target\")\ntask_test = as_task_regr(dattest, \"target\")\npo = po(\"fixfactors\")\npo$train(list(task_train))\n#> $output\n#> <TaskRegr:dattrain> (4 x 3)\n#> * Target: target\n#> * Properties: -\n#> * Features (2):\n#>   - fct (1): a\n#>   - ord (1): b\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npo$predict(list(task_test))[[1]]$data()\n#>    target    a    b\n#> 1:      1    a    a\n#> 2:      2    b    b\n#> 3:      3    c    c\n#> 4:      4 <NA> <NA>\n```\n:::\n\n\n#### 因子编码\n\n对因子、有序因子、字符特征进行编码。参数 `method` 指定编码方法：\n\n- “one-hot”：独热编码；\n\n- “treatment”：虚拟编码，创建 $n-1$ 列，留出每个因子变量的第一个因子水平（见`stats::conc.treatment()`）；\n\n- “helmert”：根据 Helmert 对比度创建列（见`stats::conc.helmert()`）；\n\n- “poly”：根据正交多项式创建对比列（见`stats::conc.poly()`）；\n\n- “sum”：创建对比度相加为零的列，（见`stats::conc.sum()`）\n\n新创建的列按模式 `[column-name].[x]` 命名，其中 `x` 是 ”one-hot” 和 ”treatment” 编码的各自因子水平，否则是一个整数序列。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"penguins\")\npoe = po(\"encode\", method = \"one-hot\")   # 独热编码\npoe$train(list(task))[[1]]$data()[, 7:11]\n#>      island.Biscoe island.Dream island.Torgersen sex.female sex.male\n#>   1:             0            0                1          0        1\n#>   2:             0            0                1          1        0\n#>   3:             0            0                1          1        0\n#>   4:             0            0                1         NA       NA\n#>   5:             0            0                1          1        0\n#>  ---                                                                \n#> 340:             0            1                0          0        1\n#> 341:             0            1                0          1        0\n#> 342:             0            1                0          0        1\n#> 343:             0            1                0          0        1\n#> 344:             0            1                0          1        0\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoe = po(\"encode\", method = \"treatment\")  # 虚拟编码\npoe$train(list(task))[[1]]$data()[, 7:9]\n#>      island.Dream island.Torgersen sex.male\n#>   1:            0                1        1\n#>   2:            0                1        0\n#>   3:            0                1        0\n#>   4:            0                1       NA\n#>   5:            0                1        0\n#>  ---                                       \n#> 340:            1                0        1\n#> 341:            1                0        0\n#> 342:            1                0        1\n#> 343:            1                0        1\n#> 344:            1                0        0\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoe = po(\"encode\", method = \"helmert\")  # Helmert 编码\npoe$train(list(task))[[1]]$data()[, 7:9]\n#>      island.1 island.2 sex.1\n#>   1:        0        2     1\n#>   2:        0        2    -1\n#>   3:        0        2    -1\n#>   4:        0        2    NA\n#>   5:        0        2    -1\n#>  ---                        \n#> 340:        1       -1     1\n#> 341:        1       -1    -1\n#> 342:        1       -1     1\n#> 343:        1       -1     1\n#> 344:        1       -1    -1\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoe = po(\"encode\", method = \"poly\")  # 多项编码\npoe$train(list(task))[[1]]$data()[, 7:9]\n#>           island.1   island.2      sex.1\n#>   1:  7.071068e-01  0.4082483  0.7071068\n#>   2:  7.071068e-01  0.4082483 -0.7071068\n#>   3:  7.071068e-01  0.4082483 -0.7071068\n#>   4:  7.071068e-01  0.4082483         NA\n#>   5:  7.071068e-01  0.4082483 -0.7071068\n#>  ---                                    \n#> 340: -7.850462e-17 -0.8164966  0.7071068\n#> 341: -7.850462e-17 -0.8164966 -0.7071068\n#> 342: -7.850462e-17 -0.8164966  0.7071068\n#> 343: -7.850462e-17 -0.8164966  0.7071068\n#> 344: -7.850462e-17 -0.8164966 -0.7071068\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoe = po(\"encode\", method = \"sum\")  # sum 编码\npoe$train(list(task))[[1]]$data()[, 7:9]\n#>      island.1 island.2 sex.1\n#>   1:       -1       -1    -1\n#>   2:       -1       -1     1\n#>   3:       -1       -1     1\n#>   4:       -1       -1    NA\n#>   5:       -1       -1     1\n#>  ---                        \n#> 340:        0        1    -1\n#> 341:        0        1     1\n#> 342:        0        1    -1\n#> 343:        0        1    -1\n#> 344:        0        1     1\n```\n:::\n\n\n::: {.callout-warning}\n以下内容暂缓：\n\n- 效应编码\n\n- 日期时间特征\n\n- 文本特征\n:::\n\n### 处理不均衡数据\n\n通过采样对任务进行类平衡，可能有利于对不平衡的训练数据进行分类，采样只发生在训练阶段。\n\n#### 欠采样与过采样\n\n欠采样：只保留多数类的一部分行；过采样：对少数类进行超量采样（重复数据点）。\n\nPipeOp 名字为 ”classbalancing”，参数：\n\n- ratio：相对于 `$reference` 值，要保留的类的行数的比率，默认为 1；\n\n- reference：`$ratio` 的值是根据什么来衡量的。可以是 “all”（默认值，所有类的平均实例数），“major”（拥有最多实例的类的实例数），“minor”（拥有最少实例的类的实例数），“nonmajor”（除主要类外所有类的平均实例数），“nonminor”（除次要类外所有类的平均实例数），以及 “one”（`$ratio` 决定每个类的实例数）；\n\n- adjust：哪些类要向上/向下采样。可以是 “all”（默认）、“major”、“minor”、“nonmajor”、“nonminor”、“upsample”（只过采样）和“downsample”（只欠采样）；\n\n- shuffle：是否对结果任务的行进行洗牌，默认为 `TRUE`。如果数据被过采样且 `shuffle = FALSE`，结果任务将以原始顺序显示原始行（在欠采样中不被移除），然后是所有新添加的行，按目标类别排序。\n\n过/欠采样的过程如下：\n\n- 首先，计算” 目标类计数”，方法是取 `reference` 参数所指示的所有类的平均数（例如，如果 reference 参数是 ”nonmajor”：所有不是 “major” 类的类的平均数，即拥有最多样本的类），然后将其与比率参数的值相乘。如果 reference 是 ”one”，那么“目标类计数” 就是比率的值（即 1 * 比率）。\n\n- 然后，对于每个被 `adjust` 参数引用的类（例如，若调整是“nonminor”：每个不是样本最少的类），PipeOpClassBalancing 要么抛出样本（欠采样），要么增加与随机选择的样本相等的额外行（过采样），直到这些类的样本数等于“目标类计数”。\n\n- 使用 `task$filter()` 来删除行。当在过采样过程中添加相同的行时，那么由于[混淆]，不能使用 `task$row_roles$use` 来复制行；而是使用 `task$rbind()` 函数，并附加一个新的 data.table，其中包含所有被复制的行，其次数正好与被添加的行相同。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"german_credit\")\ntable(task$truth())\n#> \n#> good  bad \n#>  700  300\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## 欠采样\nopb_down = po(\"classbalancing\", reference = \"minor\", adjust = \"major\")\n# 默认ratio = 1, 若ratio = 2, 结果是600 good, 300 bad\nresult = opb_down$train(list(task))[[1]]\ntable(result$truth())\n#> \n#> good  bad \n#>  300  300\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## 过采样\nopb_up = po(\"classbalancing\", reference = \"major\", adjust = \"minor\")\n# 默认ratio = 1, 若ratio = 2, 结果是700 good, 1400 bad\nresult = opb_up$train(list(task))[[1]]\ntable(result$truth())\n#> \n#> good  bad \n#>  700  700\n```\n:::\n\n\n#### SMOTE 法\n\n用 SMOTE 算法创建少数类别的合成观测，生成一个更平衡的数据集。该算法为每个少数类观测取样，基于该观测的 $K$ 个最近邻居生成新观测。它只能应用于具有纯数值特征的任务。详见 `smotefamily::SMOTE`。\n\nPipeOp 名字：“smote”，其参数（具体参阅 `SMOTE()`）：\n\n- K：用于抽取新值的近邻的数量；\n\n- dup_size：合成的少数实例在原始多数实例数量上的期望次数。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 只支持double 型特征, 需安装 smotefamily 包\npop = po(\"colapply\", applicator = as.numeric,\n         affect_columns = selector_type(\"integer\")) %>>%\n  po(\"encodeimpact\") %>>%\n  po(\"smote\", K = 5, dup_size = 1)  # 少数类增加 1 倍\nresult = pop$train(task)[[1]]\ntable(result$truth())\n#> \n#> good  bad \n#>  700  600\n```\n:::\n\n\n### 目标变换\n\n为了提高预测能力，对于异方差或右偏的因变量数据，经常需要做取对数变换、或 Box-Cox 变换，以变成正态数据，再进行回归建模，预测值要回到原数据量级，还要做逆变换。\n\nmlr3pipelines 包还提供了目标变换管道，将目标变换及其逆变换，都封装到图学习器，这样就省了手动做两次变换，还能整体进行调参或基准测试。\n\n对于方差逐渐变大的异方差的时间序列数据，或右偏分布的数据，可以尝试做对数变换或开平方变换，以稳定方差和变成正态分布。\n\n对数变换特别有用，因为具有可解释性：**对数值的变化是原始尺度上的相对（百分比）变化**。若使用以 10 为底的对数，则对数刻度上每增加 1 对应原始刻度上的乘以 10。\n\n注意，原始数据若存在零或负，则不能取对数或开根号，解决办法是做平移：$a = \\max \\{ 0, -\\min \\{x_i\\} + \\varepsilon \\}$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"mtcars\")\nlearner = lrn(\"regr.lm\")\ng_ppl = ppl(\"targettrafo\", graph = learner)\ng_ppl$param_set$values$targetmutate.trafo = function(x) log(x)\ng_ppl$param_set$values$targetmutate.inverter = function(x) list(response = exp(x$response))\ng_ppl$plot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-132-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngl = as_learner(g_ppl)\ngl$train(task)\ngl$predict(task)\n#> <PredictionRegr> for 32 observations:\n#>     row_ids truth response\n#>           1  21.0 21.67976\n#>           2  21.0 21.10831\n#>           3  22.8 25.73690\n#> ---                       \n#>          30  19.7 19.58533\n#>          31  15.0 14.11015\n#>          32  21.4 23.11105\n```\n:::\n\n\n# 嵌套重抽样\n\n构建模型，是如何从一组潜在的候选模型（如不同的算法，不同的超参数，不同的特征子集）中选择最佳模型。在构建模型过程中所使用的重抽样划分，不应该原样用来评估最终选择模型的性能。\n\n通过在相同的测试集或相同的 CV 划分上反复评估学习器，测试集的信息会“泄露”到评估中，导致最终的性能估计偏于乐观。\n\n模型构建的所有部分（包括模型选择、预处理）都应该纳入到训练数据的模型寻找过程中。测试集应该只使用一次，测试集只有在模型完全训练好之后才能被使用，例如已确定好了超参数。这样从测试集获得的性能才是真实性能的无偏估计。\n\n对于本身需要重抽样的步骤（如超参数调参），这需要两个嵌套的重抽样循环，即内层调参和外层评估都需要重抽样策略。\n\n下面从实例来看：\n\n1. 使用3 折交叉验证来获得不同的非测试集和测试集（外层重抽样）\n\n2. 在非测试集上使用 4 折交叉验证来获得不同的内层训练集和内层测试集（内层重抽样）\n\n3. 使用内层数据划分来做超参数调参\n\n4. 使用经过内层重抽样调参的最优超参数在外层非测试集上拟合学习器\n\n5. 评估学习器在外层测试集上的性能\n\n6. 对外层重抽样的三折中的每一折重复执行 2 - 5\n\n7. 三个性能值被汇总为一个无偏的性能估计\n\n过程看起来复杂，实现非常简单，只需要将内层重抽样放在自动调参器，外层重抽样放在 `resample()` 中，其中的学习器换成自动调参器。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 内层重抽样（超参数调参）\nat = auto_tuner(\n  tuner = tnr(\"grid_search\", resolution = 10),\n  learner = lrn(\"classif.rpart\", cp = to_tune(lower = .001, upper = .1)),\n  resampling = rsmp(\"cv\", folds = 4),\n  measure = msr(\"classif.acc\"),\n  term_evals = 5\n)\n\n# 外层重抽样\ntask = tsk(\"pima\")\nouter_resampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, at, outer_resampling, store_models = TRUE)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 提取内层重抽样结果，查看最优的超参数\nextract_inner_tuning_results(rr)\n#>    iteration    cp classif.acc learner_param_vals  x_domain task_id\n#> 1:         1 0.067   0.7480469          <list[2]> <list[1]>    pima\n#> 2:         2 0.012   0.7578125          <list[2]> <list[1]>    pima\n#> 3:         3 0.012   0.7343750          <list[2]> <list[1]>    pima\n#>             learner_id resampling_id\n#> 1: classif.rpart.tuned            cv\n#> 2: classif.rpart.tuned            cv\n#> 3: classif.rpart.tuned            cv\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 查看外层重抽样的每次结果\nrr$score()\n#>    task_id          learner_id resampling_id iteration classif.ce\n#> 1:    pima classif.rpart.tuned            cv         1  0.2968750\n#> 2:    pima classif.rpart.tuned            cv         2  0.2695312\n#> 3:    pima classif.rpart.tuned            cv         3  0.2578125\n#> Hidden columns: task, learner, resampling, prediction\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 查看外层重抽样的平均模型性能\nrr$aggregate()\n#> classif.ce \n#>  0.2747396\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 用全部数据自动调参，预测新数据\nat$train(task)\ndat = task$data()[1:5, -1]\nat$predict_newdata(dat)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 若改用tune_nested() 函数，嵌套重抽样过程还可以进一步简化\nrr = tune_nested(\n  tuner = tnr(\"grid_search\", resolution = 10),\n  task = task,\n  learner = lrn(\"classif.rpart\", cp = to_tune(lower = .001, upper = .1)),\n  inner_resampling = rsmp(\"cv\", folds = 4),\n  outer_resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.acc\"),\n  term_evals = 5\n)\n```\n:::\n\n\n# 超参数调参\n\n机器学习的模型参数是模型的一阶（直接）参数，是训练模型时用梯度下降法寻优的参数，比如正则化回归模型的回归系数；而超参数是模型的二阶参数，需要事先设定为某值，才能开始训练一阶模型参数，比如正则化回归模型的惩罚参数、KNN 的邻居数等。\n\n超参数会对所训练模型的性能产生重大影响，所以不能是随便或凭经验随便指定，而是需要设定很多种备选配置，从中选出让模型性能最优的超参数配置，这就是**超参数调参**。\n\n超参数调参是用 mlr3tuning 包实现，是一项多方联动的系统工作，需要设定：搜索空间、学习器、任务、重抽样策略、模型性能度量指标、终止条件。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.svm\")\nlearner$param_set  # 查看学习器的超参数\n#> <ParamSet>\n#>                id    class lower upper nlevels          default parents value\n#>  1:     cachesize ParamDbl  -Inf   Inf     Inf               40              \n#>  2: class.weights ParamUty    NA    NA     Inf                               \n#>  3:         coef0 ParamDbl  -Inf   Inf     Inf                0  kernel      \n#>  4:          cost ParamDbl     0   Inf     Inf                1    type      \n#>  5:         cross ParamInt     0   Inf     Inf                0              \n#> ---                                                                          \n#> 12:            nu ParamDbl  -Inf   Inf     Inf              0.5    type      \n#> 13:         scale ParamUty    NA    NA     Inf             TRUE              \n#> 14:     shrinking ParamLgl    NA    NA       2             TRUE              \n#> 15:     tolerance ParamDbl     0   Inf     Inf            0.001              \n#> 16:          type ParamFct    NA    NA       2 C-classification\n```\n:::\n\n\n## 独立调参\n\n适合传统的机器学习套路：将数据集按留出法重抽样划分为训练集和测试集，在训练集上做内层重抽样，多次“训练集拟合模型+ 验证集评估性能”，根据平均性能选出最优超参数。\n\n1. 选取任务，划分数据集，将测试集索引设置为留出\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"iris\")\nsplit = partition(task, ratio = .8)\ntask$set_row_roles(split$test, \"holdout\")\n```\n:::\n\n\n2. 选择学习器，同时指定部分超参数，需要调参的超参数用 `to_tune()` 设定搜索范围：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.svm\", type = \"C-classification\", kernel = \"radial\",\n              cost = to_tune(.1, 10), gamma = to_tune(0, 5))\n```\n:::\n\n\n3. 用 `tune()` 对学习器做超参数调参，需要设定（有些是可选）：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = tune(\n  tuner = tnr(\"grid_search\", batch_size = 5),\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 5),\n  measures = msr(\"classif.ce\")\n)\n```\n:::\n\n\n4. 提取超参数调参结果\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance$result\n#>    cost     gamma learner_param_vals  x_domain classif.ce\n#> 1:  5.6 0.5555556          <list[4]> <list[2]> 0.03333333\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance$archive  # 调参档案\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(instance, type = \"surface\", cols_x = c(\"cost\", \"gamma\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-146-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n5. 用最优超参数更新学习器参数，在训练集上训练模型，在测试集上做预测\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner$param_set$values = instance$result_learner_param_vals\nlearner$train(task)\npredictions = learner$predict(task, row_ids = split$test)\npredictions\n#> <PredictionClassif> for 30 observations:\n#>     row_ids     truth  response\n#>           5    setosa    setosa\n#>          13    setosa    setosa\n#>          15    setosa    setosa\n#> ---                            \n#>         142 virginica virginica\n#>         144 virginica virginica\n#>         146 virginica virginica\n```\n:::\n\n\n## 自动调参器\n\n上述独立调参有两个不足：\n\n- 调参得到的最优超参数需要手动更新到学习器；\n\n- 不方便实现嵌套重抽样。\n\n自动调参器可以弥补上述不足，AutoTuner 是将超参数调参与学习器封装到一起，能实现自动调参的过程，并且可以像其他学习器一样使用。\n\n1. 创建任务、选择学习器\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"iris\")\nlearner = lrn(\"classif.svm\", type = \"C-classification\", kernel = \"radial\")\n```\n:::\n\n\n2. 用 `ps()` 生成搜索空间，用 `auto_tuner()` 创建自动调参器，注意：不需要提供任务，其它参数与独立调参的 `tune()` 基本一样：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsearch_space = ps(cost = p_dbl(0.1, 10), gamma = p_int(0, 5))\nat = auto_tuner(\n  tuner = tnr(\"grid_search\"),\n  learner = learner,\n  search_space = search_space,\n  resampling = rsmp(\"cv\", folds = 5),\n  measure = msr(\"classif.ce\")\n)\n```\n:::\n\n\n3. 外层采用 4 折交叉验证重抽样，设置 `store_models = TRUE` 保存每次的模型：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr = resample(task, at, rsmp(\"cv\", folds = 4), store_models = TRUE)\n```\n:::\n\n\n这就是执行嵌套重抽样，外层（整体拟合模型+ 评估）循环 4 次，每次内层（超参数调参）循环 5 次。\n\n4. 查看结果：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr$aggregate()  # 总的平均模型性能, 也可以提供其它度量\n#> classif.ce \n#> 0.06009957\nrr$score()      # 外层 4 次迭代的每次结果\n#>    task_id        learner_id resampling_id iteration classif.ce\n#> 1:    iris classif.svm.tuned            cv         1 0.07894737\n#> 2:    iris classif.svm.tuned            cv         2 0.02631579\n#> 3:    iris classif.svm.tuned            cv         3 0.05405405\n#> 4:    iris classif.svm.tuned            cv         4 0.08108108\n#> Hidden columns: task, learner, resampling, prediction\nextract_inner_tuning_results(rr)  # 内层每次的调参结果\n#>    iteration cost gamma classif.ce learner_param_vals  x_domain task_id\n#> 1:         1 10.0     1 0.04505929          <list[4]> <list[2]>    iris\n#> 2:         2  1.2     1 0.05375494          <list[4]> <list[2]>    iris\n#> 3:         3  2.3     3 0.04466403          <list[4]> <list[2]>    iris\n#> 4:         4  8.9     1 0.05335968          <list[4]> <list[2]>    iris\n#>           learner_id resampling_id\n#> 1: classif.svm.tuned            cv\n#> 2: classif.svm.tuned            cv\n#> 3: classif.svm.tuned            cv\n#> 4: classif.svm.tuned            cv\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nextract_inner_tuning_archives(rr) # 内层调参档案\n```\n:::\n\n\n5. 在全部数据上自动调参，预测新数据\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nat$train(task)\ndat = task$data()[1:5, -1]\nat$predict_newdata(dat)\n```\n:::\n\n\n注：调参结束后，也可以取出最优超参数，更新学习器参数：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner$param_set$values = at$tuning_result$learner_param_vals[[1]]\n```\n:::\n\n\n或者按最优超参数重新创建学习器。\n\n另外，上述的“自动调参+ 外层重抽样”，若改用嵌套调参 `tune_nested()` 实现，代码会更加简洁：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr = tune_nested(\n  tuner = tnr(\"grid_search\"),\n  task = task,\n  learner = learner,\n  search_space = search_space,\n  inner_resampling = rsmp(\"cv\", folds = 5),\n  outer_resampling = rsmp(\"cv\", folds = 4),\n  measure = msr(\"classif.ce\")\n)\n```\n:::\n\n\n## 定制搜索空间\n\n用 `ps()` 创建搜索空间，它支持 5 种超参数类型构建：\n\n- `p_dbl()`：实值型（double）\n\n- `p_int()`：整数型\n\n- `p_fct()`：离散型（factor）\n\n- `p_lgl()`：逻辑性\n\n- `p_uty()`：Untyped\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsearch_space = ps(cost = p_dbl(.1, 10),\n                  kernel = p_fct(c(\"polynomial\", \"radial\")))\n```\n:::\n\n\n### 变换\n\n对于数值型超参数，在调参时经常希望前期的点比后期的点更密集一些。这可以通过对数-指数变换来实现，这也适用于大范围搜索空间。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntibble(x = 1:20,\n       y = exp(seq(log(3), log(50), length.out = 20))) %>% \n  ggplot(aes(x, y)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-157-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n当 $x$ 均匀变化时，变换后作为超参数能前密后疏。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsearch_space = ps(\n  cost = p_dbl(log(.1), log(10), trafo = \\(x) exp(x)),\n  kernel = p_fct(c(\"polynomial\", \"radial\"))\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 查看调参网格\nparams = generate_design_grid(search_space, resolution = 5)\nparams\n#> <Design> with 10 rows:\n#>              cost     kernel\n#>  1: -2.302585e+00 polynomial\n#>  2: -2.302585e+00     radial\n#>  3: -1.151293e+00 polynomial\n#>  4: -1.151293e+00     radial\n#>  5:  2.220446e-16 polynomial\n#>  6:  2.220446e-16     radial\n#>  7:  1.151293e+00 polynomial\n#>  8:  1.151293e+00     radial\n#>  9:  2.302585e+00 polynomial\n#> 10:  2.302585e+00     radial\n```\n:::\n\n\n### 依赖关系\n\n有些超参数只有在另一个参数取某些值时才有意义。例如，支持向量机有一个 `degree` 参数，只有在 `kernel` 为 ”polynomial” 时才有效。这可以用 `depends` 参数来指定：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsearch_space = ps(\n  cost = p_dbl(log(.1), log(10), trafo = \\(x) exp(x)),\n  kernel = p_fct(c(\"polynomial\", \"radial\")),\n  degree = p_int(1, 3, depends = kernel == \"polynomial\")\n)\n```\n:::\n\n\n### 图学习器调参\n\n图学习器一旦成功创建，就可以像普通学习器一样使用，超参数调参时，原算法的超参数名字都自动带了学习器名字前缀，另外还可以对管道参数调参。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"pima\")\n```\n:::\n\n\n该任务包含缺失值，还有若干因子特征，都需要做预处理：插补和特征工程。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprep = gunion(list(\n  po(\"imputehist\"),\n  po(\"missind\", affect_columns = selector_type(c(\"numeric\", \"integer\")))\n)) %>>%\n  po(\"featureunion\") %>>%\n  po(\"encode\") %>>%\n  po(\"removeconstants\")\n```\n:::\n\n\n选择三个学习器：KNN、SVM、Ranger 作为三分支分别拟合模型，再合并分支保证是一个输出结果：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearners = list(\n  knn = lrn(\"classif.kknn\", id = \"kknn\"),\n  svm = lrn(\"classif.svm\", id = \"svm\", type = \"C-classification\"),\n  rf = lrn(\"classif.ranger\", id = \"ranger\")\n)\ngraph = ppl(\"branch\", learners)\n```\n:::\n\n\n将预处理图和算法图连接得到整个图，并可视化图：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph = prep %>>% graph\ngraph$plot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-164-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n转化为图学习器，查看其超参数：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglearner = as_learner(graph)\nglearner$param_set\n#> <ParamSetCollection>\n#>                            id    class lower upper nlevels          default\n#>  1:          branch.selection ParamFct    NA    NA       3   <NoDefault[3]>\n#>  2:     encode.affect_columns ParamUty    NA    NA     Inf    <Selector[1]>\n#>  3:             encode.method ParamFct    NA    NA       5   <NoDefault[3]>\n#>  4: imputehist.affect_columns ParamUty    NA    NA     Inf   <NoDefault[3]>\n#>  5:             kknn.distance ParamDbl     0   Inf     Inf                2\n#> ---                                                                        \n#> 59:                    svm.nu ParamDbl  -Inf   Inf     Inf              0.5\n#> 60:                 svm.scale ParamUty    NA    NA     Inf             TRUE\n#> 61:             svm.shrinking ParamLgl    NA    NA       2             TRUE\n#> 62:             svm.tolerance ParamDbl     0   Inf     Inf            0.001\n#> 63:                  svm.type ParamFct    NA    NA       2 C-classification\n#>      parents            value\n#>  1:                       knn\n#>  2:                          \n#>  3:                   one-hot\n#>  4:                          \n#>  5:                          \n#> ---                          \n#> 59: svm.type                 \n#> 60:                          \n#> 61:                          \n#> 62:                          \n#> 63:          C-classification\n```\n:::\n\n\n可见，所有超参数都多了学习器或管道名字前缀，比如 kknn.k 是KNN 学习器的邻居数参数 k。\n\n嵌套重抽样超参数调参，与前文语法一样，为了加速计算，启动并行：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfuture::plan(\"multicore\")\n```\n:::\n\n\n设置搜索空间，用 `tune_nested()` 做嵌套调参：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsearch_space = ps(\n  branch.selection = p_fct(c(\"kknn\", \"svm\", \"ranger\")),\n  kknn.k = p_int(3, 50, logscale = TRUE,\n                 depends = branch.selection == \"kknn\"),\n  svm.cost = p_dbl(-1, 1, trafo = \\(x) 10^x,\n                   depends = branch.selection == \"svm\"),\n  ranger.mtry = p_int(1, 8,\n                      depends = branch.selection == \"ranger\")\n)\nrr = tune_nested(\n  tuner = tnr(\"random_search\"),\n  task = task,\n  learner = glearner,\n  inner_resampling = rsmp(\"cv\", folds = 3),\n  outer_resampling = rsmp(\"cv\", folds = 4),\n  measure = msr(\"classif.ce\"),\n  term_evals = 10\n)\n```\n:::\n\n\n查看结果：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr$aggregate()    # 总的平均模型性能\n#> classif.ce \n#>   0.296875\nrr$score()        # 外层 4 次迭代每次的模型性能\n#>    task_id\n#> 1:    pima\n#> 2:    pima\n#> 3:    pima\n#> 4:    pima\n#>                                                                                      learner_id\n#> 1: imputehist.missind.featureunion.encode.removeconstants.branch.kknn.svm.ranger.unbranch.tuned\n#> 2: imputehist.missind.featureunion.encode.removeconstants.branch.kknn.svm.ranger.unbranch.tuned\n#> 3: imputehist.missind.featureunion.encode.removeconstants.branch.kknn.svm.ranger.unbranch.tuned\n#> 4: imputehist.missind.featureunion.encode.removeconstants.branch.kknn.svm.ranger.unbranch.tuned\n#>    resampling_id iteration classif.ce\n#> 1:            cv         1  0.3854167\n#> 2:            cv         2  0.2239583\n#> 3:            cv         3  0.3020833\n#> 4:            cv         4  0.2760417\n#> Hidden columns: task, learner, resampling, prediction\n```\n:::\n\n\n# 特征选择\n\n当数据集包含很多特征时，只提取最重要的部分特征来建模，称为**特征选择**。特征选择可以增强模型的解释性、加速学习过程、改进学习器性能。\n\n常用的特征选择方法有两种：过滤法、包装法。另外，有些学习器内部提供了选择有助于做预测的特征子集的方法，称为是嵌入法。\n\n## 过滤法\n\n过滤法，基于某种衡量特征重要度的指标（如相关系数），用外部算法计算变量的排名，只选用排名靠前的若干特征，用 mlr3filters 包实现。\n\n### 基于重要度指标\n\n过滤法给每个特征计算一个重要度指标值，基于此可以对特征进行排序，然后就可以选出特征子集。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"sonar\")\nfilter = flt(\"auc\")\nas.data.table(filter$calculate(task))\n#>     feature       score\n#>  1:     V11 0.281136807\n#>  2:     V12 0.242918176\n#>  3:     V10 0.232701774\n#>  4:     V49 0.231262190\n#>  5:      V9 0.230844246\n#> ---                    \n#> 56:     V60 0.012213244\n#> 57:     V30 0.006965729\n#> 58:     V57 0.004876010\n#> 59:     V55 0.003204235\n#> 60:     V40 0.002182595\n```\n:::\n\n\n### 基于学习器的变量重要度\n\n有些学习器可以计算变量重要度，特别是基于树的模型，目前支持：\n\n\"classif.featureless\"，\"regr.featureless\"，\"classif.ranger\"，\"regr.ranger\"，\"classif.rpart\"，\"regr.rpart\"，\"classif.xgboost\"，\"regr.xgboost\"\n\n有些学习器需要在创建时” 激活” 其变量重要性度量。例如，通过 ranger 包来使用随机森林的 ”impurity” 度量：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"iris\")\nlearner = lrn(\"classif.ranger\", importance = \"impurity\")\nfilter = flt(\"importance\", learner = learner)\nfilter$calculate(task)\nas.data.table(filter)\n#>         feature     score\n#> 1:  Petal.Width 45.511305\n#> 2: Petal.Length 41.987469\n#> 3: Sepal.Length  9.381471\n#> 4:  Sepal.Width  2.361094\n```\n:::\n\n\n使用上述特征选择可以对特征得分可视化，根据肘法确定保留特征数，然后用 `task$select()` 选择特征；也可以直接通过管道连接学习器构建图学习器：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"spam\")\ngraph = po(\"filter\", filter = flt(\"auc\"), filter.frac = .5) %>>%\n  po(\"learner\", lrn(\"classif.rpart\"))\ngraph$plot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-171-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = as_learner(graph)\nrr = resample(task, learner, rsmp(\"cv\", folds = 5))\n#> INFO  [15:14:48.723] [mlr3] Applying learner 'auc.classif.rpart' on task 'spam' (iter 1/5)\n#> INFO  [15:14:48.900] [mlr3] Applying learner 'auc.classif.rpart' on task 'spam' (iter 2/5)\n#> INFO  [15:14:49.141] [mlr3] Applying learner 'auc.classif.rpart' on task 'spam' (iter 3/5)\n#> INFO  [15:14:49.364] [mlr3] Applying learner 'auc.classif.rpart' on task 'spam' (iter 4/5)\n#> INFO  [15:14:49.552] [mlr3] Applying learner 'auc.classif.rpart' on task 'spam' (iter 5/5)\nrr$aggregate()\n#> classif.ce \n#>  0.1051952\n```\n:::\n\n\n## 包装法\n\n包装法，随机选择部分特征拟合模型并评估模型性能，通过交叉验证找到最佳的特征子集，用 mlr3fselect 包实现。\n\n包装法特征选择，与超参数调参道理完全一样。\n\n### 独立特征选择\n\n适合传统的机器学习套路：将数据集按留出法重抽样划分为训练集和测试集，在训练集上做内层重抽样，多次“训练集拟合模型 + 验证集评估性能”，根据平均性能选出最优特征子集。\n\n1. 选取任务，划分数据集，将测试集索引设置为留出\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"pima\")\nsplit = partition(task, ratio = .8)\ntask$set_row_roles(split$test, \"holdout\")\n```\n:::\n\n\n2. 选择学习器\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.rpart\")\n```\n:::\n\n\n3. 用 `fselect()` 对学习器做特征选择\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = fselect(\n  fselector = fs(\"rfe\"),\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 4),\n  measures = msr(\"classif.ce\"),\n  term_evals = 10,\n  store_models = TRUE\n)\n```\n:::\n\n\n查看特征选择结果：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance$result  # 最佳特征子集\n#>     age glucose insulin mass pedigree pregnant pressure triceps\n#> 1: TRUE    TRUE   FALSE TRUE     TRUE    FALSE    FALSE   FALSE\n#>                     features classif.ce\n#> 1: age,glucose,mass,pedigree  0.2801227\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance$archive  # 特征选择档案\n#> <ArchiveFSelect>\n#>     age glucose insulin mass pedigree pregnant pressure triceps classif.ce\n#> 1: TRUE    TRUE    TRUE TRUE     TRUE     TRUE     TRUE    TRUE       0.29\n#> 2: TRUE    TRUE   FALSE TRUE     TRUE    FALSE    FALSE   FALSE       0.28\n#>    warnings errors runtime_learners  x_domain              timestamp batch_nr\n#> 1:        0      0             0.08 <list[8]> 2023-09-12 15:14:50.35        1\n#> 2:        0      0             0.07 <list[8]> 2023-09-12 15:14:50.65        2\n#>                     importance\n#> 1: 8.0,6.8,5.2,4.8,4.8,2.8,...\n#> 2:             4.0,2.5,2.0,1.5\n```\n:::\n\n\n4. 对任务选择特征自己，拟合最终模型\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask$select(instance$result_feature_set)\nlearner$train(task)\n```\n:::\n\n\n### 自动特征选择\n\n上述独立特征选择有两个不足：\n\n- 特征选择得到的最优特征子集需要手动更新到任务；\n\n- 不方便实现嵌套重抽样。\n\n自动特征选择器可以弥补上述不足，AutoFSelector 是将特征选择与学习器封装到一起，能实现自动特征选择的过程，并且可以像其他学习器一样使用。\n\n1. 创建任务、选择学习器\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"pima\")\nlearner = lrn(\"classif.rpart\")\n```\n:::\n\n\n2. 用 `auto_fselector()` 创建自动特征选择器，注意：不需要提供任务，其它参数与独立特征选择的 `fselect()` 基本一样：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nafs = auto_fselector(\n  fselector = fs(\"random_search\", batch_size = 5),\n  learner = lrn(\"classif.rpart\"),\n  resampling = rsmp(\"cv\", folds = 4),\n  measure = msr(\"classif.ce\"),\n  term_evals = 10\n)\n```\n:::\n\n\n3. 外层采用 5 折交叉验证重抽样，设置 `store_models = TRUE` 保存每次的模型：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr = resample(task, afs, rsmp(\"cv\", folds = 5), store_models = TRUE)\n```\n:::\n\n\n查看结果：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr$aggregate()  # 总的平均模型性能, 也可以提供其它度量\n#> classif.ce \n#>  0.2746796\n```\n:::\n\n\n注：若外层重抽样不想做这么多次，可直接用留出重抽样，或者划分训练集测试集，然后将 afs 当学习器使用。\n\n另外，上述的“自动特征选择+ 外层重抽样”，若改用嵌套特征选择 `fselect_nested()` 实现，代码会更加简洁：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr = fselect_nested(\n  fselector = fs(\"random_search\", batch_size = 5),\n  task = tsk(\"pima\"),\n  learner = lrn(\"classif.rpart\"),\n  inner_resampling = rsmp(\"cv\", folds = 4),\n  outer_resampling = rsmp(\"cv\", folds = 5),\n  measure = msr(\"classif.ce\"),\n  term_evals = 10\n)\n```\n:::\n\n\n# 模型解释\n\n机器学习模型预测性能强大，但天生不好解释。R 有两个通用框架致力于机器学习模型的解释（支持但不属于 mlr3verse）：iml 包和 DALEX 包。\n\n## iml 包\n\niml 包提供了分析任何黑盒机器学习模型的工具。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat = tsk(\"penguins\")$data() %>% na.omit()\ntask = as_task_classif(dat, target = \"species\")\nlearner = lrn(\"classif.ranger\", predict_type = \"prob\")\nlearner$train(task)\nlearner$model\n#> Ranger result\n#> \n#> Call:\n#>  ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      probability = self$predict_type == \"prob\", case.weights = task$weights$weight,      num.threads = 1L) \n#> \n#> Type:                             Probability estimation \n#> Number of trees:                  500 \n#> Sample size:                      333 \n#> Number of independent variables:  7 \n#> Mtry:                             2 \n#> Target node size:                 10 \n#> Variable importance mode:         none \n#> Splitrule:                        gini \n#> OOB prediction error (Brier s.):  0.01793959\n```\n:::\n\n\niml 包中的所有解释方法都需要机器学习模型和数据封装在 Predictor 对象中。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(iml)\n```\n:::\n\n\n### 特征效应\n\n特征效应（FeatureEffects），是计算所有给定特征对模型预测的影响。实现了不同的方法：累积局部效应（ALE）图，部分依赖图（PDP）和个体条件期望（ICE）曲线。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmod = Predictor$new(learner, data = dat, y = \"species\")\neffect = FeatureEffects$new(mod)\neffect$plot(features = c(\"bill_length\", \"bill_depth\",\n                         \"flipper_length\", \"body_mass\", \"year\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-186-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n可见，除了年份，所有的特征都提供了有意义的可解释信息。\n\n### 夏普利值\n\n计算具有夏普利（Shapley）值的单个观测的特征贡献（一种来自合作博弈理论的方法），即单个数据点的特征值是如何影响其预测的。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmod = Predictor$new(learner, data = dat, y = \"species\")\nshapley = Shapley$new(mod, x.interest = dat[1,])\nplot(shapley)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-187-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n$\\phi$值给出了纵轴上数值的概率增加或减少，例如，如果 `bill_depth = 18.7`，那么一只企鹅是 Gentoo 企鹅的可能性就比较小，而是 Adelie 企鹅的可能性则比 Chinstrap 企鹅大得多。\n\n### 特征重要度\n\n根据特征重排后模型的预测误差的增量，计算特征的重要度。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmod = Predictor$new(learner, data = dat, y = \"species\")\nimp = FeatureImp$new(mod, loss = \"ce\")\nimp$plot(features = c(\"bill_length\", \"bill_depth\",\n                      \"flipper_length\", \"body_mass\", \"year\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-188-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n可见，bill_length 具有很高的特征重要度。\n\n注：以上模型解释也可以分别在训练集和测试集上来做，以看对比效果。\n\n## DALEX 包\n\nDALEX 包可以透视任何模型并帮助探索和解释其行为，用各种方法帮助理解输入变量和模型输出之间的联系，有助于在单个观测以及整个数据特征的层面上解释模型。所有的模型解释器都是与模型无关的，可以在不同的模型之间进行比较。该包是 “DrWhy.AI” 可视化模型解释系列包的基石。\n\n![模型解释方法分类](DALEX.jpg){#fig-DALEX}\n\n以 fifa 数据，预测球员价值 value_eur 为例。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(DALEX)\nlibrary(DALEXtra)\nfifa[, c(\"nationality\", \"overall\", \"potential\", \"wage_eur\")] = NULL\nfifa = fifa %>% mutate(across(everything(), .fns = as.numeric))\n```\n:::\n\n\n创建任务，选择包含 250 棵决策树的随机森林学习器，训练模型：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = as_task_regr(fifa, target = \"value_eur\")\nranger = lrn(\"regr.ranger\", num.trees = 250)\nranger$train(task)\nranger$model\n#> Ranger result\n#> \n#> Call:\n#>  ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      case.weights = task$weights$weight, num.threads = 1L, num.trees = 250L) \n#> \n#> Type:                             Regression \n#> Number of trees:                  250 \n#> Sample size:                      5000 \n#> Number of independent variables:  37 \n#> Mtry:                             6 \n#> Target node size:                 5 \n#> Variable importance mode:         none \n#> Splitrule:                        variance \n#> OOB prediction error (MSE):       1.048416e+13 \n#> R squared (OOB):                  0.8666863\n```\n:::\n\n\n在开始解释模型行为之前，先创建一个解释器：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nranger_exp = explain_mlr3(ranger, data = fifa, y = fifa$value_eur,\n                          label = \"Ranger RF\")\n#> Preparation of a new explainer is initiated\n#>   -> model label       :  Ranger RF \n#>   -> data              :  5000  rows  38  cols \n#>   -> target variable   :  5000  values \n#>   -> predict function  :  yhat.LearnerRegr  will be used (  default  )\n#>   -> predicted values  :  No value for predict function target column. (  default  )\n#>   -> model_info        :  package mlr3 , ver. 0.16.1 , task regression (  default  ) \n#>   -> predicted values  :  numerical, min =  452214 , mean =  7473663 , max =  88491700  \n#>   -> residual function :  difference between y and yhat (  default  )\n#>   -> residuals         :  numerical, min =  -7777793 , mean =  -375.8719 , max =  17675900  \n#>   A new explainer has been created!\n```\n:::\n\n\n### 特征层面的解释\n\n`model_parts()` 函数基于排列组合的重要度来计算变量的重要度：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfifa_vi = model_parts(ranger_exp)\nhead(fifa_vi)\n#>              variable mean_dropout_loss     label\n#> 1        _full_model_           1334872 Ranger RF\n#> 2           value_eur           1334872 Ranger RF\n#> 3    movement_balance           1397653 Ranger RF\n#> 4           weight_kg           1398740 Ranger RF\n#> 5 goalkeeping_kicking           1405245 Ranger RF\n#> 6           height_cm           1412798 Ranger RF\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(fifa_vi, max_vars = 12, show_boxplots = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-193-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n一旦知道了哪些变量是最重要的，就可以使用部分依赖图来显示模型（平均而言）如何随着选定变量的变化而变化。在本例中，它们显示了特定变量和球员价值之间的平均关系。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvars = c(\"age\", \"movement_reactions\", \"skill_ball_control\", \"skill_dribbling\")\nfifa_pd = model_profile(ranger_exp, variables = vars)$agr_profiles\nfifa_pd\n#> Top profiles    : \n#>              _vname_   _label_ _x_  _yhat_ _ids_\n#> 1 skill_ball_control Ranger RF   5 7175057     0\n#> 2    skill_dribbling Ranger RF   7 7750317     0\n#> 3    skill_dribbling Ranger RF  11 7735097     0\n#> 4    skill_dribbling Ranger RF  12 7734165     0\n#> 5    skill_dribbling Ranger RF  13 7732068     0\n#> 6    skill_dribbling Ranger RF  14 7730236     0\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(fifa_pd)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-195-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n可见，大多数球员特征的一般趋势是相同的。技能越高，球员的价值就越高，只有一个变量例外——年龄。\n\n### 观测层面的解释\n\n探索模型在单个观测/球员身上的表现，下面以 Cristiano Ronaldo 为例。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nronaldo = fifa[\"Cristiano Ronaldo\", ]\nronaldo_bd = predict_parts(ranger_exp, new_observation = ronaldo)\nhead(ronaldo_bd)\n#>                                       contribution\n#> Ranger RF: intercept                       7473663\n#> Ranger RF: movement_reactions = 96        10954295\n#> Ranger RF: skill_ball_control = 92         8641092\n#> Ranger RF: mentality_positioning = 95      5606252\n#> Ranger RF: attacking_finishing = 94        4532342\n#> Ranger RF: skill_dribbling = 89            4401913\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(ronaldo_bd)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-197-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n上图展示了各变量对最终预测的贡献估计。Ronaldo 是一名前锋，因此影响他身价的特征是那些与进攻有关的特征，如 attacking_ volleys 或 skill_dribbling。唯一具有负向影响的特征是 age。\n\n检查模型的局部行为的另一种方法是用 SHapley Additive exPlanations（SHAP）。它在局部显示了变量对单一观测的贡献。\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nronaldo_shap = predict_parts(ranger_exp, new_observation = ronaldo,\n                             type = \"shap\")\nplot(ronaldo_shap)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-198-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n特征效应有部分依赖关系图，对于观测也有相应的版本：Ceteris Paribus。它显示了当只改变一个变量而其他变量保持不变时，模型对观测的反应（蓝点代表原始值）：\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nronaldo_cp = predict_profile(ranger_exp, ronaldo, variables = vars)\nplot(ronaldo_cp, variables = vars)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-199-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n更多机器学习模型解释理论方法请参阅 Interpretable Machine Learning: A Guide for Making Black\nBox Models Explainable.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}