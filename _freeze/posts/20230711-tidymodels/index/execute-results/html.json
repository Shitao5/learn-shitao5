{
  "hash": "0d6374febf0e3d8489757d5480976240",
  "result": {
    "markdown": "---\ntitle: \"ç†Ÿæ‚‰ Tidymodels\"\ndate: \"2023-07-11\"\ndate-modified: \"2023-07-23\"\nimage: \"tidymodels.png\"\nexecute: \n  cache: true\ncategories: \n  - Data Science\n  - R\n  - tidymodels\n---\n\n\n::: {.callout-note title='Progress'}\nLearning Progress: Completed. 100%.ðŸ˜¸\n:::\n\n::: {.callout-tip title=\"Learning Source\"}\n- <https://www.tidymodels.org/>\n:::\n\n# tidymodels ä¸­çš„åŒ…\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-1_56d0a60adec037f6b8a23e61da021096'}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n\n## rsample\n\nå®˜ç½‘ï¼š<https://rsample.tidymodels.org/>\n\n### Index\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-3_408d9e0eb7dcbc1b94fb931985ef3ecc'}\n\n```{.r .cell-code}\nlibrary(mlbench)\n\ndata(\"LetterRecognition\")\nlobstr::obj_size(LetterRecognition)\n#> 2.64 MB\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-4_ca547bdb9ddba849177784039ea6b7f4'}\n\n```{.r .cell-code}\nset.seed(35222)\nboots <- bootstraps(LetterRecognition, times = 50)\nlobstr::obj_size(boots)\n#> 6.69 MB\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-5_0b88fa94380bc27449bd78ee4782bd52'}\n\n```{.r .cell-code}\n# Object size per resample\nlobstr::obj_size(boots) / nrow(boots)\n#> 133.74 kB\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-6_16d47f4def1300f08497cce7c63fffb9'}\n\n```{.r .cell-code}\nas.numeric(lobstr::obj_size(boots)) / as.numeric(lobstr::obj_size(LetterRecognition))\n#> [1] 2.528489\n```\n:::\n\n\nThe memory usage for 50 bootstrap samples is less than 3-fold more than the original data set.\n\n### Get started\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-7_f7c263ca39248e0658270dc444f77ae4'}\n\n```{.r .cell-code}\nset.seed(8584)\nbt_resamples <- bootstraps(mtcars, times = 3)\nbt_resamples\n#> # Bootstrap sampling \n#> # A tibble: 3 Ã— 2\n#>   splits          id        \n#>   <list>          <chr>     \n#> 1 <split [32/14]> Bootstrap1\n#> 2 <split [32/12]> Bootstrap2\n#> 3 <split [32/14]> Bootstrap3\n```\n:::\n\n\nThe resamples are stored in the `splits` column in an object that has class `rsplit`.\n\nIn this package we use the following terminology for the two partitions that comprise a resample:\n\n- The *analysis* data are those that we selected in the resample. For a bootstrap, this is the sample with replacement. For 10-fold cross-validation, this is the 90% of the data. These data are often used to fit a model or calculate a statistic in traditional bootstrapping.\n- The *assessment* data are usually the section of the original data not covered by the analysis set. Again, in 10-fold CV, this is the 10% held out. These data are often used to evaluate the performance of a model that was fit to the analysis data.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-8_aca1158fb65ceaa64ca3ba23a2f6b73a'}\n\n```{.r .cell-code}\nfirst_resample <- bt_resamples$splits[[1]]\nfirst_resample\n#> <Analysis/Assess/Total>\n#> <32/14/32>\n```\n:::\n\n\nThis indicates that there were 32 data points in the analysis set, 14 instances were in the assessment set, and that the original data contained 32 data points. These results can also be determined using the `dim` function on an `rsplit` object.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-9_8a1c75562fb25233daaa676aa535ec4f'}\n\n```{.r .cell-code}\nhead(assessment(first_resample))\n#>                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n#> Mazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\n#> Hornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\n#> Merc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n#> Merc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n#> Merc 280       19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\n#> Merc 280C      17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\n```\n:::\n\n\n\n## parsnip\n\nå®˜ç½‘ï¼š<https://parsnip.tidymodels.org/>\n\n### Index\n\nOne challenge with different modeling functions available in R *that do the same thing* is that they can have different interfaces and arguments. For example, to fit a random forest regression model, we might have:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-10_b48dfba27cbdd809641f6852414a1d20'}\n\n```{.r .cell-code  code-fold=\"true\"}\n# From randomForest\nrf_1 <- randomForest(\n  y ~ ., \n  data = dat, \n  mtry = 10, \n  ntree = 2000, \n  importance = TRUE\n)\n\n# From ranger\nrf_2 <- ranger(\n  y ~ ., \n  data = dat, \n  mtry = 10, \n  num.trees = 2000, \n  importance = \"impurity\"\n)\n\n# From sparklyr\nrf_3 <- ml_random_forest(\n  dat, \n  intercept = FALSE, \n  response = \"y\", \n  features = names(dat)[names(dat) != \"y\"], \n  col.sample.rate = 10,\n  num.trees = 2000\n)\n```\n:::\n\n\nIn this example:\n\n- the **type** of model is â€œrandom forestâ€,\n- the **mode** of the model is â€œregressionâ€ (as opposed to classification, etc), and\n- the computational **engine** is the name of the R package.\n\nThe goals of parsnip are to:\n\n- Separate the definition of a model from its evaluation.\n- Decouple the model specification from the implementation (whether the implementation is in R, spark, or something else). For example, the user would call `rand_forest` instead of `ranger::ranger` or other specific packages.\n- Harmonize argument names (e.g. `n.trees`, `ntrees`, `trees`) so that users only need to remember a single name. This will help `across` model types too so that `trees` will be the same argument across random forest as well as boosting or bagging.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-11_36ed74b9fad5bd196521405558328a5f'}\n\n```{.r .cell-code}\n# use ranger\nrand_forest(mtry = 10, trees = 2000) %>% \n  set_engine(\"ranger\", importrance = \"impurity\") %>% \n  set_mode(\"regression\")\n#> Random Forest Model Specification (regression)\n#> \n#> Main Arguments:\n#>   mtry = 10\n#>   trees = 2000\n#> \n#> Engine-Specific Arguments:\n#>   importrance = impurity\n#> \n#> Computational engine: ranger\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-12_6742f3c2ef4f9981ac55283c94f43865'}\n\n```{.r .cell-code}\n# use Spark\nrand_forest(mtry = 10, trees = 2000) %>% \n  set_engine(\"spark\") %>% \n  set_mode(\"regression\")\n#> Random Forest Model Specification (regression)\n#> \n#> Main Arguments:\n#>   mtry = 10\n#>   trees = 2000\n#> \n#> Computational engine: spark\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-13_ea8227aae4aace8e765b2518a794e2cb'}\n\n```{.r .cell-code}\nset.seed(192)\nrand_forest(mtry = 10, trees = 2000) %>%\n  set_engine(\"ranger\", importance = \"impurity\") %>%\n  set_mode(\"regression\") %>%\n  fit(mpg ~ ., data = mtcars)\n#> parsnip model object\n#> \n#> Ranger result\n#> \n#> Call:\n#>  ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~10,      x), num.trees = ~2000, importance = ~\"impurity\", num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) \n#> \n#> Type:                             Regression \n#> Number of trees:                  2000 \n#> Sample size:                      32 \n#> Number of independent variables:  10 \n#> Mtry:                             10 \n#> Target node size:                 5 \n#> Variable importance mode:         impurity \n#> Splitrule:                        variance \n#> OOB prediction error (MSE):       5.725636 \n#> R squared (OOB):                  0.8423737\n```\n:::\n\n\n### Get started\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-14_e9152de01510cc6c2ee4d63e93bdc468'}\n\n```{.r .cell-code}\ntune_mtry <- rand_forest(trees = 2000, mtry = tune())  # tune() is a placeholder\ntune_mtry\n#> Random Forest Model Specification (unknown mode)\n#> \n#> Main Arguments:\n#>   mtry = tune()\n#>   trees = 2000\n#> \n#> Computational engine: ranger\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-15_9be215d6615168df0f5c1f249add94e4'}\n\n```{.r .cell-code}\nrf_with_seed <- \n  rand_forest(trees = 2000, mtry = tune(), mode = \"regression\") %>%\n  set_engine(\"ranger\", seed = 63233)\nrf_with_seed\n#> Random Forest Model Specification (regression)\n#> \n#> Main Arguments:\n#>   mtry = tune()\n#>   trees = 2000\n#> \n#> Engine-Specific Arguments:\n#>   seed = 63233\n#> \n#> Computational engine: ranger\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-16_b1f7fef8fe92c3cd4299d6c0de46da4f'}\n\n```{.r .cell-code}\nrf_with_seed %>% \n  set_args(mtry = 4) %>% \n  set_engine(\"ranger\") %>% \n  fit(mpg ~ ., data = mtcars)\n#> parsnip model object\n#> \n#> Ranger result\n#> \n#> Call:\n#>  ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~4,      x), num.trees = ~2000, num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1)) \n#> \n#> Type:                             Regression \n#> Number of trees:                  2000 \n#> Sample size:                      32 \n#> Number of independent variables:  10 \n#> Mtry:                             4 \n#> Target node size:                 5 \n#> Variable importance mode:         none \n#> Splitrule:                        variance \n#> OOB prediction error (MSE):       5.723276 \n#> R squared (OOB):                  0.8424386\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-17_1b5d41f803ec60f775eae9a01690b9ff'}\n\n```{.r .cell-code}\n# or using the `randomForest` package\nset.seed(56982)\nrf_with_seed %>% \n  set_args(mtry = 4) %>% \n  set_engine(\"randomForest\") %>% \n  fit(mpg ~ ., data = mtcars)\n#> parsnip model object\n#> \n#> \n#> Call:\n#>  randomForest(x = maybe_data_frame(x), y = y, ntree = ~2000, mtry = min_cols(~4,      x)) \n#>                Type of random forest: regression\n#>                      Number of trees: 2000\n#> No. of variables tried at each split: 4\n#> \n#>           Mean of squared residuals: 5.524901\n#>                     % Var explained: 84.3\n```\n:::\n\n\n## recipes\n\nå®˜ç½‘ï¼š<https://recipes.tidymodels.org/>\n\n### Index\n\nWith recipes, you can use dplyr-like pipeable sequences of feature engineering steps to get your data ready for modeling. For example, to create a recipe containing an outcome plus two numeric predictors and then center and scale (â€œnormalizeâ€) the predictors:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-18_b32d62d78704666a33c0d0758806329e'}\n\n```{.r .cell-code}\ndata(ad_data)\n\nrecipe(Class ~ tau + VEGF, data = ad_data) %>% \n  step_normalize(all_numeric_predictors())\n#> \n#> â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> \n#> â”€â”€ Inputs\n#> Number of variables by role\n#> outcome:   1\n#> predictor: 2\n#> \n#> â”€â”€ Operations\n#> â€¢ Centering and scaling for: all_numeric_predictors()\n```\n:::\n\n\n### Get started\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-19_0421f8485d88c04e30a309357b150d32'}\n\n```{.r .cell-code}\nset.seed(55)\ntrain_test_split <- initial_split(credit_data)\n\ncredit_train <- training(train_test_split)\ncredit_test <- testing(train_test_split)\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-20_d655635d3ee7ac3eb04fee63d8dad6e4'}\n\n```{.r .cell-code}\n# there are some missing values\nvapply(credit_train, function(x) mean(!is.na(x)), numeric(1))\n#>    Status Seniority      Home      Time       Age   Marital   Records       Job \n#> 1.0000000 1.0000000 0.9982036 1.0000000 1.0000000 0.9997006 1.0000000 0.9994012 \n#>  Expenses    Income    Assets      Debt    Amount     Price \n#> 1.0000000 0.9104790 0.9892216 0.9955090 1.0000000 1.0000000\n```\n:::\n\n\nRather than remove these, their values will be imputed.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-21_91fea9ca1bdfa9c6eeab004eaad271aa'}\n\n```{.r .cell-code}\n# an initial recipe\nrec_obj <- recipe(Status ~ ., data = credit_train)\nrec_obj\n#> \n#> â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> \n#> â”€â”€ Inputs\n#> Number of variables by role\n#> outcome:    1\n#> predictor: 13\n```\n:::\n\n\nThere are many ways to do this and `recipes` includes a few steps for this purpose:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-22_7d339e1c26f72e51f3fc8028d3bf72a0'}\n\n```{.r .cell-code}\ngrep(\"impute_\", ls(\"package:recipes\"), value = TRUE)\n#> [1] \"step_impute_bag\"    \"step_impute_knn\"    \"step_impute_linear\"\n#> [4] \"step_impute_lower\"  \"step_impute_mean\"   \"step_impute_median\"\n#> [7] \"step_impute_mode\"   \"step_impute_roll\"\n```\n:::\n\n\nHere, K-nearest neighbor imputation will be used. This works for both numeric and non-numeric predictors and defaults K to five To do this, it selects all predictors and then removes those that are numeric:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-23_7179f35346ebba81b9e6e6b93551bd6b'}\n\n```{.r .cell-code}\nimputed <- rec_obj %>% \n  step_impute_knn(all_predictors())\nimputed\n#> \n#> â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> \n#> â”€â”€ Inputs\n#> Number of variables by role\n#> outcome:    1\n#> predictor: 13\n#> \n#> â”€â”€ Operations\n#> â€¢ K-nearest neighbor imputation for: all_predictors()\n```\n:::\n\n\nIt is important to realize that the *specific* variables have not been declared yet (as shown when the recipe is printed above). In some preprocessing steps, variables will be added or removed from the current list of possible variables.\n\nSince some predictors are categorical in nature (i.e. nominal), it would make sense to convert these factor predictors into numeric dummy variables (aka indicator variables) using `step_dummy()`. To do this, the step selects all non-numeric predictors:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-24_84a592f5673d61303b352c089f636fd3'}\n\n```{.r .cell-code}\nind_vars <- imputed %>% \n  step_dummy(all_nominal_predictors())\nind_vars\n#> \n#> â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> \n#> â”€â”€ Inputs\n#> Number of variables by role\n#> outcome:    1\n#> predictor: 13\n#> \n#> â”€â”€ Operations\n#> â€¢ K-nearest neighbor imputation for: all_predictors()\n#> â€¢ Dummy variables from: all_nominal_predictors()\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-25_a2943bc7b8eb6d7e7c8d3071304fd126'}\n\n```{.r .cell-code}\nstrandardozed <- ind_vars %>% \n  step_center(all_numeric_predictors()) %>% \n  step_scale(all_numeric_predictors())\nstrandardozed\n#> \n#> â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> \n#> â”€â”€ Inputs\n#> Number of variables by role\n#> outcome:    1\n#> predictor: 13\n#> \n#> â”€â”€ Operations\n#> â€¢ K-nearest neighbor imputation for: all_predictors()\n#> â€¢ Dummy variables from: all_nominal_predictors()\n#> â€¢ Centering for: all_numeric_predictors()\n#> â€¢ Scaling for: all_numeric_predictors()\n```\n:::\n\n\nThe `prep` function is used with a recipe and a data set:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-26_5ab4d2bee7eb22ba1d9cbcb8b3184582'}\n\n```{.r .cell-code}\ntrained_rec <- prep(strandardozed, training = credit_train)\ntrained_rec\n#> \n#> â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> \n#> â”€â”€ Inputs\n#> Number of variables by role\n#> outcome:    1\n#> predictor: 13\n#> \n#> â”€â”€ Training information\n#> Training data contained 3340 data points and 322 incomplete rows.\n#> \n#> â”€â”€ Operations\n#> â€¢ K-nearest neighbor imputation for: Seniority, Home, Time, Age, ... | Trained\n#> â€¢ Dummy variables from: Home, Marital, Records, Job | Trained\n#> â€¢ Centering for: Seniority, Time, Age, Expenses, Income, Assets, ... | Trained\n#> â€¢ Scaling for: Seniority, Time, Age, Expenses, Income, Assets, ... | Trained\n```\n:::\n\n\nNow that the statistics have been estimated, the preprocessing can be *applied* to the training and test set:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-27_bf4a1023393953a9b4e801ac31334c9a'}\n\n```{.r .cell-code}\ntrain_data <- bake(trained_rec, new_data = credit_train)\ntest_data <- bake(trained_rec, new_data = credit_test)\n```\n:::\n\n\nAnother type of operation that can be added to a recipes is a *check*. Checks conduct some sort of data validation and, if no issue is found, returns the data as-is; otherwise, an error is thrown.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-28_1a16a660727f6c60790ab1fb639e618a'}\n\n```{.r .cell-code}\ntrained_rec %>% \n  check_missing(contains(\"Marital\"))\n#> \n#> â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> \n#> â”€â”€ Inputs\n#> Number of variables by role\n#> outcome:    1\n#> predictor: 13\n#> \n#> â”€â”€ Training information\n#> Training data contained 3340 data points and 322 incomplete rows.\n#> \n#> â”€â”€ Operations\n#> â€¢ K-nearest neighbor imputation for: Seniority, Home, Time, Age, ... | Trained\n#> â€¢ Dummy variables from: Home, Marital, Records, Job | Trained\n#> â€¢ Centering for: Seniority, Time, Age, Expenses, Income, Assets, ... | Trained\n#> â€¢ Scaling for: Seniority, Time, Age, Expenses, Income, Assets, ... | Trained\n#> â€¢ Check missing values for: contains(\"Marital\")\n```\n:::\n\n\n## workflows\n\nå®˜ç½‘ï¼š<https://workflows.tidymodels.org/>\n\n### Index\n\nA workflow is an object that can bundle together your pre-processing, modeling, and post-processing requests. For example, if you have a `recipe` and `parsnip` model, these can be combined into a workflow. The advantages are:\n\n- You donâ€™t have to keep track of separate objects in your workspace.\n- The recipe prepping and model fitting can be executed using a single call to `fit()`.\n- If you have custom tuning parameter settings, these can be defined using a simpler interface when combined with {tune}.\n- In the future, workflows will be able to add post-processing operations, such as modifying the probability cutoff for two-class models.\n\nSuppose you were modeling data on cars. Sayâ€¦the fuel efficiency of 32 cars. You know that the relationship between engine displacement and miles-per-gallon is nonlinear, and you would like to model that as a spline before adding it to a Bayesian linear regression model. \n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-29_35c52f739c152ee27cb9b1ab86fcb324'}\n\n```{.r .cell-code}\n# recipe\nspline_cars <- recipe(mpg ~ ., data = mtcars) %>% \n  step_ns(disp, deg_free = 10)\n\n# parsnip\nbayes_lm <- linear_reg() %>% \n  set_engine(\"stan\")\n\n# workflow\ncar_wflow <- workflow() %>% \n  add_recipe(spline_cars) %>% \n  add_model(bayes_lm)\n```\n:::\n\n\nNow you can prepare the recipe and estimate the model via a single call to `fit()`:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-30_5a3b67eedfa36a0e28b64197dc60fc77'}\n\n```{.r .cell-code}\nfit(car_wflow, data = mtcars)\n#> â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#> Preprocessor: Recipe\n#> Model: linear_reg()\n#> \n#> â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> 1 Recipe Step\n#> \n#> â€¢ step_ns()\n#> \n#> â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> stan_glm\n#>  family:       gaussian [identity]\n#>  formula:      ..y ~ .\n#>  observations: 32\n#>  predictors:   20\n#> ------\n#>             Median MAD_SD\n#> (Intercept)  49.1   17.4 \n#> cyl          -3.3    1.4 \n#> hp            0.0    0.0 \n#> drat         -2.7    1.6 \n#> wt           -1.1    1.5 \n#> qsec          0.2    0.6 \n#> vs           -2.8    2.0 \n#> am           -1.1    1.7 \n#> gear          2.8    1.2 \n#> carb          0.3    1.2 \n#> disp_ns_01   -9.7    3.8 \n#> disp_ns_02  -12.7    4.2 \n#> disp_ns_03   -4.8    4.0 \n#> disp_ns_04  -15.2    5.8 \n#> disp_ns_05    0.3    5.0 \n#> disp_ns_06   -6.5    5.3 \n#> disp_ns_07   -3.1    6.2 \n#> disp_ns_08    5.7    5.7 \n#> disp_ns_09   -9.3    7.8 \n#> disp_ns_10   -4.4    5.7 \n#> \n#> Auxiliary parameter(s):\n#>       Median MAD_SD\n#> sigma 1.6    0.3   \n#> \n#> ------\n#> * For help interpreting the printed output see ?print.stanreg\n#> * For info on the priors used see ?prior_summary.stanreg\n```\n:::\n\n\nYou can alter existing workflows using `update_recipe()` / `update_model()` and `remove_recipe()` / `remove_model()`.\n\n### Get started\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-31_2d8b56a6d3cc3b3e5d06aade7e1b907d'}\n\n```{.r .cell-code}\nlibrary(modeldata)\n\n# This gives us access to the 3 partitions:\n# - `bivariate_train`: Training set\n# - `bivariate_val`: Validation set\n# - `bivariate_test`: Test set\ndata(\"bivariate\")\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-32_90e19366fa21879d6e43e2cac5126d4a'}\n\n```{.r .cell-code}\nggplot(bivariate_train, aes(A, B, col = Class)) +\n  geom_point(alpha = .3) +\n  coord_equal(ratio = 20)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-32-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-33_d6f5bcbb1386c69bca28b86d9fe96dcb'}\n\n```{.r .cell-code}\nbivariate_train %>% \n  pivot_longer(A:B, names_to = \"predictor\") %>% \n  ggplot(aes(Class, value)) +\n  geom_boxplot() +\n  facet_wrap(~predictor, scales = \"free_y\") +\n  scale_y_log10()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-33-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\nIn the first plot above, the separation appears to happen linearly, and a straight, diagonal boundary might do well. We could use `glm()` directly to create a logistic regression, but we will use the `tidymodels` infrastructure and start by making a `parsnip` model object.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-34_b6d0d1e3c84f847cd1efb211e1a8cfcd'}\n\n```{.r .cell-code}\nlogit_mod <- logistic_reg() %>% \n  set_engine(\"glm\")\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-35_65bade2f40140e9fa7b5df4f58df3ba0'}\n\n```{.r .cell-code}\nglm_workflow <-\n  workflow() %>%\n  add_model(logit_mod)\n\nsimple_glm <- \n  glm_workflow %>% \n  add_formula(Class ~ .) %>% \n  fit(data = bivariate_train)\nsimple_glm\n#> â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#> Preprocessor: Formula\n#> Model: logistic_reg()\n#> \n#> â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> Class ~ .\n#> \n#> â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> \n#> Call:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n#> \n#> Coefficients:\n#> (Intercept)            A            B  \n#>    1.731243     0.002622    -0.064411  \n#> \n#> Degrees of Freedom: 1008 Total (i.e. Null);  1006 Residual\n#> Null Deviance:\t    1329 \n#> Residual Deviance: 1133 \tAIC: 1139\n```\n:::\n\n\nTo evaluate this model, the ROC curve will be computed along with its corresponding AUC.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-36_7c6195cb3e43fb87615f2a5576e6c0a3'}\n\n```{.r .cell-code}\nsimple_glm_probs <- \n  predict(simple_glm, bivariate_val, type = \"prob\") %>% \n  bind_cols(bivariate_val)\n\nsimple_glm_roc <- \n  simple_glm_probs %>% \n  roc_curve(Class, .pred_One)\n\nsimple_glm_probs %>% roc_auc(Class, .pred_One)\n#> # A tibble: 1 Ã— 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 roc_auc binary         0.773\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-37_f62a657d9358383f2f2f77110126c48e'}\n\n```{.r .cell-code}\nautoplot(simple_glm_roc)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-37-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\nSince there are two correlated predictors with skewed distributions and strictly positive values, it might be intuitive to use their ratio instead of the pair. Weâ€™ll try that next by recycling the initial workflow and just adding a different formula:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-38_64a4dc9336ec4bf6b4c9dc368026d724'}\n\n```{.r .cell-code}\nratio_glm <- \n  glm_workflow %>% \n  add_formula(Class ~ I(A/B)) %>% \n  fit(data = bivariate_train)\n\nratio_glm_probs <- \n  predict(ratio_glm, bivariate_val, type = \"prob\") %>% \n  bind_cols(bivariate_val)\n\nratio_glm_roc <- \n  ratio_glm_probs %>% \n  roc_curve(Class, .pred_One)\n\nratio_glm_probs %>% roc_auc(Class, .pred_One)\n#> # A tibble: 1 Ã— 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 roc_auc binary         0.765\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-39_594fa6863e99665f725a3b70dc34d923'}\n\n```{.r .cell-code}\nautoplot(simple_glm_roc) +\n  geom_path(\n    data = ratio_glm_roc,\n    aes(x = 1 - specificity, y = sensitivity),\n    col = \"#FDE725FF\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-39-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n**More complex feature engineering**\n\nInstead of combining the two predictors, would it help the model if we were to resolve the skewness of the variables? To test this theory, one option would be to use the Box-Cox transformation on each predictor individually to see if it recommends a nonlinear transformation. The transformation can encode a variety of different functions including the log transform, square root, inverse, and fractional transformations in-between these.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-40_05a0e78b450a119d73f449479e216e02'}\n\n```{.r .cell-code}\ntrans_recipe <- \n  recipe(Class ~ ., data = bivariate_train) %>% \n  step_BoxCox(all_predictors())\n\ntrans_glm <- \n  glm_workflow %>% \n  add_recipe(trans_recipe) %>% \n  fit(data = bivariate_train)\n\ntrans_glm_probs <- \n  predict(trans_glm, bivariate_val, type = \"prob\") %>% \n  bind_cols(bivariate_val)\n\ntrans_glm_roc <- \n  trans_glm_probs %>% \n  roc_curve(Class, .pred_One)\n\ntrans_glm_probs %>% roc_auc(Class, .pred_One)\n#> # A tibble: 1 Ã— 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 roc_auc binary         0.815\n\nautoplot(simple_glm_roc) +\n  geom_path(\n    data = ratio_glm_roc,\n    aes(x = 1 - specificity, y = sensitivity),\n    col = \"#FDE725FF\"\n  ) +\n  geom_path(\n    data = trans_glm_roc,\n    aes(x = 1 - specificity, y = sensitivity),\n    col = \"#21908CFF\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-40-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-41_10a84103ca1ac19ea3bb810a22ffa034'}\n\n```{.r .cell-code}\nggplot(bivariate_train, aes(1/A, 1/B, col = Class)) + \n  geom_point(alpha = .3) +\n  coord_equal(ratio = 1/12)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-41-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-42_f0ba6007cbe227a3aa93faaa7afe2b4c'}\n\n```{.r .cell-code}\npca_recipe <- \n  trans_recipe %>% \n  step_normalize(A, B) %>% \n  step_pca(A, B, num_comp = 2)\n\npca_glm <- \n  glm_workflow %>% \n  add_recipe(pca_recipe) %>% \n  fit(data = bivariate_train)\n\npca_glm_probs <- \n  predict(pca_glm, bivariate_val, type = \"prob\") %>% \n  bind_cols(bivariate_val)\n\npac_glm_roc <- \n  pca_glm_probs %>% \n  roc_curve(Class, .pred_One)\n\npca_glm_probs %>% roc_auc(Class, .pred_One)\n#> # A tibble: 1 Ã— 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 roc_auc binary         0.815\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-43_396fea1c65ee10260152231ddc166558'}\n\n```{.r .cell-code}\n# using the test set\ntest_prob <- \n  predict(trans_glm, bivariate_test, type = \"prob\") %>% \n  bind_cols(bivariate_test)\n\ntest_roc <- \n  test_prob %>% \n  roc_curve(Class, .pred_One)\n\ntest_prob %>% roc_auc(Class, .pred_One)\n#> # A tibble: 1 Ã— 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 roc_auc binary         0.862\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-44_bc07a3b2b40d6b0d9100c6ccc8fb6cbe'}\n\n```{.r .cell-code}\nautoplot(test_roc)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-44-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n## tune\n\nå®˜ç½‘ï¼š<https://tune.tidymodels.org/>\n\n### Index\n\nThe goal of tune is to facilitate hyperparameter tuning for the tidymodels packages. It relies heavily on {recipes}, {parsnip}, and {dials}.\n\n### Get started\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-45_ab4bd9773946003ab043f23bb6f89b9a'}\n\n```{.r .cell-code}\nset.seed(4595)\n\ndata_split <- \n  ames %>% \n  mutate(Sale_Price = log10(Sale_Price)) %>% \n  initial_split(strata = Sale_Price)\n\names_train <- training(data_split)\name_test <- testing(data_split)\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-46_487c2741fd071be9c8f88f779c7b87fc'}\n\n```{.r .cell-code}\names_train %>% \n  select(Sale_Price, Longitude, Latitude) %>% \n  pivot_longer(cols = 2:3,\n               names_to = \"predictor\",\n               values_to = \"value\") %>% \n  ggplot(aes(value, Sale_Price)) +\n  geom_point(alpha = .2) +\n  geom_smooth(se = FALSE) +\n  facet_wrap(~ predictor, scales = \"free_x\")\n#> `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-46-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-47_b0d97a2f88b0389094d080c76ec50538'}\n\n```{.r .cell-code}\names_rec <- \n  recipe(Sale_Price ~ Gr_Liv_Area + Longitude + Latitude, data = ames_train) %>% \n  step_log(Gr_Liv_Area, base = 10) %>% \n  step_ns(Longitude, deg_free = tune(\"long df\")) %>% \n  step_ns(Latitude, deg_free = tune(\"lat df\"))\n```\n:::\n\n\nThe function `dials::parameters()` can detect and collect the parameters that have been flagged for tuning:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-48_7f4fe7d0b1c9760c1f47383fb17ed519'}\n\n```{.r .cell-code}\nparameters(ames_rec)\n#> Warning: `parameters.workflow()` was deprecated in tune 0.1.6.9003.\n#> â„¹ Please use `hardhat::extract_parameter_set_dials()` instead.\n#> Collection of 2 parameters for tuning\n#> \n#>  identifier     type    object\n#>     long df deg_free nparam[+]\n#>      lat df deg_free nparam[+]\n```\n:::\n\n\nThe {dials} package has default ranges for many parameters. The generic parameter function for `deg_free` has a fairly small range:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-49_098441a74b85745cd3f57d27d2efd3ef'}\n\n```{.r .cell-code}\ndeg_free()\n#> Degrees of Freedom (quantitative)\n#> Range: [1, 5]\n```\n:::\n\n\nHowever, there is a dials function that is more appropriate for splines:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-50_46d5963936ce4b62e97702f87b546ba4'}\n\n```{.r .cell-code}\nspline_degree()\n#> Spline Degrees of Freedom (quantitative)\n#> Range: [1, 10]\n```\n:::\n\n\nThe parameter objects can be easily changed using the `update()` function:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-51_c77618613c5af891992428e0c8b3ce66'}\n\n```{.r .cell-code}\names_param <- \n  ames_rec %>% \n  parameters() %>% \n  update(\n    `long df` = spline_degree(),\n    `lat df` = spline_degree()\n  )\n\names_param\n#> Collection of 2 parameters for tuning\n#> \n#>  identifier     type    object\n#>     long df deg_free nparam[+]\n#>      lat df deg_free nparam[+]\n```\n:::\n\n\n**Grid Search**\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-52_7db5efdca6fccfd20f66d2353cf6b263'}\n\n```{.r .cell-code}\nspline_grid <- grid_max_entropy(ames_param, size = 10)\nspline_grid\n#> # A tibble: 10 Ã— 2\n#>    `long df` `lat df`\n#>        <int>    <int>\n#>  1         2        4\n#>  2         7        7\n#>  3         4        6\n#>  4        10        4\n#>  5         1        8\n#>  6         5        1\n#>  7         4       10\n#>  8         6        4\n#>  9        10       10\n#> 10         9        1\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-53_64ed2a2ccf0922e05021282fe9d2deb8'}\n\n```{.r .cell-code}\n# A regular grid\ndf_vals <- seq(2, 18, by = 2)\nspline_grid <- expand.grid(`long df` = df_vals,\n                           `lat df` = df_vals)\n```\n:::\n\n\nThere are two other ingredients that are required before tuning.\n\nFirst is a model specification. Using {parsnip}, a basic linear model can be used:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-54_a24a86559930fd375cf68fa23042fe06'}\n\n```{.r .cell-code}\n# a basic linear model\nlm_mod <- linear_reg() %>% set_engine(\"lm\")\n\n# simple 10-fold cross-validation\nset.seed(2453)\ncv_splits <- vfold_cv(ames_train, v = 10, strata = Sale_Price)\n```\n:::\n\n\nThe root mean squared error will be used to measure performance (and this is the default for regression problems).\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-55_c6e0ca699241d73887481ddb26f38830'}\n\n```{.r .cell-code}\names_res <- tune_grid(lm_mod, ames_rec,\n                      resamples = cv_splits,\n                      grid = spline_grid)\names_res\n#> # Tuning results\n#> # 10-fold cross-validation using stratification \n#> # A tibble: 10 Ã— 4\n#>    splits             id     .metrics           .notes          \n#>    <list>             <chr>  <list>             <list>          \n#>  1 <split [1976/221]> Fold01 <tibble [162 Ã— 6]> <tibble [0 Ã— 3]>\n#>  2 <split [1976/221]> Fold02 <tibble [162 Ã— 6]> <tibble [0 Ã— 3]>\n#>  3 <split [1976/221]> Fold03 <tibble [162 Ã— 6]> <tibble [0 Ã— 3]>\n#>  4 <split [1976/221]> Fold04 <tibble [162 Ã— 6]> <tibble [0 Ã— 3]>\n#>  5 <split [1977/220]> Fold05 <tibble [162 Ã— 6]> <tibble [0 Ã— 3]>\n#>  6 <split [1977/220]> Fold06 <tibble [162 Ã— 6]> <tibble [0 Ã— 3]>\n#>  7 <split [1978/219]> Fold07 <tibble [162 Ã— 6]> <tibble [0 Ã— 3]>\n#>  8 <split [1978/219]> Fold08 <tibble [162 Ã— 6]> <tibble [0 Ã— 3]>\n#>  9 <split [1979/218]> Fold09 <tibble [162 Ã— 6]> <tibble [0 Ã— 3]>\n#> 10 <split [1980/217]> Fold10 <tibble [162 Ã— 6]> <tibble [0 Ã— 3]>\n```\n:::\n\n\nThe `.metrics` column has all of the holdout performance estimates for each parameter combination:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-56_3d5941563ca4b10390552fa56e0f4e9f'}\n\n```{.r .cell-code}\names_res$.metrics[[1]]\n#> # A tibble: 162 Ã— 6\n#>    `long df` `lat df` .metric .estimator .estimate .config              \n#>        <dbl>    <dbl> <chr>   <chr>          <dbl> <chr>                \n#>  1         2        2 rmse    standard      0.0985 Preprocessor01_Model1\n#>  2         2        2 rsq     standard      0.683  Preprocessor01_Model1\n#>  3         4        2 rmse    standard      0.0985 Preprocessor02_Model1\n#>  4         4        2 rsq     standard      0.683  Preprocessor02_Model1\n#>  5         6        2 rmse    standard      0.0973 Preprocessor03_Model1\n#>  6         6        2 rsq     standard      0.690  Preprocessor03_Model1\n#>  7         8        2 rmse    standard      0.0962 Preprocessor04_Model1\n#>  8         8        2 rsq     standard      0.697  Preprocessor04_Model1\n#>  9        10        2 rmse    standard      0.0960 Preprocessor05_Model1\n#> 10        10        2 rsq     standard      0.699  Preprocessor05_Model1\n#> # â„¹ 152 more rows\n```\n:::\n\n\nTo get the average metric value for each parameter combination, `collect_metrics()` can be put to use:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-57_f4a87d21a54dcb823430c73b6be6657a'}\n\n```{.r .cell-code}\nestimates <- collect_metrics(ames_res)\nestimates\n#> # A tibble: 162 Ã— 8\n#>    `long df` `lat df` .metric .estimator   mean     n std_err .config           \n#>        <dbl>    <dbl> <chr>   <chr>       <dbl> <int>   <dbl> <chr>             \n#>  1         2        2 rmse    standard   0.100     10 0.00145 Preprocessor01_Moâ€¦\n#>  2         2        2 rsq     standard   0.674     10 0.00777 Preprocessor01_Moâ€¦\n#>  3         4        2 rmse    standard   0.100     10 0.00153 Preprocessor02_Moâ€¦\n#>  4         4        2 rsq     standard   0.675     10 0.00814 Preprocessor02_Moâ€¦\n#>  5         6        2 rmse    standard   0.0994    10 0.00161 Preprocessor03_Moâ€¦\n#>  6         6        2 rsq     standard   0.680     10 0.00885 Preprocessor03_Moâ€¦\n#>  7         8        2 rmse    standard   0.0989    10 0.00159 Preprocessor04_Moâ€¦\n#>  8         8        2 rsq     standard   0.684     10 0.00946 Preprocessor04_Moâ€¦\n#>  9        10        2 rmse    standard   0.0990    10 0.00165 Preprocessor05_Moâ€¦\n#> 10        10        2 rsq     standard   0.683     10 0.0101  Preprocessor05_Moâ€¦\n#> # â„¹ 152 more rows\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-58_0e00ad4af983fd093cbbbefdc96c1a7f'}\n\n```{.r .cell-code}\nestimates %>% \n  filter(.metric == \"rmse\") %>% \n  arrange(mean)\n#> # A tibble: 81 Ã— 8\n#>    `long df` `lat df` .metric .estimator   mean     n std_err .config           \n#>        <dbl>    <dbl> <chr>   <chr>       <dbl> <int>   <dbl> <chr>             \n#>  1        16       12 rmse    standard   0.0948    10 0.00120 Preprocessor53_Moâ€¦\n#>  2        18       12 rmse    standard   0.0948    10 0.00121 Preprocessor54_Moâ€¦\n#>  3        16        8 rmse    standard   0.0949    10 0.00118 Preprocessor35_Moâ€¦\n#>  4        16       10 rmse    standard   0.0949    10 0.00118 Preprocessor44_Moâ€¦\n#>  5        16       18 rmse    standard   0.0949    10 0.00119 Preprocessor80_Moâ€¦\n#>  6        16       16 rmse    standard   0.0949    10 0.00122 Preprocessor71_Moâ€¦\n#>  7        18       10 rmse    standard   0.0949    10 0.00119 Preprocessor45_Moâ€¦\n#>  8        18        8 rmse    standard   0.0949    10 0.00119 Preprocessor36_Moâ€¦\n#>  9        18       18 rmse    standard   0.0950    10 0.00120 Preprocessor81_Moâ€¦\n#> 10        16       14 rmse    standard   0.0950    10 0.00117 Preprocessor62_Moâ€¦\n#> # â„¹ 71 more rows\n```\n:::\n\n\nSmaller degrees of freedom values correspond to more linear functions, but the grid search indicates that more nonlinearity is better. What was the relationship between these two parameters and RMSE?\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-59_36f434b80d66b7d723fb254ced5b2616'}\n\n```{.r .cell-code}\nautoplot(ames_res, metric = \"rmse\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-59-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\nInterestingly, latitude does *not* do well with degrees of freedom less than 8. How nonlinear are the optimal degrees of freedom?\n\nLetâ€™s plot these spline functions over the data for both good and bad values of `deg_free`:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-60_ab4f4757bf3a704085013a9d2da32c0f'}\n\n```{.r .cell-code}\names_train %>% \n  select(Sale_Price, Longitude, Latitude) %>% \n  pivot_longer(cols = 2:3,\n               names_to = \"predictor\",\n               values_to = \"value\") %>% \n  ggplot(aes(value, Sale_Price)) +\n  geom_point(alpha = .2) +\n  geom_smooth(se = FALSE, method = lm,\n              formula = y ~ splines::ns(x, df = 3),\n              col = \"red\") +\n  geom_smooth(se = FALSE, method = lm,\n              formula = y ~ splines::ns(x, df = 16)) +\n  scale_y_log10() +\n  facet_wrap(~ predictor, scales = \"free_x\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-60-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\nLooking at these plots, the smaller degrees of freedom (red) are clearly under-fitting. Visually, the more complex splines (blue) might indicate that there is overfitting but this would result in poor RMSE values when computed on the hold-out data.\n\nBased on these results, a new recipe would be created with the optimized values (using the entire training set) and this would be combined with a linear model created form the entire training set.\n\n**Model Optimization**\n\nInstead of a linear regression, a nonlinear model might provide good performance. A K-nearest-neighbor fit will also be optimized. For this example, the number of neighbors and the distance weighting function will be optimized:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-61_9d2720a8293e33ea2b26f8f2c06689b5'}\n\n```{.r .cell-code}\nknn_mod <- \n  nearest_neighbor(neighbors = tune(), weight_func = tune()) %>% \n  set_engine(\"kknn\") %>% \n  set_mode(\"regression\")\n\nknn_wflow <- \n  workflow() %>% \n  add_model(knn_mod) %>% \n  add_recipe(ames_rec)\n\nknn_param <- \n  knn_wflow %>% \n  parameters() %>% \n  update(\n    `long df` = spline_degree(c(2, 18)), \n    `lat df` = spline_degree(c(2, 18)),\n    neighbors = neighbors(c(3, 50)),\n    weight_func = weight_func(values = c(\"rectangular\", \"inv\", \"gaussian\", \"triangular\"))\n  )\n#> Warning: `parameters.workflow()` was deprecated in tune 0.1.6.9003.\n#> â„¹ Please use `hardhat::extract_parameter_set_dials()` instead.\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-62_4fd7b931de779b090e42a5f224b0fe31'}\n\n```{.r .cell-code}\n# conduct the search\nctrl <- control_bayes(verbose = TRUE)\nset.seed(8154)\nknn_search <- tune_bayes(knn_wflow, resamples = cv_splits, \n                         initial = 5, iter = 20,\n                         param_info = knn_param, control = ctrl)\n```\n:::\n\n\nVisually, the performance gain was:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-63_9b8a90f1df473256b71c3fb0be6be472'}\n\n```{.r .cell-code}\nautoplot(knn_search, type = \"performance\", metric = \"rmse\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-63-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\nThe best results here were:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-64_c71d01cb555e18aabe0fe2e2fc925cc3'}\n\n```{.r .cell-code}\ncollect_metrics(knn_search) %>% \n  filter(.metric == \"rmse\") %>% \n  arrange(mean)\n#> # A tibble: 18 Ã— 11\n#>    neighbors weight_func `long df` `lat df` .metric .estimator   mean     n\n#>        <int> <chr>           <int>    <int> <chr>   <chr>       <dbl> <int>\n#>  1         7 gaussian           10        8 rmse    standard   0.0826    10\n#>  2         9 gaussian            8        8 rmse    standard   0.0829    10\n#>  3         9 inv                 8        6 rmse    standard   0.0831    10\n#>  4         7 gaussian           11       10 rmse    standard   0.0835    10\n#>  5         9 gaussian           11        8 rmse    standard   0.0835    10\n#>  6        15 gaussian            4        7 rmse    standard   0.0837    10\n#>  7         3 gaussian           10        7 rmse    standard   0.0843    10\n#>  8         4 triangular         10        7 rmse    standard   0.0847    10\n#>  9         4 inv                 6       10 rmse    standard   0.0853    10\n#> 10         5 gaussian           16       13 rmse    standard   0.0857    10\n#> 11         7 gaussian           15        3 rmse    standard   0.0869    10\n#> 12         5 gaussian           17       17 rmse    standard   0.0879    10\n#> 13        30 inv                11        8 rmse    standard   0.0894    10\n#> 14        12 inv                18        2 rmse    standard   0.0900    10\n#> 15         3 triangular          9       18 rmse    standard   0.0904    10\n#> 16        49 triangular          8        4 rmse    standard   0.0914    10\n#> 17        34 gaussian           14        9 rmse    standard   0.0926    10\n#> 18        19 rectangular         2       16 rmse    standard   0.0981    10\n#> # â„¹ 3 more variables: std_err <dbl>, .config <chr>, .iter <int>\n```\n:::\n\n\n## yardstick\n\nå®˜ç½‘ï¼š<https://yardstick.tidymodels.org/>\n\n### Index\n\n{yardstick} is a package to estimate how well models are working using tidy data principles.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-65_445971c91c4a24c69f6a5f3c22a2fca8'}\n\n```{.r .cell-code}\nhead(two_class_example)\n#>    truth      Class1       Class2 predicted\n#> 1 Class2 0.003589243 0.9964107574    Class2\n#> 2 Class1 0.678621054 0.3213789460    Class1\n#> 3 Class2 0.110893522 0.8891064779    Class2\n#> 4 Class1 0.735161703 0.2648382969    Class1\n#> 5 Class2 0.016239960 0.9837600397    Class2\n#> 6 Class1 0.999275071 0.0007249286    Class1\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-66_a2b40ba4effb47c67b2ce363d1b9e79e'}\n\n```{.r .cell-code}\nmetrics(two_class_example, truth, predicted)\n#> # A tibble: 2 Ã— 3\n#>   .metric  .estimator .estimate\n#>   <chr>    <chr>          <dbl>\n#> 1 accuracy binary         0.838\n#> 2 kap      binary         0.675\n\ntwo_class_example %>% \n  roc_auc(truth, Class1)\n#> # A tibble: 1 Ã— 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 roc_auc binary         0.939\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-67_28a16b373468ac259fac16cfd05cecd0'}\n\n```{.r .cell-code}\nhpc_cv <- hpc_cv %>% as_tibble()\nhpc_cv\n#> # A tibble: 3,467 Ã— 7\n#>    obs   pred     VF      F       M          L Resample\n#>    <fct> <fct> <dbl>  <dbl>   <dbl>      <dbl> <chr>   \n#>  1 VF    VF    0.914 0.0779 0.00848 0.0000199  Fold01  \n#>  2 VF    VF    0.938 0.0571 0.00482 0.0000101  Fold01  \n#>  3 VF    VF    0.947 0.0495 0.00316 0.00000500 Fold01  \n#>  4 VF    VF    0.929 0.0653 0.00579 0.0000156  Fold01  \n#>  5 VF    VF    0.942 0.0543 0.00381 0.00000729 Fold01  \n#>  6 VF    VF    0.951 0.0462 0.00272 0.00000384 Fold01  \n#>  7 VF    VF    0.914 0.0782 0.00767 0.0000354  Fold01  \n#>  8 VF    VF    0.918 0.0744 0.00726 0.0000157  Fold01  \n#>  9 VF    VF    0.843 0.128  0.0296  0.000192   Fold01  \n#> 10 VF    VF    0.920 0.0728 0.00703 0.0000147  Fold01  \n#> # â„¹ 3,457 more rows\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-68_be38dbd6355c2fadfd2dea6fb1944206'}\n\n```{.r .cell-code}\n# Macro averaged multiclass precision\nprecision(hpc_cv, obs, pred)\n#> # A tibble: 1 Ã— 3\n#>   .metric   .estimator .estimate\n#>   <chr>     <chr>          <dbl>\n#> 1 precision macro          0.631\n\n# Micro averaged multiclass precision\nprecision(hpc_cv, obs, pred, estimator = \"micro\")\n#> # A tibble: 1 Ã— 3\n#>   .metric   .estimator .estimate\n#>   <chr>     <chr>          <dbl>\n#> 1 precision micro          0.709\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-69_29582b1f2f99f20b349a27569d665d35'}\n\n```{.r .cell-code}\nhpc_cv %>% \n  group_by(Resample) %>% \n  roc_auc(obs, VF:L)\n#> # A tibble: 10 Ã— 4\n#>    Resample .metric .estimator .estimate\n#>    <chr>    <chr>   <chr>          <dbl>\n#>  1 Fold01   roc_auc hand_till      0.813\n#>  2 Fold02   roc_auc hand_till      0.817\n#>  3 Fold03   roc_auc hand_till      0.869\n#>  4 Fold04   roc_auc hand_till      0.849\n#>  5 Fold05   roc_auc hand_till      0.811\n#>  6 Fold06   roc_auc hand_till      0.836\n#>  7 Fold07   roc_auc hand_till      0.825\n#>  8 Fold08   roc_auc hand_till      0.846\n#>  9 Fold09   roc_auc hand_till      0.828\n#> 10 Fold10   roc_auc hand_till      0.812\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-70_cfa8b8d1d53fe596c96b591b2ea1b401'}\n\n```{.r .cell-code}\nhpc_cv %>% \n  group_by(Resample) %>% \n  roc_curve(obs, VF:L) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-70-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n## broom\n\nå®˜ç½‘ï¼š<https://broom.tidymodels.org/>\n\n### Index\n\n`tidy()` produces a `tibble()` where each row contains information about an important component of the model. For regression models, this often corresponds to regression coefficients. This is can be useful if you want to inspect a model or create custom visualizations.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-71_e595125e7d7eb3ecbc4d48c0770994f5'}\n\n```{.r .cell-code}\ndata(\"trees\")\n\nfit <- lm(Volume ~ Girth + Height, trees)\ntidy(fit)\n#> # A tibble: 3 Ã— 5\n#>   term        estimate std.error statistic  p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)  -58.0       8.64      -6.71 2.75e- 7\n#> 2 Girth          4.71      0.264     17.8  8.22e-17\n#> 3 Height         0.339     0.130      2.61 1.45e- 2\n```\n:::\n\n\n`glance()` returns a tibble with exactly one row of goodness of fitness measures and related statistics. This is useful to check for model misspecification and to compare many models.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-72_9a89a0c245ee0ff31ac55f7b688147dc'}\n\n```{.r .cell-code}\nglance(fit)\n#> # A tibble: 1 Ã— 12\n#>   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n#>       <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n#> 1     0.948         0.944  3.88      255. 1.07e-18     2  -84.5  177.  183.\n#> # â„¹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n:::\n\n\n`augment()` adds columns to a dataset, containing information such as fitted values, residuals or cluster assignments. All columns added to a dataset have `.` prefix to prevent existing columns from being overwritten.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-73_222e5108b422d40c5c71872966eb5755'}\n\n```{.r .cell-code}\naugment(fit, data = trees)\n#> # A tibble: 31 Ã— 9\n#>    Girth Height Volume .fitted .resid   .hat .sigma   .cooksd .std.resid\n#>    <dbl>  <dbl>  <dbl>   <dbl>  <dbl>  <dbl>  <dbl>     <dbl>      <dbl>\n#>  1   8.3     70   10.3    4.84  5.46  0.116    3.79 0.0978        1.50  \n#>  2   8.6     65   10.3    4.55  5.75  0.147    3.77 0.148         1.60  \n#>  3   8.8     63   10.2    4.82  5.38  0.177    3.78 0.167         1.53  \n#>  4  10.5     72   16.4   15.9   0.526 0.0592   3.95 0.000409      0.140 \n#>  5  10.7     81   18.8   19.9  -1.07  0.121    3.95 0.00394      -0.294 \n#>  6  10.8     83   19.7   21.0  -1.32  0.156    3.94 0.00840      -0.370 \n#>  7  11       66   15.6   16.2  -0.593 0.115    3.95 0.00114      -0.162 \n#>  8  11       75   18.2   19.2  -1.05  0.0515   3.95 0.00138      -0.277 \n#>  9  11.1     80   22.6   21.4   1.19  0.0920   3.95 0.00348       0.321 \n#> 10  11.2     75   19.9   20.2  -0.288 0.0480   3.95 0.0000968    -0.0759\n#> # â„¹ 21 more rows\n```\n:::\n\n\n## dials\n\n### Index\n\nThis package contains *infrastructure* to create and manage values of tuning parameters for the tidymodels packages.\n\n### Get started\n\nIn any case, some information is needed to create a grid or to validate whether a candidate value is appropriate (e.g. the number of neighbors should be a positive integer). {dials} is designed to:\n\n- Create an easy to use framework for describing and querying tuning parameters. This can include getting sequences or random tuning values, validating current values, transforming parameters, and other tasks.\n- Standardize the names of different parameters. Different packages in R use different argument names for the same quantities. dials proposes some standardized names so that the user doesnâ€™t need to memorize the syntactical minutiae of every package.\n- Work with the other tidymodels packages for modeling and machine learning using tidyverse principles.\n\n**Numeric Parameters**\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-74_23f69ec7d2dae8d28f78cc38958fa3c2'}\n\n```{.r .cell-code}\ncost_complexity()\n#> Cost-Complexity Parameter (quantitative)\n#> Transformer: log-10 [1e-100, Inf]\n#> Range (transformed scale): [-10, -1]\n```\n:::\n\n\nNote that this parameter is handled in log units and the default range of values is between `10^-10` and `0.1`. The range of possible values can be returned and changed based on some utility functions. Weâ€™ll use the pipe operator here:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-75_f2705696a6a179400ef31758e62de143'}\n\n```{.r .cell-code}\ncost_complexity() %>% range_get()\n#> $lower\n#> [1] 1e-10\n#> \n#> $upper\n#> [1] 0.1\n\ncost_complexity() %>% range_set(c(-5, 1))\n#> Cost-Complexity Parameter (quantitative)\n#> Transformer: log-10 [1e-100, Inf]\n#> Range (transformed scale): [-5, 1]\n\n# Or using the `range` argument\ncost_complexity(range = c(-5, 1))\n#> Cost-Complexity Parameter (quantitative)\n#> Transformer: log-10 [1e-100, Inf]\n#> Range (transformed scale): [-5, 1]\n```\n:::\n\n\nValues for this parameter can be obtained in a few different ways. To get a sequence of values that span the range:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-76_940dd89938013bf2d15241d0d0dfa054'}\n\n```{.r .cell-code}\n# Natural units\ncost_complexity() %>% value_seq(n = 4)\n#> [1] 1e-10 1e-07 1e-04 1e-01\n\n# Stay in the transformed space\ncost_complexity() %>% value_seq(n = 4, original = FALSE)\n#> [1] -10  -7  -4  -1\n```\n:::\n\n\nRandom values can be sampled too:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-77_ea5864bebee62bcdf7df6585abe6035e'}\n\n```{.r .cell-code}\nset.seed(5473)\ncost_complexity() %>% value_sample(n = 4)\n#> [1] 6.905948e-09 8.463338e-04 3.448626e-06 5.902285e-10\n```\n:::\n\n\n**Discrete Parameters**\n\nIn the discrete case there is no notion of a range. The parameter objects are defined by their discrete values.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-78_67a9ce09a42db90e0784b3ab78aaeef2'}\n\n```{.r .cell-code}\nweight_func()\n#> Distance Weighting Function  (qualitative)\n#> 10 possible values include:\n#> 'rectangular', 'triangular', 'epanechnikov', 'biweight', 'triweight', 'cos', ...\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-79_b8e33c57bfd668b868120b12aab9c76b'}\n\n```{.r .cell-code}\nweight_func() %>% value_set(c(\"rectangular\", \"triangular\"))\n#> Distance Weighting Function  (qualitative)\n#> 2 possible values include:\n#> 'rectangular' and 'triangular'\n\nweight_func() %>% value_sample(3)\n#> [1] \"inv\"      \"biweight\" \"optimal\"\n\nweight_func() %>% value_seq(3)\n#> [1] \"rectangular\"  \"triangular\"   \"epanechnikov\"\n```\n:::\n\n\n# tidymodels get started\n\nåœ°å€ï¼š<https://www.tidymodels.org/start/>\n\n## Build a model\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-80_98a6f4a6bb4ea85aa0fab1a1bf0a0fcd'}\n\n```{.r .cell-code}\nlibrary(broom.mixed) # for converting bayesian models to tidy tibbles\nlibrary(dotwhisker)  # for visualizing regression results\n```\n:::\n\n\n\n### The sea urchins data\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-81_10d6998e5b492f1768245bee83fe5206'}\n\n```{.r .cell-code}\nurchins <-\n  read_csv(\"https://tidymodels.org/start/models/urchins.csv\", show_col_types = FALSE) %>% \n  setNames(c(\"food_regime\", \"initial_volume\", \"width\")) %>% \n  mutate(food_regime = factor(food_regime, levels = c(\"Initial\", \"Low\", \"High\")))\n\nurchins\n#> # A tibble: 72 Ã— 3\n#>    food_regime initial_volume width\n#>    <fct>                <dbl> <dbl>\n#>  1 Initial                3.5 0.01 \n#>  2 Initial                5   0.02 \n#>  3 Initial                8   0.061\n#>  4 Initial               10   0.051\n#>  5 Initial               13   0.041\n#>  6 Initial               13   0.061\n#>  7 Initial               15   0.041\n#>  8 Initial               15   0.071\n#>  9 Initial               16   0.092\n#> 10 Initial               17   0.051\n#> # â„¹ 62 more rows\n```\n:::\n\n\nAs a first step in modeling, itâ€™s always a good idea to plot the data:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-82_4e51063560080dd3b0f635364d35b170'}\n\n```{.r .cell-code}\nggplot(urchins, aes(x = initial_volume,\n                    y = width,\n                    group = food_regime,\n                    col = food_regime)) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE) +\n  scale_color_viridis_d(option = \"plasma\", end = .7)\n#> `geom_smooth()` using formula = 'y ~ x'\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-82-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\nWe can see that urchins that were larger in volume at the start of the experiment tended to have wider sutures at the end, but the slopes of the lines look different so this effect may depend on the feeding regime condition.\n\n### Build and fit a model\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-83_7295ce999626665d8fc72b5cf645a5d5'}\n\n```{.r .cell-code}\nlm_fit <- linear_reg() %>% \n  fit(width ~ initial_volume * food_regime, data = urchins)\nlm_fit\n#> parsnip model object\n#> \n#> \n#> Call:\n#> stats::lm(formula = width ~ initial_volume * food_regime, data = data)\n#> \n#> Coefficients:\n#>                    (Intercept)                  initial_volume  \n#>                      0.0331216                       0.0015546  \n#>                 food_regimeLow                 food_regimeHigh  \n#>                      0.0197824                       0.0214111  \n#>  initial_volume:food_regimeLow  initial_volume:food_regimeHigh  \n#>                     -0.0012594                       0.0005254\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-84_bdc0a38f3190dd1161c7f9664819a695'}\n\n```{.r .cell-code}\ntidy(lm_fit)\n#> # A tibble: 6 Ã— 5\n#>   term                            estimate std.error statistic  p.value\n#>   <chr>                              <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)                     0.0331    0.00962      3.44  0.00100 \n#> 2 initial_volume                  0.00155   0.000398     3.91  0.000222\n#> 3 food_regimeLow                  0.0198    0.0130       1.52  0.133   \n#> 4 food_regimeHigh                 0.0214    0.0145       1.47  0.145   \n#> 5 initial_volume:food_regimeLow  -0.00126   0.000510    -2.47  0.0162  \n#> 6 initial_volume:food_regimeHigh  0.000525  0.000702     0.748 0.457\n```\n:::\n\n\nThis kind of output can be used to generate a dot-and-whisker plot of our regression results using the {dotwhisker} package:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-85_18a353e98059bfc1218c592d90b96618'}\n\n```{.r .cell-code}\ntidy(lm_fit) %>% \n  dwplot(dot_args = list(size = 2, color = \"black\"),\n         whisker_args = list(color = \"black\"),\n         vline = geom_vline(xintercept = 0, color = \"grey50\",\n                            linetype = 2))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-85-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n### Use a model to predict\n\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-86_fa93f69cb12c6597f8aa950d20d36a1c'}\n\n```{.r .cell-code}\nnew_points <- expand_grid(initial_volume = 20,\n                          food_regime = c(\"Initial\", \"Low\", \"High\"))\nnew_points\n#> # A tibble: 3 Ã— 2\n#>   initial_volume food_regime\n#>            <dbl> <chr>      \n#> 1             20 Initial    \n#> 2             20 Low        \n#> 3             20 High\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-87_1ec2b8fc835b97cc79cc944e8108f5c9'}\n\n```{.r .cell-code}\nmean_pred <- predict(lm_fit, new_data = new_points)\nmean_pred\n#> # A tibble: 3 Ã— 1\n#>    .pred\n#>    <dbl>\n#> 1 0.0642\n#> 2 0.0588\n#> 3 0.0961\n\nconf_int_pred <- predict(lm_fit, new_data = new_points, type = \"conf_int\")\nconf_int_pred\n#> # A tibble: 3 Ã— 2\n#>   .pred_lower .pred_upper\n#>         <dbl>       <dbl>\n#> 1      0.0555      0.0729\n#> 2      0.0499      0.0678\n#> 3      0.0870      0.105\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-88_320c10c1f34443744280dcac33de1ce4'}\n\n```{.r .cell-code}\nnew_points %>% \n  bind_cols(mean_pred) %>% \n  bind_cols(conf_int_pred) %>% \n  ggplot(aes(x = food_regime)) +\n  geom_point(aes(y = .pred)) + \n  geom_errorbar(aes(ymin = .pred_lower,\n                    ymax = .pred_upper),\n                width = .2) +\n  labs(y = \"urchin size\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-88-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n### Model with a different engine\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-89_eaaac2ba87a83bbde79493f8291759b3'}\n\n```{.r .cell-code}\n# set the prior distribution\nprior_dist <- rstanarm::student_t(df = 1)\n\nset.seed(123)\n\n# make the paarsnip model\nbayes_mod <- linear_reg() %>% \n  set_engine(\"stan\",\n             prior_intercept = prior_dist,\n             prior = prior_dist)\n\n# train the model\nbayes_fit <- \n  bayes_mod %>% \n  fit(width ~ initial_volume * food_regime, data = urchins)\n\nprint(bayes_fit, digits = 5)\n#> parsnip model object\n#> \n#> stan_glm\n#>  family:       gaussian [identity]\n#>  formula:      width ~ initial_volume * food_regime\n#>  observations: 72\n#>  predictors:   6\n#> ------\n#>                                Median   MAD_SD  \n#> (Intercept)                     0.03330  0.00933\n#> initial_volume                  0.00155  0.00039\n#> food_regimeLow                  0.01963  0.01278\n#> food_regimeHigh                 0.02089  0.01454\n#> initial_volume:food_regimeLow  -0.00126  0.00051\n#> initial_volume:food_regimeHigh  0.00055  0.00072\n#> \n#> Auxiliary parameter(s):\n#>       Median  MAD_SD \n#> sigma 0.02128 0.00186\n#> \n#> ------\n#> * For help interpreting the printed output see ?print.stanreg\n#> * For info on the priors used see ?prior_summary.stanreg\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-90_c49273ca8d5b865d50d3021ef5aac464'}\n\n```{.r .cell-code}\ntidy(bayes_fit, conf.int = TRUE)\n#> # A tibble: 6 Ã— 5\n#>   term                            estimate std.error  conf.low conf.high\n#>   <chr>                              <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 (Intercept)                     0.0333    0.00933   0.0177    0.0486  \n#> 2 initial_volume                  0.00155   0.000395  0.000882  0.00220 \n#> 3 food_regimeLow                  0.0196    0.0128   -0.00190   0.0408  \n#> 4 food_regimeHigh                 0.0209    0.0145   -0.00296   0.0456  \n#> 5 initial_volume:food_regimeLow  -0.00126   0.000515 -0.00208  -0.000411\n#> 6 initial_volume:food_regimeHigh  0.000550  0.000717 -0.000643  0.00168\n```\n:::\n\n\n## Preprocess your data with recipes\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-91_6f00d9e97371b84155610465ee8cff76'}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(tidyverse)\n\nlibrary(nycflights13)    # for flight data\nlibrary(skimr)           # for variable summaries\n```\n:::\n\n\nLetâ€™s use the nycflights13 data to predict whether a plane arrives more than 30 minutes late.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-92_a36bea6ef7eba8db63fcbb5669f5710b'}\n\n```{.r .cell-code}\nflight_data <- flights %>% \n  mutate(\n    arr_delay = factor(ifelse(arr_delay >= 30, \"late\", \"on_time\")),\n    date = as_date(time_hour)\n  ) %>% \n  inner_join(weather, join_by(origin, time_hour)) %>% \n  select(dep_time, flight, origin, dest, air_time, distance, \n         carrier, date, arr_delay, time_hour) %>% \n  drop_na() %>% \n  # For creating models, it is better to have qualitative columns\n  # encoded as factors (instead of character strings)\n  mutate(across(where(is.character), as_factor))\n\nflight_data %>% \n  count(arr_delay) %>% \n  mutate(prop = n / sum(n))\n#> # A tibble: 2 Ã— 3\n#>   arr_delay      n  prop\n#>   <fct>      <int> <dbl>\n#> 1 late       52540 0.161\n#> 2 on_time   273279 0.839\n```\n:::\n\n\nWe can see that about 16% of the flights in this data set arrived more than 30 minutes late.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-93_c47c953f6c8eeef9d5001157addbee70'}\n\n```{.r .cell-code}\nglimpse(flight_data)\n#> Rows: 325,819\n#> Columns: 10\n#> $ dep_time  <int> 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, 558, â€¦\n#> $ flight    <int> 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 49, 71â€¦\n#> $ origin    <fct> EWR, LGA, JFK, JFK, LGA, EWR, EWR, LGA, JFK, LGA, JFK, JFK, â€¦\n#> $ dest      <fct> IAH, IAH, MIA, BQN, ATL, ORD, FLL, IAD, MCO, ORD, PBI, TPA, â€¦\n#> $ air_time  <dbl> 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 158, 3â€¦\n#> $ distance  <dbl> 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, 1028,â€¦\n#> $ carrier   <fct> UA, UA, AA, B6, DL, UA, B6, EV, B6, AA, B6, B6, UA, UA, AA, â€¦\n#> $ date      <date> 2013-01-01, 2013-01-01, 2013-01-01, 2013-01-01, 2013-01-01,â€¦\n#> $ arr_delay <fct> on_time, on_time, late, on_time, on_time, on_time, on_time, â€¦\n#> $ time_hour <dttm> 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 05:00:â€¦\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-94_bbd8ee7ac48231aaaf600d9d09b5fbc5'}\n\n```{.r .cell-code}\nflight_data %>% skim(dest, carrier)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |           |\n|:------------------------|:----------|\n|Name                     |Piped data |\n|Number of rows           |325819     |\n|Number of columns        |10         |\n|_______________________  |           |\n|Column type frequency:   |           |\n|factor                   |2          |\n|________________________ |           |\n|Group variables          |None       |\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts                                     |\n|:-------------|---------:|-------------:|:-------|--------:|:----------------------------------------------|\n|dest          |         0|             1|FALSE   |      104|ATL: 16771, ORD: 16507, LAX: 15942, BOS: 14948 |\n|carrier       |         0|             1|FALSE   |       16|UA: 57489, B6: 53715, EV: 50868, DL: 47465     |\n:::\n:::\n\n\nBecause weâ€™ll be using a simple logistic regression model, the variables `dest` and `carrier` will be converted to dummy variables. However, some of these values do not occur very frequently and this could complicate our analysis. Weâ€™ll discuss specific steps later in this article that we can add to our recipe to address this issue before modeling.\n\n### Data splitting\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-95_1bb8eb924a79ea6c8da7b5605eddc1be'}\n\n```{.r .cell-code}\nset.seed(222)\n\ndata_split <- initial_split(flight_data, prop = 3/4)\n\ntrain_data <- training(data_split)\ntest_data <- testing(data_split)\n```\n:::\n\n\n### Create recipe and roles\n\nWe can use the `update_role()` function to let recipes know that `flight` and `time_hour` are variables with a custom role that we called `\"ID\"` (a role can have any character value). Whereas our formula included all variables in the training set other than `arr_delay` as predictors, this tells the recipe to keep these two variables but not use them as either outcomes or predictors.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-96_3314c225910faf69542452f610121625'}\n\n```{.r .cell-code}\nflights_rec <- \n  recipe(arr_delay ~ ., data = train_data) %>% \n  update_role(flight, time_hour, new_role = \"ID\")\n```\n:::\n\n\nTo get the current set of variables and roles, use the `summary()` function:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-97_b7601b0d273e8b1c18a1a04ddea6a7dc'}\n\n```{.r .cell-code}\nsummary(flights_rec)\n#> # A tibble: 10 Ã— 4\n#>    variable  type      role      source  \n#>    <chr>     <list>    <chr>     <chr>   \n#>  1 dep_time  <chr [2]> predictor original\n#>  2 flight    <chr [2]> ID        original\n#>  3 origin    <chr [3]> predictor original\n#>  4 dest      <chr [3]> predictor original\n#>  5 air_time  <chr [2]> predictor original\n#>  6 distance  <chr [2]> predictor original\n#>  7 carrier   <chr [3]> predictor original\n#>  8 date      <chr [1]> predictor original\n#>  9 time_hour <chr [1]> ID        original\n#> 10 arr_delay <chr [3]> outcome   original\n```\n:::\n\n\n### Create features\n\nItâ€™s possible that the numeric date variable is a good option for modeling; perhaps the model would benefit from a linear trend between the log-odds of a late arrival and the numeric date variable. However, it might be better to add model terms *derived* from the date that have a better potential to be important to the model. For example, we could derive the following meaningful features from the single `date` variable:\n\n- the day of the week,\n- the month, and\n- whether or not the date corresponds to a holiday.\n\nLetâ€™s do all three of these by adding steps to our recipe:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-98_b1b6212433de38feacee640a978b0b28'}\n\n```{.r .cell-code}\nflights_rec <- \n  recipe(arr_delay ~ ., data = train_data) %>% \n  update_role(flight, time_hour, new_role = \"ID\") %>% \n  step_date(date, features = c(\"dow\", \"month\")) %>% \n  step_holiday(date,\n               holidays = timeDate::listHolidays(\"US\"),\n               keep_original_cols = FALSE)\n```\n:::\n\n\nUnlike the standard model formula methods in R, a recipe **does not** automatically create these dummy variables for you; youâ€™ll need to tell your recipe to add this step. This is for two reasons. First, many models do not require numeric predictors, so dummy variables may not always be preferred. Second, recipes can also be used for purposes outside of modeling, where non-dummy versions of the variables may work better. For example, you may want to make a table or a plot with a variable as a single factor. For those reasons, you need to explicitly tell recipes to create dummy variables using `step_dummy()`:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-99_3a7a046e2d6d5d8b3096489834a9f24c'}\n\n```{.r .cell-code}\nflights_rec <- \n  recipe(arr_delay ~ ., data = train_data) %>% \n  update_role(flight, time_hour, new_role = \"ID\") %>% \n  step_date(date, features = c(\"dow\", \"month\")) %>%               \n  step_holiday(date, \n               holidays = timeDate::listHolidays(\"US\"), \n               keep_original_cols = FALSE) %>%\n  step_dummy(all_nominal_predictors())\n```\n:::\n\n\nWe need one final step to add to our recipe. Since `carrier` and `dest` have some infrequently occurring factor values, it is possible that dummy variables might be created for values that donâ€™t exist in the training set. For example, there is one destination that is only in the test set:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-100_d72cfdc43bf42e86746f290271cf693d'}\n\n```{.r .cell-code}\ntest_data %>% \n  distinct(dest) %>% \n  anti_join(train_data, join_by(dest))\n#> # A tibble: 1 Ã— 1\n#>   dest \n#>   <fct>\n#> 1 LEX\n```\n:::\n\n\nWhen the recipe is applied to the training set, a column is made for LEX because the factor levels come from `flight_data` (not the training set), but this column will contain all zeros. This is a â€œzero-variance predictorâ€ that has no information within the column. While some R functions will not produce an error for such predictors, it usually causes warnings and other issues. `step_zv()` will remove columns from the data when the training set data have a single value, so it is added to the recipe *after* `step_dummy()`:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-101_73e754cf9f7fb17668519de984256642'}\n\n```{.r .cell-code}\nflights_rec <- \n  recipe(arr_delay ~ ., data = train_data) %>% \n  update_role(flight, time_hour, new_role = \"ID\") %>% \n  step_date(date, features = c(\"dow\", \"month\")) %>%\n  step_holiday(date, \n               holidays = timeDate::listHolidays(\"US\"), \n               keep_original_cols = FALSE) %>% \n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())\n```\n:::\n\n\n### Fit a model with a recipe\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-102_c83ccfd869a9ba4268bcbf4a76eef074'}\n\n```{.r .cell-code}\nlr_mod <- \n  logistic_reg() %>% \n  set_engine(\"glm\")\n```\n:::\n\n\nWe will want to use our recipe across several steps as we train and test our model. We will:\n\n- **Process the recipe using the training set:** This involves any estimation or calculations based on the training set. For our recipe, the training set will be used to determine which predictors should be converted to dummy variables and which predictors will have zero-variance in the training set, and should be slated for removal.\n\n- **Apply the recipe to the training set:** We create the final predictor set on the training set.\n\n- **Apply the recipe to the test set:** We create the final predictor set on the test set. Nothing is recomputed and no information from the test set is used here; the dummy variable and zero-variance results from the training set are applied to the test set.\n\nTo simplify this process, we can use a model *workflow*, which pairs a model and recipe together. This is a straightforward approach because different recipes are often needed for different models, so when a model and recipe are bundled, it becomes easier to train and test workflows. Weâ€™ll use the workflows package from tidymodels to bundle our parsnip model (`lr_mod`) with our recipe (`flights_rec`).\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-103_e752b7109f8b66b5f3a2da428258f9ad'}\n\n```{.r .cell-code}\nflights_wflow <- \n  workflow() %>% \n  add_model(lr_mod) %>% \n  add_recipe(flights_rec)\n\nflights_wflow\n#> â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#> Preprocessor: Recipe\n#> Model: logistic_reg()\n#> \n#> â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> 4 Recipe Steps\n#> \n#> â€¢ step_date()\n#> â€¢ step_holiday()\n#> â€¢ step_dummy()\n#> â€¢ step_zv()\n#> \n#> â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> Logistic Regression Model Specification (classification)\n#> \n#> Computational engine: glm\n```\n:::\n\n\nNow, there is a single function that can be used to prepare the recipe and train the model from the resulting predictors:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-104_e3187600a756eb928f0bfe9097fc858d'}\n\n```{.r .cell-code}\nflights_fit <- \n  flights_wflow %>% \n  fit(data = train_data)\n```\n:::\n\n\nThis object has the finalized recipe and fitted model objects inside. You may want to extract the model or recipe objects from the workflow. To do this, you can use the helper functions `extract_fit_parsnip()` and `extract_recipe()`. For example, here we pull the fitted model object then use the `broom::tidy()` function to get a tidy tibble of model coefficients:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-105_b9d807c15712d1a9c4109f57bbd4e09b'}\n\n```{.r .cell-code}\nflights_fit %>% \n  extract_fit_parsnip() %>% \n  tidy()\n#> # A tibble: 158 Ã— 5\n#>    term                         estimate std.error statistic  p.value\n#>    <chr>                           <dbl>     <dbl>     <dbl>    <dbl>\n#>  1 (Intercept)                   6.56    2.10           3.12 1.80e- 3\n#>  2 dep_time                     -0.00166 0.0000141   -118.   0       \n#>  3 air_time                     -0.0440  0.000563     -78.2  0       \n#>  4 distance                      0.00508 0.00150        3.38 7.13e- 4\n#>  5 date_USChristmasDay           1.35    0.178          7.59 3.32e-14\n#>  6 date_USColumbusDay            0.721   0.170          4.23 2.33e- 5\n#>  7 date_USCPulaskisBirthday      0.804   0.139          5.78 7.38e- 9\n#>  8 date_USDecorationMemorialDay  0.582   0.117          4.96 7.22e- 7\n#>  9 date_USElectionDay            0.945   0.190          4.97 6.73e- 7\n#> 10 date_USGoodFriday             1.24    0.167          7.44 1.04e-13\n#> # â„¹ 148 more rows\n```\n:::\n\n\n### Use a trained workflow to predict\n\nOur goal was to predict whether a plane arrives more than 30 minutes late. We have just:\n\n- Built the model (`lr_mod`),\n\n- Created a preprocessing recipe (`flights_rec`),\n\n- Bundled the model and recipe (`flights_wflow`), and\n\n- Trained our workflow using a single call to `fit()`.\n\nThe next step is to use the trained workflow (`flights_fit`) to predict with the unseen test data, which we will do with a single call to `predict()`. The `predict()` method applies the recipe to the new data, then passes them to the fitted model.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-106_3f0ea060c1685481c36ad2ca8f832aa8'}\n\n```{.r .cell-code}\npredict(flights_fit, test_data)\n#> # A tibble: 81,455 Ã— 1\n#>    .pred_class\n#>    <fct>      \n#>  1 on_time    \n#>  2 on_time    \n#>  3 on_time    \n#>  4 on_time    \n#>  5 on_time    \n#>  6 on_time    \n#>  7 on_time    \n#>  8 on_time    \n#>  9 on_time    \n#> 10 on_time    \n#> # â„¹ 81,445 more rows\n```\n:::\n\n\nBecause our outcome variable here is a factor, the output from `predict()` returns the predicted class: `late` versus `on_time.` But, letâ€™s say we want the predicted class probabilities for each flight instead. To return those, we can specify `type = \"prob\"` when we use `predict()` or use `augment()` with the model plus test data to save them together:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-107_b81f6e6d97259bf42f04e4bf49e25664'}\n\n```{.r .cell-code}\nflights_aug <- \n  augment(flights_fit, test_data)\n\nflights_aug %>% \n  select(arr_delay, time_hour, flight,\n         .pred_class, .pred_on_time)\n#> # A tibble: 81,455 Ã— 5\n#>    arr_delay time_hour           flight .pred_class .pred_on_time\n#>    <fct>     <dttm>               <int> <fct>               <dbl>\n#>  1 on_time   2013-01-01 05:00:00   1545 on_time             0.945\n#>  2 on_time   2013-01-01 05:00:00   1714 on_time             0.949\n#>  3 on_time   2013-01-01 06:00:00    507 on_time             0.964\n#>  4 on_time   2013-01-01 06:00:00   5708 on_time             0.961\n#>  5 on_time   2013-01-01 06:00:00     71 on_time             0.962\n#>  6 on_time   2013-01-01 06:00:00    194 on_time             0.975\n#>  7 on_time   2013-01-01 06:00:00   1124 on_time             0.963\n#>  8 on_time   2013-01-01 05:00:00   1806 on_time             0.981\n#>  9 on_time   2013-01-01 06:00:00   1187 on_time             0.935\n#> 10 on_time   2013-01-01 06:00:00   4650 on_time             0.931\n#> # â„¹ 81,445 more rows\n```\n:::\n\n\nLetâ€™s use the area under the ROC curve as our metric, computed using `roc_curve()` and `roc_auc()` from the yardstick package.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-108_b03e1f0b0ce0eb66891f470302ec7f5c'}\n\n```{.r .cell-code}\nflights_aug %>% \n  roc_curve(truth = arr_delay, .pred_late) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-108-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\nSimilarly, `roc_auc()` estimates the area under the curve:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-109_3ee96f49eb512d6491e8b7f4a4ee4b11'}\n\n```{.r .cell-code}\nflights_aug %>% \n  roc_auc(truth = arr_delay, .pred_late)\n#> # A tibble: 1 Ã— 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 roc_auc binary         0.764\n```\n:::\n\n\n<!-- We leave it to the reader to test out this workflow without this recipe. You can use workflows::add_formula(arr_delay ~ .) instead of add_recipe() (remember to remove the identification variables first!), and see whether our recipe improved our modelâ€™s ability to predict late arrivals.   -->\n\n\n\n\n\n## Evaluate your model with resampling\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-111_f2530df1ba9507d6d758b14c43ead9dd'}\n\n```{.r .cell-code}\nlibrary(modeldata)\n```\n:::\n\n\n### The cell image data\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-112_82b78d378d496fc58c15762ea87f3b11'}\n\n```{.r .cell-code}\ndata(cells)\ncells\n#> # A tibble: 2,019 Ã— 58\n#>    case  class angle_ch_1 area_ch_1 avg_inten_ch_1 avg_inten_ch_2 avg_inten_ch_3\n#>    <fct> <fct>      <dbl>     <int>          <dbl>          <dbl>          <dbl>\n#>  1 Test  PS        143.         185           15.7           4.95           9.55\n#>  2 Train PS        134.         819           31.9         207.            69.9 \n#>  3 Train WS        107.         431           28.0         116.            63.9 \n#>  4 Train PS         69.2        298           19.5         102.            28.2 \n#>  5 Test  PS          2.89       285           24.3         112.            20.5 \n#>  6 Test  WS         40.7        172          326.          654.           129.  \n#>  7 Test  WS        174.         177          260.          596.           124.  \n#>  8 Test  PS        180.         251           18.3           5.73          17.2 \n#>  9 Test  WS         18.9        495           16.1          89.5           13.7 \n#> 10 Test  WS        153.         384           17.7          89.9           20.4 \n#> # â„¹ 2,009 more rows\n#> # â„¹ 51 more variables: avg_inten_ch_4 <dbl>, convex_hull_area_ratio_ch_1 <dbl>,\n#> #   convex_hull_perim_ratio_ch_1 <dbl>, diff_inten_density_ch_1 <dbl>,\n#> #   diff_inten_density_ch_3 <dbl>, diff_inten_density_ch_4 <dbl>,\n#> #   entropy_inten_ch_1 <dbl>, entropy_inten_ch_3 <dbl>,\n#> #   entropy_inten_ch_4 <dbl>, eq_circ_diam_ch_1 <dbl>,\n#> #   eq_ellipse_lwr_ch_1 <dbl>, eq_ellipse_oblate_vol_ch_1 <dbl>, â€¦\n```\n:::\n\n\n### Predicting image segmentation quality\n\nA cell-based experiment might involve millions of cells so it is unfeasible to visually assess them all. Instead, a subsample can be created and these cells can be manually labeled by experts as either poorly segmented (`PS`) or well-segmented (`WS`). If we can predict these labels accurately, the larger data set can be improved by filtering out the cells most likely to be poorly segmented.\n\n### Back to the cells data\n\nThe rates of the classes are somewhat imbalanced; there are more poorly segmented cells than well-segmented cells:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-113_74523816477d293ba1942516161fc32e'}\n\n```{.r .cell-code}\ncells %>% \n  count(class) %>% \n  mutate(prop = n / sum(n))\n#> # A tibble: 2 Ã— 3\n#>   class     n  prop\n#>   <fct> <int> <dbl>\n#> 1 PS     1300 0.644\n#> 2 WS      719 0.356\n```\n:::\n\n\n### Data splitting\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-114_98cda16729eed321bc5b8d7014d951b8'}\n\n```{.r .cell-code}\nset.seed(123)\ncell_split <- initial_split(cells %>% select(-case),\n                            strata = class)\n```\n:::\n\n\nHere we used the `strata` argument, which conducts a stratified split. This ensures that, despite the imbalance we noticed in our `class` variable, our training and test data sets will keep roughly the same proportions of poorly and well-segmented cells as in the original data. After the initial_split, the `training()` and `testing()` functions return the actual data sets.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-115_cc4f7f66d133493fd49e2abebda4616d'}\n\n```{.r .cell-code}\ncell_train <- training(cell_split)\ncell_test <- testing(cell_split)\n\n# training set proportions by class\ncell_train %>% \n  count(class) %>% \n  mutate(prop = n / sum(n))\n#> # A tibble: 2 Ã— 3\n#>   class     n  prop\n#>   <fct> <int> <dbl>\n#> 1 PS      975 0.644\n#> 2 WS      539 0.356\n\n# test set proportions by class\ncell_test %>% \n  count(class) %>% \n  mutate(prop =  n / sum(n))\n#> # A tibble: 2 Ã— 3\n#>   class     n  prop\n#>   <fct> <int> <dbl>\n#> 1 PS      325 0.644\n#> 2 WS      180 0.356\n```\n:::\n\n\n### Modeling\n\nOne of the benefits of a random forest model is that it is very low maintenance; it requires very little preprocessing of the data and the default parameters tend to give reasonable results. For that reason, we wonâ€™t create a recipe for the `cells` data.\n\nTo fit a random forest model on the training set, letâ€™s use the {parsnip} package with the {ranger} engine. We first define the model that we want to create:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-116_4e059b8baa2308f31d872a80ef270671'}\n\n```{.r .cell-code}\nrf_mod <- \n  rand_forest(trees = 1000) %>% \n  set_engine(\"ranger\") %>% \n  set_mode(\"classification\")\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-117_4832fc1c91e047633ffbf44009809d1d'}\n\n```{.r .cell-code}\nset.seed(234)\n\nrf_fit <- \n  rf_mod %>% \n  fit(class ~ ., data = cell_train)\n\nrf_fit\n#> parsnip model object\n#> \n#> Ranger result\n#> \n#> Call:\n#>  ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) \n#> \n#> Type:                             Probability estimation \n#> Number of trees:                  1000 \n#> Sample size:                      1514 \n#> Number of independent variables:  56 \n#> Mtry:                             7 \n#> Target node size:                 10 \n#> Variable importance mode:         none \n#> Splitrule:                        gini \n#> OOB prediction error (Brier s.):  0.1187479\n```\n:::\n\n\n### Estimating performance\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-118_faaeefef84484e9961185b49a8916b64'}\n\n```{.r .cell-code}\nrf_testing_pred <- \n  predict(rf_fit, cell_test) %>% \n  bind_cols(predict(rf_fit, cell_test, type = \"prob\")) %>% \n  bind_cols(cell_test %>% select(class))\n\nrf_testing_pred %>% \n  roc_auc(truth = class, .pred_PS)\n#> # A tibble: 1 Ã— 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 roc_auc binary         0.891\n\nrf_testing_pred %>% \n  accuracy(truth = class, .pred_class)\n#> # A tibble: 1 Ã— 3\n#>   .metric  .estimator .estimate\n#>   <chr>    <chr>          <dbl>\n#> 1 accuracy binary         0.814\n```\n:::\n\n\n### Resampling to the rescue\n\nThe final resampling estimates for the model are the **averages** of the performance statistics replicates.\n\n### Fit a model with resampling\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-119_8789425ee3140220684790a51f716d51'}\n\n```{.r .cell-code}\nset.seed(345)\n\nfolds <- vfold_cv(cell_train, v = 10)\nfolds\n#> #  10-fold cross-validation \n#> # A tibble: 10 Ã— 2\n#>    splits             id    \n#>    <list>             <chr> \n#>  1 <split [1362/152]> Fold01\n#>  2 <split [1362/152]> Fold02\n#>  3 <split [1362/152]> Fold03\n#>  4 <split [1362/152]> Fold04\n#>  5 <split [1363/151]> Fold05\n#>  6 <split [1363/151]> Fold06\n#>  7 <split [1363/151]> Fold07\n#>  8 <split [1363/151]> Fold08\n#>  9 <split [1363/151]> Fold09\n#> 10 <split [1363/151]> Fold10\n```\n:::\n\n\nThe list column for `splits` contains the information on which rows belong in the analysis and assessment sets. There are functions that can be used to extract the individual resampled data called `analysis()` and `assessment()`.\n\nYou have several options for building an object for resampling:\n\n- Resample a model specification preprocessed with a formula or recipe, or\n- Resample a `workflow()` that bundles together a model specification and formula/recipe.\n\nFor this example, letâ€™s use a `workflow()` that bundles together the random forest model and a formula, since we are not using a recipe. Whichever of these options you use, the syntax to `fit_resamples()` is very similar to `fit()`:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-120_836d0c0d0be4b11449d847c98f21569d'}\n\n```{.r .cell-code}\nrf_wf <- \n  workflow() %>% \n  add_model(rf_mod) %>% \n  add_formula(class ~ .)\n\nset.seed(456)\nrf_fit_rs <- \n  rf_wf %>% \n  fit_resamples(folds)\n\nrf_fit_rs\n#> # Resampling results\n#> # 10-fold cross-validation \n#> # A tibble: 10 Ã— 4\n#>    splits             id     .metrics         .notes          \n#>    <list>             <chr>  <list>           <list>          \n#>  1 <split [1362/152]> Fold01 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]>\n#>  2 <split [1362/152]> Fold02 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]>\n#>  3 <split [1362/152]> Fold03 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]>\n#>  4 <split [1362/152]> Fold04 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]>\n#>  5 <split [1363/151]> Fold05 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]>\n#>  6 <split [1363/151]> Fold06 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]>\n#>  7 <split [1363/151]> Fold07 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]>\n#>  8 <split [1363/151]> Fold08 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]>\n#>  9 <split [1363/151]> Fold09 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]>\n#> 10 <split [1363/151]> Fold10 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]>\n```\n:::\n\n\nThe results are similar to the `folds` results with some extra columns. The column `.metrics` contains the performance statistics created from the 10 assessment sets. These can be manually unnested but the tune package contains a number of simple functions that can extract these data:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-121_02e635fb52004ea7ffd3cc5fbf7028a6'}\n\n```{.r .cell-code}\ncollect_metrics(rf_fit_rs)\n#> # A tibble: 2 Ã— 6\n#>   .metric  .estimator  mean     n std_err .config             \n#>   <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 accuracy binary     0.833    10 0.00876 Preprocessor1_Model1\n#> 2 roc_auc  binary     0.905    10 0.00627 Preprocessor1_Model1\n```\n:::\n\n\nThink about these values we now have for accuracy and AUC. These performance metrics are now more realistic (i.e. lower) than our ill-advised first attempt at computing performance metrics in the section above. If we wanted to try different model types for this data set, we could more confidently compare performance metrics computed using resampling to choose between models. Also, remember that at the end of our project, we return to our test set to estimate final model performance.\n\nResampling allows us to simulate how well our model will perform on new data, and the test set acts as the final, unbiased check for our modelâ€™s performance.\n\n## Tune model parameters\n\n### Introduction\n\nSome model parameters cannot be learned directly from a data set during model training; these kinds of parameters are called **hyperparameters**. Some examples of hyperparameters include the number of predictors that are sampled at splits in a tree-based model (we call this `mtry` in tidymodels) or the learning rate in a boosted tree model (we call this `learn_rate`). Instead of learning these kinds of hyperparameters during model training, we can estimate the best values for these values by training many models on resampled data sets and exploring how well all these models perform. This process is called **tuning**.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-122_7cca536215addd3e95c2114595862034'}\n\n```{.r .cell-code}\nlibrary(rpart.plot)\nlibrary(vip)\n```\n:::\n\n\n### Predicting image segemention, but better\n\n\n\n\n\nRandom forest models are a tree-based ensemble method, and typically perform well with default hyperparameters. However, the accuracy of some other tree-based models, such as boosted tree models or decision tree models, can be sensitive to the values of hyperparameters. In this article, we will train a **decision tree** model. There are several hyperparameters for decision tree models that can be tuned for better performance. Letâ€™s explore:\n\n- the complexity parameter (which we call `cost_complexity` in tidymodels) for the tree, and\n- the maximum `tree_depth`.\n\nTuning these hyperparameters can improve model performance because decision tree models are prone to overfitting. This happens because single tree models tend to fit the training data *too well* â€” so well, in fact, that they over-learn patterns present in the training data that end up being detrimental when predicting new data.\n\n### Tuning hyperparameters\n\nTo tune the decision tree hyperparameters `cost_complexity` and `tree_depth`, we create a model specification that identifies which hyperparameters we plan to tune.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-124_83102cac317b0e29aa4207d5b0732f64'}\n\n```{.r .cell-code}\ntune_spec <- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %>% \n  set_engine(\"rpart\") %>% \n  set_mode(\"classification\")\n\ntune_spec\n#> Decision Tree Model Specification (classification)\n#> \n#> Main Arguments:\n#>   cost_complexity = tune()\n#>   tree_depth = tune()\n#> \n#> Computational engine: rpart\n```\n:::\n\n\nWe can create a regular grid of values to try using some convenience functions for each hyperparameter:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-125_556141b5c1a748144194323cffb162be'}\n\n```{.r .cell-code}\ntree_grid <- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = 5)\n\ntree_grid\n#> # A tibble: 25 Ã— 2\n#>    cost_complexity tree_depth\n#>              <dbl>      <int>\n#>  1    0.0000000001          1\n#>  2    0.0000000178          1\n#>  3    0.00000316            1\n#>  4    0.000562              1\n#>  5    0.1                   1\n#>  6    0.0000000001          4\n#>  7    0.0000000178          4\n#>  8    0.00000316            4\n#>  9    0.000562              4\n#> 10    0.1                   4\n#> # â„¹ 15 more rows\n```\n:::\n\n\nArmed with our grid filled with 25 candidate decision tree models, letâ€™s create cross-validation folds for tuning:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-126_df6933eb0f14166231703e34beadf3e1'}\n\n```{.r .cell-code}\nset.seed(234)\ncell_folds <- vfold_cv(cell_train)\n```\n:::\n\n\n### Model tuning with a grid\n\nWe are ready to tune! Letâ€™s use `tune_grid()` to fit models at all the different values we chose for each tuned hyperparameter. There are several options for building the object for tuning:\n\nTune a model specification along with a recipe or model, or\n\nTune a `workflow()` that bundles together a model specification and a recipe or model preprocessor.\n\nHere we use a `workflow()` with a straightforward formula; if this model required more involved data preprocessing, we could use `add_recipe()` instead of `add_formula()`.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-127_9c14f00ae9074a571f2caaabe7b38d6c'}\n\n```{.r .cell-code}\nset.seed(345)\n\ntree_wf <- \n  workflow() %>%\n  add_model(tune_spec) %>%\n  add_formula(class ~ .)\n\ntree_res <- \n  tree_wf %>% \n  tune_grid(\n    resamples = cell_folds,\n    grid = tree_grid\n  )\n\ntree_res\n#> # Tuning results\n#> # 10-fold cross-validation \n#> # A tibble: 10 Ã— 4\n#>    splits             id     .metrics          .notes          \n#>    <list>             <chr>  <list>            <list>          \n#>  1 <split [1362/152]> Fold01 <tibble [50 Ã— 6]> <tibble [0 Ã— 3]>\n#>  2 <split [1362/152]> Fold02 <tibble [50 Ã— 6]> <tibble [0 Ã— 3]>\n#>  3 <split [1362/152]> Fold03 <tibble [50 Ã— 6]> <tibble [0 Ã— 3]>\n#>  4 <split [1362/152]> Fold04 <tibble [50 Ã— 6]> <tibble [0 Ã— 3]>\n#>  5 <split [1363/151]> Fold05 <tibble [50 Ã— 6]> <tibble [0 Ã— 3]>\n#>  6 <split [1363/151]> Fold06 <tibble [50 Ã— 6]> <tibble [0 Ã— 3]>\n#>  7 <split [1363/151]> Fold07 <tibble [50 Ã— 6]> <tibble [0 Ã— 3]>\n#>  8 <split [1363/151]> Fold08 <tibble [50 Ã— 6]> <tibble [0 Ã— 3]>\n#>  9 <split [1363/151]> Fold09 <tibble [50 Ã— 6]> <tibble [0 Ã— 3]>\n#> 10 <split [1363/151]> Fold10 <tibble [50 Ã— 6]> <tibble [0 Ã— 3]>\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-128_67af4b2a015195d1da78c244f9c49630'}\n\n```{.r .cell-code}\ntree_res %>% \n  collect_metrics()\n#> # A tibble: 50 Ã— 8\n#>    cost_complexity tree_depth .metric  .estimator  mean     n std_err .config   \n#>              <dbl>      <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>     \n#>  1    0.0000000001          1 accuracy binary     0.732    10  0.0148 Preprocesâ€¦\n#>  2    0.0000000001          1 roc_auc  binary     0.777    10  0.0107 Preprocesâ€¦\n#>  3    0.0000000178          1 accuracy binary     0.732    10  0.0148 Preprocesâ€¦\n#>  4    0.0000000178          1 roc_auc  binary     0.777    10  0.0107 Preprocesâ€¦\n#>  5    0.00000316            1 accuracy binary     0.732    10  0.0148 Preprocesâ€¦\n#>  6    0.00000316            1 roc_auc  binary     0.777    10  0.0107 Preprocesâ€¦\n#>  7    0.000562              1 accuracy binary     0.732    10  0.0148 Preprocesâ€¦\n#>  8    0.000562              1 roc_auc  binary     0.777    10  0.0107 Preprocesâ€¦\n#>  9    0.1                   1 accuracy binary     0.732    10  0.0148 Preprocesâ€¦\n#> 10    0.1                   1 roc_auc  binary     0.777    10  0.0107 Preprocesâ€¦\n#> # â„¹ 40 more rows\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-129_d9667abf6ff9011fc592106667515391'}\n\n```{.r .cell-code}\ntree_res %>% \n  collect_metrics() %>% \n  ggplot(aes(cost_complexity, mean,\n             color = factor(tree_depth))) +\n  geom_line(linewidth = 1.5, alpha = .6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0) +\n  labs(color = \"tree_depth\") +\n  theme(legend.position = \"right\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-129-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\nThe `show_best()` function shows us the top 5 candidate models by default:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-130_64d79778a028c9f799d092b8a18b8af4'}\n\n```{.r .cell-code}\ntree_res %>% \n  show_best(\"accuracy\")\n#> # A tibble: 5 Ã— 8\n#>   cost_complexity tree_depth .metric  .estimator  mean     n std_err .config    \n#>             <dbl>      <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>      \n#> 1    0.0000000001          4 accuracy binary     0.807    10  0.0119 Preprocessâ€¦\n#> 2    0.0000000178          4 accuracy binary     0.807    10  0.0119 Preprocessâ€¦\n#> 3    0.00000316            4 accuracy binary     0.807    10  0.0119 Preprocessâ€¦\n#> 4    0.000562              4 accuracy binary     0.807    10  0.0119 Preprocessâ€¦\n#> 5    0.1                   4 accuracy binary     0.786    10  0.0124 Preprocessâ€¦\n```\n:::\n\n\nWe can also use the `select_best()` function to pull out the single set of hyperparameter values for our best decision tree model:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-131_c6c605a5259c11e8deea1a364f99c4f2'}\n\n```{.r .cell-code}\nbest_tree <- tree_res %>% \n  select_best(\"accuracy\")\n\nbest_tree\n#> # A tibble: 1 Ã— 3\n#>   cost_complexity tree_depth .config              \n#>             <dbl>      <int> <chr>                \n#> 1    0.0000000001          4 Preprocessor1_Model06\n```\n:::\n\n\n### Finalizing our model\n\nWe can update (or â€œfinalizeâ€) our workflow object `tree_wf` with the values from `select_best()`.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-132_4bf30f3e1cad9a8655bbf34c6aceb0eb'}\n\n```{.r .cell-code}\nfinal_wf <- \n  tree_wf %>% \n  finalize_workflow(best_tree)\n\nfinal_wf\n#> â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#> Preprocessor: Formula\n#> Model: decision_tree()\n#> \n#> â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> class ~ .\n#> \n#> â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> Decision Tree Model Specification (classification)\n#> \n#> Main Arguments:\n#>   cost_complexity = 1e-10\n#>   tree_depth = 4\n#> \n#> Computational engine: rpart\n```\n:::\n\n\nOur tuning is done!\n\n### The last fit\n\nWe can use the function `last_fit()` with our finalized model; this function *fits* the finalized model on the full training data set and evaluates the finalized model on the testing data.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-133_a442ce09b9624c2899bb03a92d48e72f'}\n\n```{.r .cell-code}\nfinal_fit <- \n  final_wf %>% \n  last_fit(cell_split)\n\nfinal_fit %>% \n  collect_metrics()\n#> # A tibble: 2 Ã— 4\n#>   .metric  .estimator .estimate .config             \n#>   <chr>    <chr>          <dbl> <chr>               \n#> 1 accuracy binary         0.802 Preprocessor1_Model1\n#> 2 roc_auc  binary         0.840 Preprocessor1_Model1\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-134_d6e21684c65112595f7c876d763a5adc'}\n\n```{.r .cell-code}\nfinal_fit %>% \n  collect_predictions() %>% \n  roc_curve(class, .pred_PS) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-134-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\nThe performance metrics from the test set indicate that we did not overfit during our tuning procedure.\n\nThe `final_fit` object contains a finalized, fitted workflow that you can use for predicting on new data or further understanding the results. You may want to extract this object, using one of the extract_ helper functions.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-135_09ee24a7af9a564bd6f5e0e71f5e0152'}\n\n```{.r .cell-code}\nfinal_tree <- extract_workflow(final_fit)\nfinal_tree\n#> â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#> Preprocessor: Formula\n#> Model: decision_tree()\n#> \n#> â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> class ~ .\n#> \n#> â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> n= 1514 \n#> \n#> node), split, n, loss, yval, (yprob)\n#>       * denotes terminal node\n#> \n#>  1) root 1514 539 PS (0.64398943 0.35601057)  \n#>    2) total_inten_ch_2< 41732.5 642  33 PS (0.94859813 0.05140187)  \n#>      4) shape_p_2_a_ch_1>=1.251801 631  27 PS (0.95721078 0.04278922) *\n#>      5) shape_p_2_a_ch_1< 1.251801 11   5 WS (0.45454545 0.54545455) *\n#>    3) total_inten_ch_2>=41732.5 872 366 WS (0.41972477 0.58027523)  \n#>      6) fiber_width_ch_1< 11.37318 406 160 PS (0.60591133 0.39408867)  \n#>       12) avg_inten_ch_1< 145.4883 293  85 PS (0.70989761 0.29010239) *\n#>       13) avg_inten_ch_1>=145.4883 113  38 WS (0.33628319 0.66371681)  \n#>         26) total_inten_ch_3>=57919.5 33  10 PS (0.69696970 0.30303030) *\n#>         27) total_inten_ch_3< 57919.5 80  15 WS (0.18750000 0.81250000) *\n#>      7) fiber_width_ch_1>=11.37318 466 120 WS (0.25751073 0.74248927)  \n#>       14) eq_ellipse_oblate_vol_ch_1>=1673.942 30   8 PS (0.73333333 0.26666667)  \n#>         28) var_inten_ch_3>=41.10858 20   2 PS (0.90000000 0.10000000) *\n#>         29) var_inten_ch_3< 41.10858 10   4 WS (0.40000000 0.60000000) *\n#>       15) eq_ellipse_oblate_vol_ch_1< 1673.942 436  98 WS (0.22477064 0.77522936) *\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-136_99ed71ff81304f23111311e08f1a5f04'}\n\n```{.r .cell-code}\nfinal_tree %>% \n  extract_fit_engine() %>% \n  rpart.plot(roundint = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-136-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\nPerhaps we would also like to understand what variables are important in this final model. We can use the {vip} package to estimate variable importance based on the modelâ€™s structure.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-137_e2dc3b891a80f2138e29816b57656e16'}\n\n```{.r .cell-code}\nfinal_tree %>% \n  extract_fit_parsnip() %>% \n  vip()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-137-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n## A predictive modeling case study\n\n### The hotel bookings data\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-138_c99520c06eb4d3624b378d50d7bfd3de'}\n\n```{.r .cell-code}\nhotels <- \n  read_csv(\"https://tidymodels.org/start/case-study/hotels.csv\",\n           show_col_types = FALSE) %>% \n  mutate(across(where(is.character), as_factor))\n\ndim(hotels)\n#> [1] 50000    23\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-139_e8368b81b54d434c5b556e0dd95c5191'}\n\n```{.r .cell-code}\nglimpse(hotels)\n#> Rows: 50,000\n#> Columns: 23\n#> $ hotel                          <fct> City_Hotel, City_Hotel, Resort_Hotel, Râ€¦\n#> $ lead_time                      <dbl> 217, 2, 95, 143, 136, 67, 47, 56, 80, 6â€¦\n#> $ stays_in_weekend_nights        <dbl> 1, 0, 2, 2, 1, 2, 0, 0, 0, 2, 1, 0, 1, â€¦\n#> $ stays_in_week_nights           <dbl> 3, 1, 5, 6, 4, 2, 2, 3, 4, 2, 2, 1, 2, â€¦\n#> $ adults                         <dbl> 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, â€¦\n#> $ children                       <fct> none, none, none, none, none, none, chiâ€¦\n#> $ meal                           <fct> BB, BB, BB, HB, HB, SC, BB, BB, BB, BB,â€¦\n#> $ country                        <fct> DEU, PRT, GBR, ROU, PRT, GBR, ESP, ESP,â€¦\n#> $ market_segment                 <fct> Offline_TA/TO, Direct, Online_TA, Onlinâ€¦\n#> $ distribution_channel           <fct> TA/TO, Direct, TA/TO, TA/TO, Direct, TAâ€¦\n#> $ is_repeated_guest              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n#> $ previous_cancellations         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n#> $ previous_bookings_not_canceled <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n#> $ reserved_room_type             <fct> A, D, A, A, F, A, C, B, D, A, A, D, A, â€¦\n#> $ assigned_room_type             <fct> A, K, A, A, F, A, C, A, D, A, D, D, A, â€¦\n#> $ booking_changes                <dbl> 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n#> $ deposit_type                   <fct> No_Deposit, No_Deposit, No_Deposit, No_â€¦\n#> $ days_in_waiting_list           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n#> $ customer_type                  <fct> Transient-Party, Transient, Transient, â€¦\n#> $ average_daily_rate             <dbl> 80.75, 170.00, 8.00, 81.00, 157.60, 49.â€¦\n#> $ required_car_parking_spaces    <fct> none, none, none, none, none, none, nonâ€¦\n#> $ total_of_special_requests      <dbl> 1, 3, 2, 1, 4, 1, 1, 1, 1, 1, 0, 1, 0, â€¦\n#> $ arrival_date                   <date> 2016-09-01, 2017-08-25, 2016-11-19, 20â€¦\n```\n:::\n\n\n### Data splitting & resampling\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-140_015fb1f10339793f2d0218cbcc352eec'}\n\n```{.r .cell-code}\nset.seed(123)\nsplits <- initial_split(hotels, strata = children)\n\nhotels_other <- training(splits)\nhotel_test <- testing(splits)\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-141_eb48572d39f4671176fb9d4ea91d7da6'}\n\n```{.r .cell-code}\n# training set proportions by children\nhotels_other %>% \n  count(children) %>% \n  mutate(prop = n / sum(n))\n#> # A tibble: 2 Ã— 3\n#>   children     n   prop\n#>   <fct>    <int>  <dbl>\n#> 1 none     34473 0.919 \n#> 2 children  3027 0.0807\n\n# test set proportion by children\nhotel_test %>% \n  count(children) %>% \n  mutate(prop = n / sum(n))\n#> # A tibble: 2 Ã— 3\n#>   children     n   prop\n#>   <fct>    <int>  <dbl>\n#> 1 none     11489 0.919 \n#> 2 children  1011 0.0809\n```\n:::\n\n\nFor this case study, rather than using multiple iterations of resampling, letâ€™s create a single resample called a *validation* set. In tidymodels, a validation set is treated as a single iteration of resampling. This will be a split from the 37,500 stays that were not used for testing, which we called `hotel_other`. This split creates two new datasets:\n\n- the set held out for the purpose of measuring performance, called the *validation* set, and\n- the remaining data used to fit the model, called the *training* set.\n\n![](imgs/validation-split.svg){width=80%}\n\nWeâ€™ll use the `validation_split()` function to allocate 20% of the `hotel_other` stays to the *validation* set and 30,000 stays to the *training* set. This means that our model performance metrics will be computed on a single set of 7,500 hotel stays. This is fairly large, so the amount of data should provide enough precision to be a reliable indicator for how well each model predicts the outcome with a single iteration of resampling.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-142_c036f24c649c59d7b9510f8f544bcb19'}\n\n```{.r .cell-code}\nset.seed(234)\nval_set <- validation_split(hotels_other,\n                            strata = children,\n                            prop = .8)\nval_set\n#> # Validation Set Split (0.8/0.2)  using stratification \n#> # A tibble: 1 Ã— 2\n#>   splits               id        \n#>   <list>               <chr>     \n#> 1 <split [30000/7500]> validation\n```\n:::\n\n\n### A first model: penalized logistic regression\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-143_cb7d7d8fbadc68791968e354720e52f6'}\n\n```{.r .cell-code}\n# build the model\nlr_mod <- \n  logistic_reg(penalty = tune(), mixture = 1) %>% \n  set_engine(\"glmnet\")\n```\n:::\n\n\nSetting `mixture` to a value of one means that the glmnet model will potentially remove irrelevant predictors and choose a simpler model.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-144_f16ca2b3b4b730f49d6b57cbf5c9cc0b'}\n\n```{.r .cell-code}\n# create the recipe\nholidays <- c(\"AllSouls\", \"AshWednesday\", \"ChristmasEve\", \n              \"Easter\", \"ChristmasDay\", \"GoodFriday\",\n              \"NewYearsDay\",\"PalmSunday\")\n\nlr_recipe <- \n  recipe(children ~ ., data = hotels_other) %>% \n  step_date(arrival_date) %>% \n  step_holiday(arrival_date, holidays = holidays) %>% \n  step_rm(arrival_date) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_predictors())\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-145_4725f64491fff1f1175ddcd5f5c19921'}\n\n```{.r .cell-code}\n# create the workflow\nlr_wflow <- \n  workflow() %>% \n  add_model(lr_mod) %>% \n  add_recipe(lr_recipe)\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-146_ed2deeb84c3c1d5e53c34d3abe9d7bad'}\n\n```{.r .cell-code}\n# create the grid for tuning\nlr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))\n```\n:::\n\n\nLetâ€™s use `tune::tune_grid()` to train these 30 penalized logistic regression models. Weâ€™ll also save the validation set predictions (via the call to `control_grid()`) so that diagnostic information can be available after the model fit. The area under the ROC curve will be used to quantify how well the model performs across a continuum of event thresholds (recall that the event rateâ€”the proportion of stays including childrenâ€” is very low for these data).\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-147_08962bbd92876f14e2bc6351b0dbe8ec'}\n\n```{.r .cell-code}\n# train and tune the model\nlr_res <- \n  lr_wflow %>% \n  tune_grid(val_set,\n            grid = lr_reg_grid,\n            control = control_grid(save_pred = TRUE),\n            metrics = metric_set(roc_auc))\n```\n:::\n\n\nIt might be easier to visualize the validation set metrics by plotting the area under the ROC curve against the range of penalty values:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-148_8b2d330276905b004e8e34ac0dcff751'}\n\n```{.r .cell-code}\nlr_plot <- \n  lr_res %>% \n  collect_metrics() %>% \n  ggplot(aes(penalty, mean)) +\n  geom_point() +\n  geom_line() +\n  ylab(\"Area under the ROC Curve\") +\n  scale_x_log10(labels = scales::label_number())\n\nlr_plot\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-148-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\nThis plots shows us that model performance is generally better at the smaller penalty values. This suggests that the majority of the predictors are important to the model. We also see a steep drop in the area under the ROC curve towards the highest penalty values. This happens because a large enough penalty will remove all predictors from the model, and not surprisingly predictive accuracy plummets with no predictors in the model.\n\nOur model performance seems to plateau at the smaller penalty values, so going by the `roc_auc` metric alone could lead us to multiple options for the â€œbestâ€ value for this hyperparameter:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-149_7467702d373bad15afcb5ba8fc6df44c'}\n\n```{.r .cell-code}\ntop_models <- \n  lr_res %>% \n  show_best(\"roc_auc\", n = 15) %>% \n  arrange(penalty)\n\ntop_models\n#> # A tibble: 15 Ã— 7\n#>     penalty .metric .estimator  mean     n std_err .config              \n#>       <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#>  1 0.000127 roc_auc binary     0.872     1      NA Preprocessor1_Model02\n#>  2 0.000161 roc_auc binary     0.872     1      NA Preprocessor1_Model03\n#>  3 0.000204 roc_auc binary     0.873     1      NA Preprocessor1_Model04\n#>  4 0.000259 roc_auc binary     0.873     1      NA Preprocessor1_Model05\n#>  5 0.000329 roc_auc binary     0.874     1      NA Preprocessor1_Model06\n#>  6 0.000418 roc_auc binary     0.874     1      NA Preprocessor1_Model07\n#>  7 0.000530 roc_auc binary     0.875     1      NA Preprocessor1_Model08\n#>  8 0.000672 roc_auc binary     0.875     1      NA Preprocessor1_Model09\n#>  9 0.000853 roc_auc binary     0.875     1      NA Preprocessor1_Model10\n#> 10 0.00108  roc_auc binary     0.876     1      NA Preprocessor1_Model11\n#> 11 0.00137  roc_auc binary     0.876     1      NA Preprocessor1_Model12\n#> 12 0.00174  roc_auc binary     0.876     1      NA Preprocessor1_Model13\n#> 13 0.00221  roc_auc binary     0.875     1      NA Preprocessor1_Model14\n#> 14 0.00281  roc_auc binary     0.874     1      NA Preprocessor1_Model15\n#> 15 0.00356  roc_auc binary     0.872     1      NA Preprocessor1_Model16\n```\n:::\n\n\nHowever, we may want to choose a penalty value further along the x-axis, closer to where we start to see the decline in model performance. For example, candidate model 12 with a penalty value of 0.00174 has effectively the same performance as the numerically best model (model 11), but might eliminate more predictors. This penalty value is marked by the solid line above. In general, fewer irrelevant predictors is better. If performance is about the same, weâ€™d prefer to choose a higher penalty value.\n\nLetâ€™s select this value and visualize the validation set ROC curve:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-150_def6e9d423e4d7543f70148ffcdd63f0'}\n\n```{.r .cell-code}\nlr_best <- \n  lr_res %>% \n  collect_metrics() %>% \n  arrange(penalty) %>% \n  slice(12)\n\nlr_best\n#> # A tibble: 1 Ã— 7\n#>   penalty .metric .estimator  mean     n std_err .config              \n#>     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1 0.00137 roc_auc binary     0.876     1      NA Preprocessor1_Model12\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-151_cf02b68c4f621d637f59e087bad3848f'}\n\n```{.r .cell-code}\nlr_auc <- \n  lr_res %>% \n  collect_predictions(parameters = lr_best) %>% \n  roc_curve(children, .pred_none) %>% \n  mutate(model = \"Logistic Regression\")\n\nautoplot(lr_auc)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-151-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\nThe level of performance generated by this logistic regression model is good, but not groundbreaking. Perhaps the linear nature of the prediction equation is too limiting for this data set. As a next step, we might consider a highly non-linear model generated using a tree-based ensemble method.\n\n### A second model: tree-based ensemble\n\nEach tree is non-linear, and aggregating across trees makes random forests also non-linear but more robust and stable compared to individual trees. Tree-based models like random forests require very little preprocessing and can effectively handle many types of predictors (sparse, skewed, continuous, categorical, etc.).\n\nAlthough the default hyperparameters for random forests tend to give reasonable results, weâ€™ll plan to tune two hyperparameters that we think could improve performance. Unfortunately, random forest models can be computationally expensive to train and to tune. The computations required for model tuning can usually be easily parallelized to improve training time. The {tune} package can do parallel processing for you, and allows users to use multiple cores or separate machines to fit models.\n\nBut, here we are using a single validation set, so parallelization isnâ€™t an option using the tune package. For this specific case study, a good alternative is provided by the engine itself. The {ranger} package offers a built-in way to compute individual random forest models in parallel. To do this, we need to know the the number of cores we have to work with. We can use the parallel package to query the number of cores on your own computer to understand how much parallelization you can do:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-152_ba051cda1622e45c4118ea7b8879bb1f'}\n\n```{.r .cell-code}\ncores <- parallel::detectCores()\ncores\n#> [1] 8\n```\n:::\n\n\nWe have 10 8 to work with. We can pass this information to the ranger engine when we set up our parsnip rand_forest() model. To enable parallel processing, we can pass engine-specific arguments like num.threads to ranger when we set the engine:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-153_4926a66a9c73da193cdc6631b746b67b'}\n\n```{.r .cell-code}\nrf_mod <- \n  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% \n  set_engine(\"ranger\", num.threads = cores) %>% \n  set_mode(\"classification\")\n```\n:::\n\n\nThis works well in this modeling context, but it bears repeating: if you use any other resampling method, let tune do the parallel processing for you â€” we typically do not recommend relying on the modeling engine (like we did here) to do this.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-154_8eddb7e455064cc33d3922eed1333782'}\n\n```{.r .cell-code}\n# create the recipe and workflow\nrf_recipe <- \n  recipe(children ~ ., data = hotels_other) %>% \n  step_date(arrival_date) %>% \n  step_holiday(arrival_date) %>% \n  step_rm(arrival_date)\n\nrf_wflow <- \n  workflow() %>% \n  add_model(rf_mod) %>% \n  add_recipe(rf_recipe)\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-155_ecccc2c803f9fead830b2864c6cf8554'}\n\n```{.r .cell-code}\n# train and tune the model\nrf_mod\n#> Random Forest Model Specification (classification)\n#> \n#> Main Arguments:\n#>   mtry = tune()\n#>   trees = 1000\n#>   min_n = tune()\n#> \n#> Engine-Specific Arguments:\n#>   num.threads = cores\n#> \n#> Computational engine: ranger\n\n# show what will be tuned\nextract_parameter_set_dials(rf_mod)\n#> Collection of 2 parameters for tuning\n#> \n#>  identifier  type    object\n#>        mtry  mtry nparam[?]\n#>       min_n min_n nparam[+]\n#> \n#> Model parameters needing finalization:\n#>    # Randomly Selected Predictors ('mtry')\n#> \n#> See `?dials::finalize` or `?dials::update.parameters` for more information.\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-156_728c9227ba2fc2103b6fe3af76ef473d'}\n\n```{.r .cell-code}\nset.seed(345)\nrf_res <- \n  rf_wflow %>% \n  tune_grid(val_set,\n            grid = 25,\n            control = control_grid(save_pred = TRUE),\n            metrics = metric_set(roc_auc))\n#> i Creating pre-processing data to finalize unknown parameter: mtry\n```\n:::\n\n\nThe message printed above *â€œCreating pre-processing data to finalize unknown parameter: mtryâ€* is related to the size of the data set. Since `mtry` depends on the number of predictors in the data set, `tune_grid()` determines the upper bound for `mtry` once it receives the data.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-157_904eb04a6e195e391812492f1128a0f3'}\n\n```{.r .cell-code}\nrf_res %>% \n  show_best(metric = \"roc_auc\")\n#> # A tibble: 5 Ã— 8\n#>    mtry min_n .metric .estimator  mean     n std_err .config              \n#>   <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n#> 1     8     7 roc_auc binary     0.925     1      NA Preprocessor1_Model13\n#> 2     9    12 roc_auc binary     0.924     1      NA Preprocessor1_Model19\n#> 3    13     4 roc_auc binary     0.924     1      NA Preprocessor1_Model05\n#> 4     6    18 roc_auc binary     0.923     1      NA Preprocessor1_Model24\n#> 5    12     7 roc_auc binary     0.923     1      NA Preprocessor1_Model01\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-158_f600132a212b5b8d9910193e12084cfc'}\n\n```{.r .cell-code}\nautoplot(rf_res)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-158-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-159_2b278b1744baed288afef74214222e5c'}\n\n```{.r .cell-code}\n# select the best model\nrf_best <- \n  rf_res %>% \n  select_best(metric = \"roc_auc\")\n\nrf_best\n#> # A tibble: 1 Ã— 3\n#>    mtry min_n .config              \n#>   <int> <int> <chr>                \n#> 1     8     7 Preprocessor1_Model13\n```\n:::\n\n\nTo calculate the data needed to plot the ROC curve, we use `collect_predictions()`. This is only possible after tuning with `control_grid(save_pred = TRUE)`. In the output, you can see the two columns that hold our class probabilities for predicting hotel stays including and not including children.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-160_8b322656c3c15bba10974cdb85b40fd8'}\n\n```{.r .cell-code}\nrf_res %>% \n  collect_predictions()\n#> # A tibble: 187,500 Ã— 8\n#>    id         .pred_none .pred_children  .row  mtry min_n children .config      \n#>    <chr>           <dbl>          <dbl> <int> <int> <int> <fct>    <chr>        \n#>  1 validation      0.790       0.210       13    12     7 none     Preprocessorâ€¦\n#>  2 validation      0.988       0.0117      20    12     7 none     Preprocessorâ€¦\n#>  3 validation      0.536       0.464       22    12     7 children Preprocessorâ€¦\n#>  4 validation      0.993       0.00737     23    12     7 none     Preprocessorâ€¦\n#>  5 validation      0.988       0.0118      31    12     7 none     Preprocessorâ€¦\n#>  6 validation      0.999       0.000843    38    12     7 none     Preprocessorâ€¦\n#>  7 validation      1           0           39    12     7 none     Preprocessorâ€¦\n#>  8 validation      0.991       0.00899     50    12     7 none     Preprocessorâ€¦\n#>  9 validation      0.974       0.0262      54    12     7 none     Preprocessorâ€¦\n#> 10 validation      0.958       0.0423      57    12     7 children Preprocessorâ€¦\n#> # â„¹ 187,490 more rows\n```\n:::\n\n\nTo filter the predictions for only our best random forest model, we can use the `parameters` argument and pass it our tibble with the best hyperparameter values from tuning, which we called `rf_best`:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-161_c430b4dd1d960b498ad646e94ac1ea70'}\n\n```{.r .cell-code}\nrf_auc <- \n  rf_res %>% \n  collect_predictions(parameters = rf_best) %>% \n  roc_curve(children, .pred_none) %>% \n  mutate(model = \"Random Forest\")\n```\n:::\n\n\nNow, we can compare the validation set ROC curves for our top penalized logistic regression model and random forest model:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-162_356f88207685786981bf14ad0e11c96d'}\n\n```{.r .cell-code}\nbind_rows(rf_auc, lr_auc) %>% \n  ggplot(aes(x = 1 - specificity, y = sensitivity,\n             col = model)) +\n  geom_path(lwd = 1.5, alpha = .8) +\n  geom_abline(lty = 3) +\n  coord_equal() +\n  scale_color_viridis_d(option = \"plasma\", end = .6)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-162-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\nThe random forest is uniformly better across event probability thresholds.\n\n### The last fit \n\nAfter selecting our best model and hyperparameter values, our last step is to fit the final model on all the rows of data not originally held out for testing (both the training and the validation sets combined), and then evaluate the model performance one last time with the held-out test set.\n\nWeâ€™ll start by building our parsnip model object again from scratch. We take our best hyperparameter values from our random forest model. When we set the engine, we add a new argument: `importance = \"impurity\"`. This will provide variable importance scores for this last model, which gives some insight into which predictors drive model performance.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-163_acbf2ab2e0be8ecef20f37c6ecf2b887'}\n\n```{.r .cell-code}\n# the last model\nlast_rf_mod <- \n  rand_forest(mtry = 8, min_n = 7, trees = 1000) %>% \n  set_engine(\"ranger\", num.threads = cores, importance = \"impurity\") %>% \n  set_mode(\"classification\")\n\n# the last workflow\nlast_rf_wflow <- \n  rf_wflow %>% \n  update_model(last_rf_mod)\n\n# the last fit\nset.seed(345)\nlast_rf_fit <- \n  last_rf_wflow %>% \n  last_fit(splits)\n\nlast_rf_fit\n#> # Resampling results\n#> # Manual resampling \n#> # A tibble: 1 Ã— 6\n#>   splits                id             .metrics .notes   .predictions .workflow \n#>   <list>                <chr>          <list>   <list>   <list>       <list>    \n#> 1 <split [37500/12500]> train/test spâ€¦ <tibble> <tibble> <tibble>     <workflow>\n```\n:::\n\n\nThis fitted workflow contains `everything`, including our final metrics based on the test set. So, how did this model do on the test set? Was the validation set a good estimate of future performance?\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-164_0848d40b4af296ffc5c8b80a807396c0'}\n\n```{.r .cell-code}\nlast_rf_fit %>% \n  collect_metrics()\n#> # A tibble: 2 Ã— 4\n#>   .metric  .estimator .estimate .config             \n#>   <chr>    <chr>          <dbl> <chr>               \n#> 1 accuracy binary         0.947 Preprocessor1_Model1\n#> 2 roc_auc  binary         0.925 Preprocessor1_Model1\n```\n:::\n\n\nThis ROC AUC value is pretty close to what we saw when we tuned the random forest model with the validation set, which is good news. That means that our estimate of how well our model would perform with new data was not too far off from how well our model actually performed with the unseen test data.\n\nWe can access those variable importance scores via the `.workflow` column. We can extract out the fit from the workflow object, and then use the vip package to visualize the variable importance scores for the top 20 features:\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-165_7705dfb5a74e02e759d4494a2077d636'}\n\n```{.r .cell-code}\nlast_rf_fit %>% \n  extract_fit_parsnip() %>% \n  vip(num_features = 20)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-165-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\nThe most important predictors in whether a hotel stay had children or not were the daily cost for the room, the type of room reserved, the time between the creation of the reservation and the arrival date, and the type of room that was ultimately assigned.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/unnamed-chunk-166_fd4aaaab48305579df4f79c737aa99d7'}\n\n```{.r .cell-code}\nlast_rf_fit %>% \n  collect_predictions() %>% \n  roc_curve(children, .pred_none) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-166-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\nBased on these results, the validation set and test set performance statistics are very close, so we would have pretty high confidence that our random forest model with the selected hyperparameters would perform well when predicting new data.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}