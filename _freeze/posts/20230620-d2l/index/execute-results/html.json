{
  "hash": "f382970a03616db54517ee5b6cc09ad2",
  "result": {
    "markdown": "---\ntitle: \"动手学深度学习\"\ndate: \"2023-06-20\"\ndate-modified: \"2023-06-30\"\nimage: \"front.png\"\ncategories: [\"Deep Learning\", \"Python\"]\n---\n\n::: {.callout-note title='Progress'}\nLearning Progress: 17.5%.\n:::\n\n::: {.callout-tip title=\"Learning Source\"}\n- <https://zh-v2.d2l.ai/index.html>\n:::\n\n\n# 前言 {.unnumbered}\n\n- 任何一种计算技术要想发挥其全部影响力，都必须得到充分的理解、充分的文档记录，并得到成熟的、维护良好的工具的支持。关键思想应该被清楚地提炼出来，尽可能减少需要让新的从业者跟上时代的入门时间。成熟的库应该自动化常见的任务，示例代码应该使从业者可以轻松地修改、应用和扩展常见的应用程序，以满足他们的需求。\n\n# 引言\n\n- 通常，即使我们不知道怎样明确地告诉计算机如何从输入映射到输出，大脑仍然能够自己执行认知功能。 换句话说，即使我们不知道如何编写计算机程序来识别“Alexa”这个词，大脑自己也能够识别它。 有了这一能力，我们就可以收集一个包含大量音频样本的*数据集*（dataset），并对包含和不包含唤醒词的样本进行标记。 利用机器学习算法，我们不需要设计一个“明确地”识别唤醒词的系统。 相反，我们只需要定义一个灵活的程序算法，其输出由许多*参数*（parameter）决定，然后使用数据集来确定当下的“最佳参数集”，这些参数通过某种性能度量方式来达到完成任务的最佳性能。  \n那么到底什么是参数呢？ 参数可以被看作旋钮，旋钮的转动可以调整程序的行为。 任一调整参数后的程序被称为*模型*（model）。 通过操作参数而生成的所有不同程序（输入-输出映射）的集合称为“模型族”。 使用数据集来选择参数的元程序被称为*学习算法*（learning algorithm）。\n\n- 深度学习与经典方法的区别主要在于：前者关注的功能强大的模型，这些模型由神经网络错综复杂的交织在一起，包含层层数据转换，因此被称为*深度学习*（deep learning）\n\n- 当一个模型在训练集上表现良好，但不能推广到测试集时，这个模型被称为*过拟合*（overfitting）的。 就像在现实生活中，尽管模拟考试考得很好，真正的考试不一定百发百中。\n\n- 虽然监督学习只是几大类机器学习问题之一，但是在工业中，大部分机器学习的成功应用都使用了监督学习。 这是因为在一定程度上，许多重要的任务可以清晰地描述为，在给定一组特定的可用数据的情况下，估计未知事物的概率。\n\n- 回归是训练一个回归函数来输出一个数值； 分类是训练一个分类器来输出预测的类别。\n\n- 分类可能变得比二项分类、多项分类复杂得多。 例如，有一些分类任务的变体可以用于寻找层次结构，层次结构假定在许多类之间存在某种关系。 因此，**并不是所有的错误都是均等的**。 人们宁愿错误地分入一个相关的类别，也不愿错误地分入一个遥远的类别，这通常被称为*层次分类*(hierarchical classification)。\n\n- 学习预测不相互排斥的类别的问题称为*多标签分类*（multi-label classification）。\n\n- 无监督学习\n    - 聚类（clustering）问题：没有标签的情况下，我们是否能给数据分类呢？\n    - 主成分分析（principal component analysis）问题：我们能否找到少量的参数来准确地捕捉数据的线性相关属性？\n    - 因果关系（causality）和概率图模型（probabilistic graphical models）问题：我们能否描述观察到的许多数据的根本原因？\n    - 生成对抗性网络（generative adversarial networks）：为我们提供一种合成数据的方法，甚至像图像和音频这样复杂的非结构化数据。\n\n- 简单的离线学习有它的魅力。 好的一面是，我们可以孤立地进行模式识别，而不必分心于其他问题。 但缺点是，解决的问题相当有限。 这时我们可能会期望人工智能不仅能够做出预测，而且能够与真实环境互动。 **与预测不同，“与真实环境互动”实际上会影响环境**。 这里的人工智能是“智能代理”，而不仅是“预测模型”。 因此，我们必须考虑到它的行为可能会影响未来的观察结果。\n\n- 在强化学习问题中，智能体（agent）在一系列的时间步骤上与环境交互。 在每个特定时间点，智能体从环境接收一些观察（observation），并且必须选择一个动作（action），然后通过某种机制（有时称为执行器）将其传输回环境，最后智能体从环境中获得奖励（reward）。 此后新一轮循环开始，智能体接收后续观察，并选择后续操作，依此类推。 强化学习的过程在 @fig-rl-environment 中进行了说明。 请注意，**强化学习的目标是产生一个好的策略（policy）**。 强化学习智能体选择的“动作”受策略控制，即一个从环境观察映射到行动的功能。\n\n![强化学习和环境之间的相互作用](img/rl-environment.svg){width=60% #fig-rl-environment}\n\n- 当环境可被完全观察到时，强化学习问题被称为*马尔可夫决策过程*（markov decision process）。 当状态不依赖于之前的操作时，我们称该问题为*上下文赌博机*（contextual bandit problem）。 当没有状态，只有一组最初未知回报的可用动作时，这个问题就是经典的*多臂赌博机*（multi-armed bandit problem）。\n\n- 如前所述，机器学习可以使用数据来学习输入和输出之间的转换，例如在语音识别中将音频转换为文本。 在这样做时，通常需要以适合算法的方式表示数据，以便将这种表示转换为输出。 深度学习是“深度”的，模型学习了许多“层”的转换，每一层提供一个层次的表示。 例如，靠近输入的层可以表示数据的低级细节，而接近分类输出的层可以表示用于区分的更抽象的概念。 由于**表示学习（representation learning）目的是寻找表示本身，因此深度学习可以称为“多级表示学习”**。\n\n- 事实证明，这些多层模型能够以以前的工具所不能的方式处理低级的感知数据。 毋庸置疑，深度学习方法中最显著的共同点是使用**端到端训练**。 也就是说，与其基于单独调整的组件组装系统，不如构建系统，然后联合调整它们的性能。 例如，在计算机视觉中，科学家们习惯于将特征工程的过程与建立机器学习模型的过程分开。 Canny边缘检测器 (Canny, 1987) 和SIFT特征提取器 (Lowe, 2004) 作为将图像映射到特征向量的算法，在过去的十年里占据了至高无上的地位。 在过去的日子里，将机器学习应用于这些问题的关键部分是提出人工设计的特征工程方法，将数据转换为某种适合于浅层模型的形式。 然而，与一个算法自动执行的数百万个选择相比，人类通过特征工程所能完成的事情很少。 当深度学习开始时，这些特征抽取器被自动调整的滤波器所取代，产生了更高的精确度。  \n因此，深度学习的一个关键优势是它不仅取代了传统学习管道末端的浅层模型，而且还取代了劳动密集型的特征工程过程。 此外，通过取代大部分特定领域的预处理，深度学习消除了以前分隔计算机视觉、语音识别、自然语言处理、医学信息学和其他应用领域的许多界限，为解决各种问题提供了一套统一的工具。\n\n# 预备知识\n\n## 数据操作 {#sec-ndarray}\n\n### 入门\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport torch\n```\n:::\n\n\n张量表示一个由数值组成的数组，这个数组可能有多个维度。 具有一个轴的张量对应数学上的向量（vector）； 具有两个轴的张量对应数学上的矩阵（matrix）； 具有两个轴以上的张量没有特殊的数学名称。\n\n除非额外指定，新的张量将存储在内存中，并采用基于CPU的计算。\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# 创建张量\nx = torch.arange(12)\nx\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\ntensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# 访问张量（沿每个轴的长度）的形状\nx.shape  \n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\ntorch.Size([12])\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# 改变张量的形状\nX = x.reshape(3, 4)\nX\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\ntensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])\n```\n:::\n:::\n\n\n要重点说明一下，虽然张量的形状发生了改变，但其元素值并没有变。 注意，通过改变张量的形状，张量的大小不会改变。\n\n此外，我们可以通过-1来调用自动计算出维度的功能。 即我们可以用`x.reshape(-1,4)`或`x.reshape(3,-1)`来取代`x.reshape(3,4)`。\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# 与 x.reshape(3, 4)效果一致\nx.reshape(-1, 4)\nx.reshape(3, -1)\n```\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# 创建形状为（2,3,4）的张量，其中所有元素都设置为0\ntorch.zeros((2, 3, 4))\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\ntensor([[[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# 创建形状为（2,3,4）的张量，其中所有元素都设置为1\ntorch.ones((2, 3, 4))\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\ntensor([[[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]],\n\n        [[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# 创建形状为（3,4）的张量，每个元素都从标准正态分布中随机采样\ntorch.randn(3, 4)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\ntensor([[ 0.1192, -1.6174,  1.0081,  0.3552],\n        [ 0.6043, -0.8024,  1.8995,  0.5002],\n        [ 1.5816, -1.1343, -0.0586, -0.7276]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# 通过 Python 列表为张量中的元素赋值\ntorch.tensor([\n  [2, 1, 4, 3],\n  [1, 2, 3, 4],\n  [4, 3, 2, 1]\n])\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\ntensor([[2, 1, 4, 3],\n        [1, 2, 3, 4],\n        [4, 3, 2, 1]])\n```\n:::\n:::\n\n\n### 运算符\n\n对于任意具有相同形状的张量， 常见的标准算术运算符（+、-、\\*、\\/和\\*\\*）都可以被升级为按元素运算。 我们可以在同一形状的任意两个张量上调用按元素操作。\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nx = torch.tensor([1, 2, 4, 8])\ny = torch.tensor([2, 2, 2, 2])\n\n# 使用逗号来表示一个具有5个元素的元组\nx + y, x - y, x * y, x / y, x ** y\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n(tensor([ 3,  4,  6, 10]),\n tensor([-1,  0,  2,  6]),\n tensor([ 2,  4,  8, 16]),\n tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n tensor([ 1,  4, 16, 64]))\n```\n:::\n:::\n\n\n“按元素”方式可以应用更多的计算，包括像求幂这样的一元运算符。\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ntorch.exp(x)\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\ntensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])\n```\n:::\n:::\n\n\n可以把多个张量连结（concatenate）在一起， 把它们端对端地叠起来形成一个更大的张量。 我们只需要提供张量列表，并给出沿哪个轴连结。\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nX = torch.arange(12, dtype = torch.float32).reshape((3, 4))\nY = torch.tensor([\n  [2.0, 1, 4, 3],\n  [1  , 2, 3, 4],\n  [4  , 3, 2, 1]\n])\ntorch.cat((X, Y), dim = 0), torch.cat((X, Y), dim = 1)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.],\n         [ 2.,  1.,  4.,  3.],\n         [ 1.,  2.,  3.,  4.],\n         [ 4.,  3.,  2.,  1.]]),\n tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))\n```\n:::\n:::\n\n\n通过逻辑运算符构建二元张量：\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nX == Y\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\ntensor([[False,  True, False,  True],\n        [False, False, False, False],\n        [False, False, False, False]])\n```\n:::\n:::\n\n\n对张量中的所有元素进行求和，会产生一个单元素张量：\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nX.sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\ntensor(66.)\n```\n:::\n:::\n\n\n### 广播机制\n\n在上面的部分中，我们看到了如何在相同形状的两个张量上执行按元素操作。 在某些情况下，即使形状不同，我们仍然可以通过调用 *广播机制*（broadcasting mechanism）来执行按元素操作。 这种机制的工作方式如下：\n\n1. 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；\n1. 对生成的数组执行按元素操作。\n\n在大多数情况下，我们将沿着数组中长度为1的轴进行广播，如下例子：\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\na = torch.arange(3).reshape((3, 1))\nb = torch.arange(2).reshape((1, 2))\na, b\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n(tensor([[0],\n         [1],\n         [2]]),\n tensor([[0, 1]]))\n```\n:::\n:::\n\n\n由于a和b分别是 $3\\times1$\n和 $1\\times2$\n矩阵，如果让它们相加，它们的形状不匹配。 我们将两个矩阵广播为一个更大的 $3\\times2$\n矩阵，如下所示：矩阵$\\mathbf{a}$将复制列， 矩阵$\\mathbf{b}$将复制行，然后再按元素相加。\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\na + b\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\ntensor([[0, 1],\n        [1, 2],\n        [2, 3]])\n```\n:::\n:::\n\n\n### 索引和切片\n\n就像在任何其他Python数组中一样，张量中的元素可以通过索引访问。 与任何Python数组一样：第一个元素的索引是0，最后一个元素索引是-1； 可以指定范围以包含第一个元素和最后一个之前的元素。\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n# 查看 X\nX\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n# 用[-1]选择最后一个元素，用[1:3]选择第二个和第三个元素\nX[-1], X[1:3]\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n(tensor([ 8.,  9., 10., 11.]),\n tensor([[ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.]]))\n```\n:::\n:::\n\n\n除读取外，我们还可以通过指定索引来将元素写入矩阵。\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nX[1, 2] = 9\nX\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  9.,  7.],\n        [ 8.,  9., 10., 11.]])\n```\n:::\n:::\n\n\n如果我们想为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值。 例如，`[0:2, :]`访问第1行和第2行，其中“`:`”代表沿轴1（列）的所有元素。 虽然我们讨论的是矩阵的索引，但这也适用于向量和超过2个维度的张量。\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nX[0:2, :] = 12\nX\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\ntensor([[12., 12., 12., 12.],\n        [12., 12., 12., 12.],\n        [ 8.,  9., 10., 11.]])\n```\n:::\n:::\n\n\n### 节省内存\n\n运行一些操作可能会导致为新结果分配内存。 例如，如果我们用`Y = X + Y`，我们将取消引用`Y`指向的张量，而是指向新分配的内存处的张量。\n\n在下面的例子中，我们用Python的`id()`函数演示了这一点， 它给我们提供了内存中引用对象的确切地址。 运行`Y = Y + X`后，我们会发现`id(Y)`指向另一个位置。 这是因为Python首先计算`Y + X`，为结果分配新的内存，然后使`Y`指向内存中的这个新位置。\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nbefore = id(Y)\nY = Y + X\nid(Y) == before\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\nFalse\n```\n:::\n:::\n\n\n这可能是不可取的，原因有两个：\n\n1. 首先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新；\n1. 如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。\n\n幸运的是，执行原地操作非常简单。 我们可以使用切片表示法将操作的结果分配给先前分配的数组，例如`Y[:] = <expression>`。 为了说明这一点，我们首先创建一个新的矩阵`Z`，其形状与另一个`Y`相同， 使用`zeros_like`来分配一个全0的块。\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\nZ = torch.zeros_like(Y)\nprint('id(Z):', id(Z))\nZ[:] = X + Y\nprint('id(Z):', id(Z))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nid(Z): 2695316729488\nid(Z): 2695316729488\n```\n:::\n:::\n\n\n如果在后续计算中没有重复使用`X`， 我们也可以使用`X[:] = X + Y`或`X += Y`来减少操作的内存开销。\n\n### 转换为其他 Python 对象\n\n将深度学习框架定义的张量转换为NumPy张量（ndarray）很容易，反之也同样容易。 torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nA = X.numpy()\nB = torch.tensor(A)\ntype(A), type(B)\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\n(numpy.ndarray, torch.Tensor)\n```\n:::\n:::\n\n\n要将大小为1的张量转换为Python标量，我们可以调用`item`函数或Python的内置函数。\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\na = torch.tensor([3.4])\na, a.item(), float(a), int(a)\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\n(tensor([3.4000]), 3.4000000953674316, 3.4000000953674316, 3)\n```\n:::\n:::\n\n\n## 数据预处理\n\n人工创建数据集：\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nimport os\n\nos.makedirs(os.path.join('.', 'data'), exist_ok = True)\ndata_file = os.path.join('.', 'data', 'house_tiny.csv')\nwith open(data_file, 'w') as f:\n  f.write('NumRooms,Allley,Price\\n') # 列名\n  f.write('NA,Pave,127500\\n')\n  f.write('2,NA,106000\\n')\n  f.write('4,NA,178100\\n')\n  f.write('NA,NA,140000\\n')\n```\n:::\n\n\n使用 Pandas 的 `read_csv()` 函数读取：\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\nimport pandas as pd\n\ndata = pd.read_csv(data_file)\nprint(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   NumRooms Allley   Price\n0       NaN   Pave  127500\n1       2.0    NaN  106000\n2       4.0    NaN  178100\n3       NaN    NaN  140000\n```\n:::\n:::\n\n\n注意，“`NaN`”项代表缺失值。 为了处理缺失的数据，典型的方法包括*插值法*和*删除法*， 其中插值法用一个替代值弥补缺失值，而删除法则直接忽略缺失值。 在这里，我们将考虑插值法。\n\n通过位置索引`iloc`，我们将`data`分成`inputs`和`outputs`， 其中前者为`data`的前两列，而后者为`data`的最后一列。 对于`inputs`中缺少的数值，我们用同一列的均值替换“`NaN`”项。\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\ninputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]\ninputs.NumRooms = inputs.NumRooms.fillna(inputs.NumRooms.mean())\nprint(inputs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   NumRooms Allley\n0       3.0   Pave\n1       2.0    NaN\n2       4.0    NaN\n3       3.0    NaN\n```\n:::\n:::\n\n\n对于`inputs`中的类别值或离散值，我们将“`NaN`”视为一个类别。 由于“巷子类型”（“Alley”）列只接受两种类型的类别值“`Pave`”和“`NaN`”， pandas可以自动将此列转换为两列“`Alley_Pave`”和“`Alley_nan`”。 巷子类型为“`Pave`”的行会将“`Alley_Pave`”的值设置为1，“`Alley_nan`”的值设置为0。 缺少巷子类型的行会将“`Alley_Pave`”和“`Alley_nan`”分别设置为0和1。\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\ninputs = pd.get_dummies(inputs, dummy_na = True, dtype = float)\nprint(inputs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   NumRooms  Allley_Pave  Allley_nan\n0       3.0          1.0         0.0\n1       2.0          0.0         1.0\n2       4.0          0.0         1.0\n3       3.0          0.0         1.0\n```\n:::\n:::\n\n\n现在`inputs`和`outputs`中的所有条目都是数值类型，它们可以转换为张量格式。 当数据采用张量格式后，可以通过在 @sec-ndarray 中引入的那些张量函数来进一步操作。\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\nX, y= torch.tensor(inputs.values), torch.tensor(outputs.values)\nX, y\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\n(tensor([[3., 1., 0.],\n         [2., 0., 1.],\n         [4., 0., 1.],\n         [3., 0., 1.]], dtype=torch.float64),\n tensor([127500, 106000, 178100, 140000]))\n```\n:::\n:::\n\n\n## 线性代数\n\n### 降维\n\n默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以指定张量沿哪一个轴来通过求和降低维度。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定`axis=0`。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\nA = torch.arange(20, dtype = torch.float32).reshape(5, 4)\nA_sum_axis0 = A.sum(axis = 0)\nA_sum_axis0, A_sum_axis0.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\n(tensor([40., 45., 50., 55.]), torch.Size([4]))\n```\n:::\n:::\n\n\n指定`axis=1`将通过汇总所有列的元素降维（轴1）。因此，输入轴1的维数在输出形状中消失。\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\nA_sum_axis1 = A.sum(axis=1)\nA_sum_axis1, A_sum_axis1.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\n(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))\n```\n:::\n:::\n\n\n沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和。\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\nA.sum(axis = [0, 1])  # 结果和A.sum()相同\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\ntensor(190.)\n```\n:::\n:::\n\n\n非降维求和：\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\n# 保持轴数不变\nsum_A = A.sum(axis = 1, keepdims = True)\nsum_A\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\ntensor([[ 6.],\n        [22.],\n        [38.],\n        [54.],\n        [70.]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\n# 通过广播将 A 除以 sum_A\nA / sum_A\n```\n\n::: {.cell-output .cell-output-display execution_count=33}\n```\ntensor([[0.0000, 0.1667, 0.3333, 0.5000],\n        [0.1818, 0.2273, 0.2727, 0.3182],\n        [0.2105, 0.2368, 0.2632, 0.2895],\n        [0.2222, 0.2407, 0.2593, 0.2778],\n        [0.2286, 0.2429, 0.2571, 0.2714]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\n# 求列和\nA.cumsum(axis = 0)\n```\n\n::: {.cell-output .cell-output-display execution_count=34}\n```\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  6.,  8., 10.],\n        [12., 15., 18., 21.],\n        [24., 28., 32., 36.],\n        [40., 45., 50., 55.]])\n```\n:::\n:::\n\n\n### 点积\n\n给定两个向量 $\\mathbf{x}, \\mathbf{y}\\in\\mathbb{R}^d$，\n它们的*点积*（dot product）$\\mathbf{x}^\\top\\mathbf{y}$\n （或 $\\langle\\mathbf{x},\\mathbf{y}\\rangle$）\n 是相同位置的按元素乘积的和：$\\mathbf{x}^\\top\\mathbf{y}=\\sum^d_{i=1} x_i y_i$。\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\nx = torch.arange(4, dtype = torch.float32)\ny = torch.ones(4, dtype = torch.float32)\nx, y, torch.dot(x, y)\n```\n\n::: {.cell-output .cell-output-display execution_count=35}\n```\n(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))\n```\n:::\n:::\n\n\n### 矩阵-向量积\n\n在代码中使用张量表示矩阵-向量积，我们使用`mv`函数。\n当我们为矩阵`A`和向量`x`调用`torch.mv(A, x)`时，会执行矩阵-向量积。\n注意，`A`的列维数（沿轴1的长度）必须与`x`的维数（其长度）相同。\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\nA.shape, x.shape, torch.mv(A, x)\n```\n\n::: {.cell-output .cell-output-display execution_count=36}\n```\n(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))\n```\n:::\n:::\n\n\n### 矩阵-矩阵乘法\n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\nB = torch.ones(4, 3)\ntorch.mm(A, B)\n```\n\n::: {.cell-output .cell-output-display execution_count=37}\n```\ntensor([[ 6.,  6.,  6.],\n        [22., 22., 22.],\n        [38., 38., 38.],\n        [54., 54., 54.],\n        [70., 70., 70.]])\n```\n:::\n:::\n\n\n### 范数\n\n线性代数中最有用的一些运算符是*范数*（norm）。\n非正式地说，向量的*范数*是表示一个向量有多大。\n这里考虑的*大小*（size）概念不涉及维度，而是分量的大小。\n\n在线性代数中，向量范数是将向量映射到标量的函数$f$。\n给定任意向量$\\mathbf{x}$，向量范数要满足一些属性。\n第一个性质是：如果我们按常数因子$\\alpha$缩放向量的所有元素，\n其范数也会按相同常数因子的*绝对值*缩放：\n\n$$\nf(\\alpha \\mathbf{x}) = |\\alpha| f(\\mathbf{x}).\n$$ {#eq-norm1}\n\n第二个性质是熟悉的三角不等式:\n\n$$\nf(\\mathbf{x} + \\mathbf{y}) \\leq f(\\mathbf{x}) + f(\\mathbf{y}).\n$$ {#eq-norm2}\n\n第三个性质简单地说范数必须是非负的:\n\n$$\nf(\\mathbf{x}) \\geq 0.\n$$ {#eq-norm3}\n\n这是有道理的。因为在大多数情况下，任何东西的最小的*大小*是0。\n最后一个性质要求范数最小为0，当且仅当向量全由0组成。\n\n$$\n\\forall i, [\\mathbf{x}]_i = 0 \\Leftrightarrow f(\\mathbf{x})=0.\n$$ {#eq-norm4}\n\n在深度学习中，我们经常试图解决优化问题：\n*最大化*分配给观测数据的概率;\n*最小化*预测和真实观测之间的距离。\n用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。\n**目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。**\n\n## 微积分\n\n在深度学习中，我们“训练”模型，不断更新它们，使它们在看到越来越多的数据时变得越来越好。\n通常情况下，变得更好意味着最小化一个*损失函数*（loss function），\n即一个衡量“模型有多糟糕”这个问题的分数。\n最终，我们真正关心的是生成一个模型，它能够在从未见过的数据上表现良好。\n但“训练”模型只能将模型与我们实际能看到的数据相拟合。\n因此，我们可以将拟合模型的任务分解为两个关键问题：\n\n- *优化*（optimization）：用模型拟合观测数据的过程；\n- *泛化*（generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。\n\n::: {.cell execution_count=39}\n``` {.python .cell-code}\nimport numpy as np\nfrom matplotlib_inline import backend_inline\n```\n:::\n\n\n::: {.cell execution_count=40}\n``` {.python .cell-code}\n# 定义 f(x)\ndef f(x):\n  return 3 * x ** 2 - 4 * x\n\n# 定义瞬时变化率计算函数\ndef numerical_lim(f, x, h):\n  return (f(x + h) - f(x)) / h\n\nh = 0.1\nfor i in range(5):\n  print(f'h={h:.5f}, numerical limit={numerical_lim(f, 1, h):.5f}')\n  h *= 0.1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nh=0.10000, numerical limit=2.30000\nh=0.01000, numerical limit=2.03000\nh=0.00100, numerical limit=2.00300\nh=0.00010, numerical limit=2.00030\nh=0.00001, numerical limit=2.00003\n```\n:::\n:::\n\n\n::: {.cell execution_count=41}\n``` {.python .cell-code}\n# 使用自定义的 d2l.py 中的函数\nimport d2l\n```\n:::\n\n\n::: {.cell execution_count=42}\n``` {.python .cell-code}\nx = np.arange(0, 3, 0.1)\nd2l.plot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])\n```\n\n::: {.cell-output .cell-output-display}\n![x=1处切线](index_files/figure-html/fig-qiexian-output-1.svg){#fig-qiexian}\n:::\n:::\n\n\n我们可以连结一个多元函数对其所有变量的偏导数，以得到该函数的*梯度*（gradient）向量。\n具体而言，设函数$f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$的输入是\n一个$n$维向量$\\mathbf{x}=[x_1,x_2,\\ldots,x_n]^\\top$，并且输出是一个标量。\n函数$f(\\mathbf{x})$相对于$\\mathbf{x}$的梯度是一个包含$n$个偏导数的向量:\n\n$$\n\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\bigg[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\bigg]^\\top,\n$$ {#eq-grandient}\n\n其中$\\nabla_{\\mathbf{x}} f(\\mathbf{x})$通常在没有歧义时被$\\nabla f(\\mathbf{x})$取代。\n\n假设$\\mathbf{x}$为$n$维向量，在微分多元函数时经常使用以下规则:\n\n- 对于所有$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$，都有$\\nabla_{\\mathbf{x}} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top$\n- 对于所有$\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$，都有$\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A}  = \\mathbf{A}$\n- 对于所有$\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$，都有$\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x}  = (\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}$\n- $\\nabla_{\\mathbf{x}} \\|\\mathbf{x} \\|^2 = \\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2\\mathbf{x}$\n\n同样，对于任何矩阵$\\mathbf{X}$，都有$\\nabla_{\\mathbf{X}} \\|\\mathbf{X} \\|_F^2 = 2\\mathbf{X}$。\n正如我们之后将看到的，梯度对于设计深度学习中的优化算法有很大用处。\n\n## 自动微分\n\n深度学习框架通过自动计算导数，即*自动微分*（automatic differentiation）来加快求导。\n实际中，根据设计好的模型，系统会构建一个*计算图*（computational graph），\n来跟踪计算是哪些数据通过哪些操作组合起来产生输出。\n自动微分使系统能够随后反向传播梯度。\n这里，*反向传播*（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。\n\n### 一个简单案例\n\n假设我们对函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量$\\mathbf{x}$求导。\n首先，我们创建变量`x`并为其分配一个初始值。\n\n::: {.cell execution_count=43}\n``` {.python .cell-code}\nimport torch\n\nx = torch.arange(4.0)\nx\n```\n\n::: {.cell-output .cell-output-display execution_count=42}\n```\ntensor([0., 1., 2., 3.])\n```\n:::\n:::\n\n\n在我们计算$y$关于$\\mathbf{x}$的梯度之前，需要一个地方来存储梯度。\n重要的是，我们不会在每次对一个参数求导时都分配新的内存。\n因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。\n注意，一个标量函数关于向量$\\mathbf{x}$的梯度是向量，并且与$\\mathbf{x}$具有相同的形状。\n\n::: {.cell execution_count=44}\n``` {.python .cell-code}\nx.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)\nx.grad  # 默认值是None\n```\n:::\n\n\n::: {.cell execution_count=45}\n``` {.python .cell-code}\ny = 2 * torch.dot(x, x)\ny\n```\n\n::: {.cell-output .cell-output-display execution_count=44}\n```\ntensor(28., grad_fn=<MulBackward0>)\n```\n:::\n:::\n\n\n`x`是一个长度为4的向量，计算`x`和`x`的点积，得到了我们赋值给`y`的标量输出。\n接下来，通过调用反向传播函数来自动计算`y`关于`x`每个分量的梯度，并打印这些梯度。\n\n::: {.cell execution_count=46}\n``` {.python .cell-code}\ny.backward()\nx.grad\n```\n\n::: {.cell-output .cell-output-display execution_count=45}\n```\ntensor([ 0.,  4.,  8., 12.])\n```\n:::\n:::\n\n\n函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于$\\mathbf{x}$的梯度应为$4\\mathbf{x}$。\n让我们快速验证这个梯度是否计算正确。\n\n::: {.cell execution_count=47}\n``` {.python .cell-code}\nx.grad == 4 * x\n```\n\n::: {.cell-output .cell-output-display execution_count=46}\n```\ntensor([True, True, True, True])\n```\n:::\n:::\n\n\n现在计算`x`的另一个函数。\n\n::: {.cell execution_count=48}\n``` {.python .cell-code}\n# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\nx.grad.zero_()\ny = x.sum()\ny.backward()\nx.grad\n```\n\n::: {.cell-output .cell-output-display execution_count=47}\n```\ntensor([1., 1., 1., 1.])\n```\n:::\n:::\n\n\n### 分离计算\n\n有时，我们希望**将某些计算移动到记录的计算图之外**]。\n例如，假设`y`是作为`x`的函数计算的，而`z`则是作为`y`和`x`的函数计算的。\n想象一下，我们想计算`z`关于`x`的梯度，但由于某种原因，希望将`y`视为一个常数，\n并且只考虑到`x`在`y`被计算后发挥的作用。\n\n这里可以分离`y`来返回一个新变量`u`，该变量与`y`具有相同的值，\n但丢弃计算图中如何计算`y`的任何信息。\n换句话说，梯度不会向后流经`u`到`x`。\n因此，下面的反向传播函数计算`z=u*x`关于`x`的偏导数，同时将`u`作为常数处理，\n而不是`z=x*x*x`关于`x`的偏导数。\n\n::: {.cell execution_count=49}\n``` {.python .cell-code}\nx.grad.zero_()\ny = x * x\nu = y.detach()\nz = u * x\n```\n:::\n\n\n由于记录了`y`的计算结果，我们可以随后在`y`上调用反向传播，\n得到`y=x*x`关于的`x`的导数，即`2*x`。\n\n::: {.cell execution_count=50}\n``` {.python .cell-code}\nx.grad.zero_()\ny.sum().backward()\nx.grad == 2 * x\n```\n\n::: {.cell-output .cell-output-display execution_count=49}\n```\ntensor([True, True, True, True])\n```\n:::\n:::\n\n\n### Python控制流的梯度计算\n\n使用自动微分的一个好处是：\n即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度。\n在下面的代码中，`while`循环的迭代次数和`if`语句的结果都取决于输入`a`的值。\n\n::: {.cell execution_count=51}\n``` {.python .cell-code}\ndef f(a):\n  b = a * 2\n  while b.norm() < 1000:\n    b = b * 2\n  if b.sum() > 0:\n    c = b\n  else:\n    c = 100 * b\n  return c\n```\n:::\n\n\n::: {.cell execution_count=52}\n``` {.python .cell-code}\n# 计算梯度\na = torch.randn(size = (), requires_grad = True)\nd = f(a)\nd.backward()\n```\n:::\n\n\n::: {.cell execution_count=53}\n``` {.python .cell-code}\na.grad == d / a\n```\n\n::: {.cell-output .cell-output-display execution_count=52}\n```\ntensor(True)\n```\n:::\n:::\n\n\n## 概率\n\n现实生活中，对于我们从工厂收到的真实骰子，我们需要检查它是否有瑕疵。\n检查骰子的唯一方法是多次投掷并记录结果。\n对于每个骰子，我们将观察到$\\{1, \\ldots, 6\\}$中的一个值。\n对于每个值，一种自然的方法是将它出现的次数除以投掷的总次数，\n即此*事件*（event）概率的*估计值*。\n*大数定律*（law of large numbers）告诉我们：\n随着投掷次数的增加，这个估计值会越来越接近真实的潜在概率。\n让我们用代码试一试！\n\n::: {.cell execution_count=54}\n``` {.python .cell-code}\nimport torch\nfrom torch.distributions import multinomial\n```\n:::\n\n\n::: {.cell execution_count=55}\n``` {.python .cell-code}\nfair_probs = torch.ones([6]) / 6\nmultinomial.Multinomial(1, fair_probs).sample()\n```\n\n::: {.cell-output .cell-output-display execution_count=54}\n```\ntensor([0., 0., 0., 0., 1., 0.])\n```\n:::\n:::\n\n\n在估计一个骰子的公平性时，我们希望从同一分布中生成多个样本。\n如果用Python的for循环来完成这个任务，速度会慢得惊人。\n因此我们使用深度学习框架的函数同时抽取多个样本，得到我们想要的任意形状的独立样本数组。\n\n::: {.cell execution_count=56}\n``` {.python .cell-code}\nmultinomial.Multinomial(10, fair_probs).sample()\n```\n\n::: {.cell-output .cell-output-display execution_count=55}\n```\ntensor([1., 3., 0., 2., 1., 3.])\n```\n:::\n:::\n\n\n现在我们知道如何对骰子进行采样，我们可以模拟1000次投掷。\n然后，我们可以统计1000次投掷后，每个数字被投中了多少次。\n具体来说，我们计算相对频率，以作为真实概率的估计。\n\n::: {.cell execution_count=57}\n``` {.python .cell-code}\n# 将结果存储为32位浮点数以进行除法\ncounts = multinomial.Multinomial(1000, fair_probs).sample()\ncounts / 1000   # 相对频率作为估计值\n```\n\n::: {.cell-output .cell-output-display execution_count=56}\n```\ntensor([0.1830, 0.1590, 0.1760, 0.1580, 0.1770, 0.1470])\n```\n:::\n:::\n\n\n# 线性神经网络\n\n## 线性回归\n\n- *仿射变换*（affine transformation）的特点是通过加权和对特征进行*线性变换*（linear transformation），并通过偏置项来进行*平移*（translation）。\n\n- 给定训练数据特征$\\mathbf{X}$和对应的已知标签$\\mathbf{y}$，\n线性回归的目标是找到一组权重向量$\\mathbf{w}$和偏置$b$：\n当给定从$\\mathbf{X}$的同分布中取样的新样本特征时，\n这组权重向量和偏置能够使得新样本预测标签的误差尽可能小。\n\n- *损失函数*（loss function）能够量化目标的*实际*值与*预测*值之间的差距。\n通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。\n回归问题中最常用的损失函数是平方误差函数。\n\n### 线性回归的基本元素\n\n- 梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值）\n关于模型参数的导数（在这里也可以称为梯度）。\n但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。\n因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本，\n这种变体叫做*小批量随机梯度下降*（minibatch stochastic gradient descent）。\n\n- *批量大小*（batch size）和*学习率*（learning rate）的值通常是手动预先指定，而不是通过模型训练得到的。\n这些可以调整但不在训练过程中更新的参数称为*超参数*（hyperparameter）。\n*调参*（hyperparameter tuning）是选择超参数的过程。\n超参数通常是我们根据训练迭代结果来调整的，\n而训练迭代结果是在独立的*验证数据集*（validation dataset）上评估得到的。\n\n- 线性回归恰好是一个在整个域中只有一个最小值的学习问题。\n但是对像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。\n深度学习实践者很少会去花费大力气寻找这样一组参数，使得在*训练集*上的损失达到最小。\n事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失，\n这一挑战被称为*泛化*（generalization）。\n\n### 矢量化加速\n\n\n::: {.cell execution_count=59}\n``` {.python .cell-code}\nn = 10000\na = torch.ones([n])\nb = torch.ones([n])\n```\n:::\n\n\n首先，我们使用for循环，每次执行一位的加法。\n\n::: {.cell execution_count=60}\n``` {.python .cell-code}\nc = torch.zeros([n])\ntimer = d2l.Timer()\nfor i in range(n):\n  c[i] = a[i] + b[i]\nf'{timer.stop():.5f} sec'\n```\n\n::: {.cell-output .cell-output-display execution_count=59}\n```\n'0.18630 sec'\n```\n:::\n:::\n\n\n或者，我们使用重载的`+`运算符来计算按元素的和。\n\n::: {.cell execution_count=61}\n``` {.python .cell-code}\ntimer.start()\nd = a + b\nf'{timer.stop():.5f} sec'\n```\n\n::: {.cell-output .cell-output-display execution_count=60}\n```\n'0.01273 sec'\n```\n:::\n:::\n\n\n结果很明显，第二种方法比第一种方法快得多。\n矢量化代码通常会带来数量级的加速。\n\n## 线性回归的从零开始实现\n\n::: {.cell execution_count=62}\n``` {.python .cell-code}\ntrue_w = torch.tensor([2, -3.4])\ntrue_b = 4.2\nfeatures, labels = d2l.synthetic_data(true_w, true_b, 1000)\n```\n:::\n\n\n::: {.callout-tip title=\"To be continued\"}\n- <https://zh-v2.d2l.ai/chapter_linear-networks/linear-regression.html>\n:::\n\n",
    "supporting": [
      "index_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}